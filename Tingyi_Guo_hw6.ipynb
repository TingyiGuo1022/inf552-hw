{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Supervised, Semi-Supervised, and Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Download the Breast Cancer Wisconsin (Diagnostic) Data Set : This data has two output classes. Use the first 20% of the positive and negative classes in the file as the test set and the rest as the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd;\n",
    "import numpy as np;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  1      2      3       4       5        6        7       8        9       10  \\\n",
       "0  M  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.3001  0.14710  0.2419   \n",
       "1  M  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.0869  0.07017  0.1812   \n",
       "2  M  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.1974  0.12790  0.2069   \n",
       "3  M  11.42  20.38   77.58   386.1  0.14250  0.28390  0.2414  0.10520  0.2597   \n",
       "4  M  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.1980  0.10430  0.1809   \n",
       "\n",
       "   ...     22     23      24      25      26      27      28      29      30  \\\n",
       "0  ...  25.38  17.33  184.60  2019.0  0.1622  0.6656  0.7119  0.2654  0.4601   \n",
       "1  ...  24.99  23.41  158.80  1956.0  0.1238  0.1866  0.2416  0.1860  0.2750   \n",
       "2  ...  23.57  25.53  152.50  1709.0  0.1444  0.4245  0.4504  0.2430  0.3613   \n",
       "3  ...  14.91  26.50   98.87   567.7  0.2098  0.8663  0.6869  0.2575  0.6638   \n",
       "4  ...  22.54  16.67  152.20  1575.0  0.1374  0.2050  0.4000  0.1625  0.2364   \n",
       "\n",
       "        31  \n",
       "0  0.11890  \n",
       "1  0.08902  \n",
       "2  0.08758  \n",
       "3  0.17300  \n",
       "4  0.07678  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read file into dataframe\n",
    "dataset = pd.read_csv(\"wdbc.data\", header = None)\n",
    "dataset = dataset.iloc[:,1:]\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a method to split dataset\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "def split_dataset(dataset_x, dataset_y, test_size, random):\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state = random)\n",
    "\n",
    "    for train_index, test_index in sss.split(dataset_x, dataset_y):\n",
    "        train_x, test_x = dataset_x.iloc[train_index,:], dataset_x.iloc[test_index,:]\n",
    "        train_y, test_y = dataset_y.iloc[train_index], dataset_y.iloc[test_index]\n",
    "    train_x = train_x.reset_index(drop=True)\n",
    "    train_y = train_y.reset_index(drop=True)\n",
    "    test_x = test_x.reset_index(drop=True)\n",
    "    test_y = test_y.reset_index(drop=True)\n",
    "    return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>11.04</td>\n",
       "      <td>14.93</td>\n",
       "      <td>70.67</td>\n",
       "      <td>372.7</td>\n",
       "      <td>0.07987</td>\n",
       "      <td>0.07079</td>\n",
       "      <td>0.03546</td>\n",
       "      <td>0.020740</td>\n",
       "      <td>0.2003</td>\n",
       "      <td>0.06246</td>\n",
       "      <td>...</td>\n",
       "      <td>12.090</td>\n",
       "      <td>20.83</td>\n",
       "      <td>79.73</td>\n",
       "      <td>447.1</td>\n",
       "      <td>0.1095</td>\n",
       "      <td>0.1982</td>\n",
       "      <td>0.15530</td>\n",
       "      <td>0.06754</td>\n",
       "      <td>0.3202</td>\n",
       "      <td>0.07287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10.75</td>\n",
       "      <td>14.97</td>\n",
       "      <td>68.26</td>\n",
       "      <td>355.3</td>\n",
       "      <td>0.07793</td>\n",
       "      <td>0.05139</td>\n",
       "      <td>0.02251</td>\n",
       "      <td>0.007875</td>\n",
       "      <td>0.1399</td>\n",
       "      <td>0.05688</td>\n",
       "      <td>...</td>\n",
       "      <td>11.950</td>\n",
       "      <td>20.72</td>\n",
       "      <td>77.79</td>\n",
       "      <td>441.2</td>\n",
       "      <td>0.1076</td>\n",
       "      <td>0.1223</td>\n",
       "      <td>0.09755</td>\n",
       "      <td>0.03413</td>\n",
       "      <td>0.2300</td>\n",
       "      <td>0.06769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>11.71</td>\n",
       "      <td>16.67</td>\n",
       "      <td>74.72</td>\n",
       "      <td>423.6</td>\n",
       "      <td>0.10510</td>\n",
       "      <td>0.06095</td>\n",
       "      <td>0.03592</td>\n",
       "      <td>0.026000</td>\n",
       "      <td>0.1339</td>\n",
       "      <td>0.05945</td>\n",
       "      <td>...</td>\n",
       "      <td>13.330</td>\n",
       "      <td>25.48</td>\n",
       "      <td>86.16</td>\n",
       "      <td>546.7</td>\n",
       "      <td>0.1271</td>\n",
       "      <td>0.1028</td>\n",
       "      <td>0.10460</td>\n",
       "      <td>0.06968</td>\n",
       "      <td>0.1712</td>\n",
       "      <td>0.07343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>8.95</td>\n",
       "      <td>15.76</td>\n",
       "      <td>58.74</td>\n",
       "      <td>245.2</td>\n",
       "      <td>0.09462</td>\n",
       "      <td>0.12430</td>\n",
       "      <td>0.09263</td>\n",
       "      <td>0.023080</td>\n",
       "      <td>0.1305</td>\n",
       "      <td>0.07163</td>\n",
       "      <td>...</td>\n",
       "      <td>9.414</td>\n",
       "      <td>17.07</td>\n",
       "      <td>63.34</td>\n",
       "      <td>270.0</td>\n",
       "      <td>0.1179</td>\n",
       "      <td>0.1879</td>\n",
       "      <td>0.15440</td>\n",
       "      <td>0.03846</td>\n",
       "      <td>0.1652</td>\n",
       "      <td>0.07722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.152000</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.1650</td>\n",
       "      <td>0.8681</td>\n",
       "      <td>0.93870</td>\n",
       "      <td>0.26500</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      2      3       4       5        6        7        8         9       10  \\\n",
       "0  11.04  14.93   70.67   372.7  0.07987  0.07079  0.03546  0.020740  0.2003   \n",
       "1  10.75  14.97   68.26   355.3  0.07793  0.05139  0.02251  0.007875  0.1399   \n",
       "2  11.71  16.67   74.72   423.6  0.10510  0.06095  0.03592  0.026000  0.1339   \n",
       "3   8.95  15.76   58.74   245.2  0.09462  0.12430  0.09263  0.023080  0.1305   \n",
       "4  20.60  29.33  140.10  1265.0  0.11780  0.27700  0.35140  0.152000  0.2397   \n",
       "\n",
       "        11  ...      22     23      24      25      26      27       28  \\\n",
       "0  0.06246  ...  12.090  20.83   79.73   447.1  0.1095  0.1982  0.15530   \n",
       "1  0.05688  ...  11.950  20.72   77.79   441.2  0.1076  0.1223  0.09755   \n",
       "2  0.05945  ...  13.330  25.48   86.16   546.7  0.1271  0.1028  0.10460   \n",
       "3  0.07163  ...   9.414  17.07   63.34   270.0  0.1179  0.1879  0.15440   \n",
       "4  0.07016  ...  25.740  39.42  184.60  1821.0  0.1650  0.8681  0.93870   \n",
       "\n",
       "        29      30       31  \n",
       "0  0.06754  0.3202  0.07287  \n",
       "1  0.03413  0.2300  0.06769  \n",
       "2  0.06968  0.1712  0.07343  \n",
       "3  0.03846  0.1652  0.07722  \n",
       "4  0.26500  0.4087  0.12400  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split into training data and test data\n",
    "dataset_x = dataset.iloc[:,1:]\n",
    "dataset_y = dataset[1].apply(lambda x: 1 if x == 'M' else 0)\n",
    "train_x, train_y, test_x, test_y = split_dataset(dataset_x, dataset_y, test_size = 0.2, random = 0)\n",
    "train_x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Monte-Carlo Simulation: \n",
    "    Repeat the following procedures for supervised, un-supervised, and semi-supervised learning M = 30 times, and use randomly selected train and test data (make sure you use 20% of both the positve and negative classes as the test set). Then compare the average scores (accuracy, precision, recall, F-score, and AUC) that you obtain from each algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. Supervised Learning: \n",
    "    Train an L1-penalized SVM to classify the data. Use 5 fold cross validation to choose the penalty parameter. Use normalized data. Report the average accuracy, precision, recall, F-score, and AUC, for both training and test sets over your M runs. Plot the ROC and report the confusion matrix for training and testing in one of the runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normailze dataset_x\n",
    "from sklearn.preprocessing import normalize\n",
    "normalize_dataset_x = pd.DataFrame(normalize(dataset_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.007925</td>\n",
       "      <td>0.004573</td>\n",
       "      <td>0.054099</td>\n",
       "      <td>0.440986</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011181</td>\n",
       "      <td>0.007635</td>\n",
       "      <td>0.081325</td>\n",
       "      <td>0.889462</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000293</td>\n",
       "      <td>0.000314</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.000052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.008666</td>\n",
       "      <td>0.007486</td>\n",
       "      <td>0.055988</td>\n",
       "      <td>0.558619</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010528</td>\n",
       "      <td>0.009862</td>\n",
       "      <td>0.066899</td>\n",
       "      <td>0.824026</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.009367</td>\n",
       "      <td>0.010109</td>\n",
       "      <td>0.061842</td>\n",
       "      <td>0.572276</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011212</td>\n",
       "      <td>0.012145</td>\n",
       "      <td>0.072545</td>\n",
       "      <td>0.812984</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.016325</td>\n",
       "      <td>0.029133</td>\n",
       "      <td>0.110899</td>\n",
       "      <td>0.551922</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021314</td>\n",
       "      <td>0.037881</td>\n",
       "      <td>0.141333</td>\n",
       "      <td>0.811515</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.001238</td>\n",
       "      <td>0.000982</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.000949</td>\n",
       "      <td>0.000247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.009883</td>\n",
       "      <td>0.006985</td>\n",
       "      <td>0.065808</td>\n",
       "      <td>0.631774</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010979</td>\n",
       "      <td>0.008120</td>\n",
       "      <td>0.074137</td>\n",
       "      <td>0.767189</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.007925  0.004573  0.054099  0.440986  0.000052  0.000122  0.000132   \n",
       "1  0.008666  0.007486  0.055988  0.558619  0.000036  0.000033  0.000037   \n",
       "2  0.009367  0.010109  0.061842  0.572276  0.000052  0.000076  0.000094   \n",
       "3  0.016325  0.029133  0.110899  0.551922  0.000204  0.000406  0.000345   \n",
       "4  0.009883  0.006985  0.065808  0.631774  0.000049  0.000065  0.000096   \n",
       "\n",
       "         7         8         9   ...        20        21        22        23  \\\n",
       "0  0.000065  0.000107  0.000035  ...  0.011181  0.007635  0.081325  0.889462   \n",
       "1  0.000030  0.000076  0.000024  ...  0.010528  0.009862  0.066899  0.824026   \n",
       "2  0.000061  0.000098  0.000029  ...  0.011212  0.012145  0.072545  0.812984   \n",
       "3  0.000150  0.000371  0.000139  ...  0.021314  0.037881  0.141333  0.811515   \n",
       "4  0.000051  0.000088  0.000029  ...  0.010979  0.008120  0.074137  0.767189   \n",
       "\n",
       "         24        25        26        27        28        29  \n",
       "0  0.000071  0.000293  0.000314  0.000117  0.000203  0.000052  \n",
       "1  0.000052  0.000079  0.000102  0.000078  0.000116  0.000038  \n",
       "2  0.000069  0.000202  0.000214  0.000116  0.000172  0.000042  \n",
       "3  0.000300  0.001238  0.000982  0.000368  0.000949  0.000247  \n",
       "4  0.000067  0.000100  0.000195  0.000079  0.000115  0.000037  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_dataset_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supersived_learning():\n",
    "    result = list()\n",
    "    plot_inf = list()\n",
    "    #split dataset randomly\n",
    "    train_x, train_y, test_x, test_y = split_dataset(normalize_dataset_x, dataset_y, \n",
    "                                                     test_size = 0.2, random = None)\n",
    "    #use cv to find best param\n",
    "    parameters = {'C': np.power(10, np.arange(-3, 7, 0.5))}\n",
    "    svc = LinearSVC(penalty='l1', dual=False)\n",
    "    clf = GridSearchCV(svc, parameters, cv=5)\n",
    "    clf.fit(train_x, train_y)\n",
    "    best_c = clf.best_params_['C']\n",
    "    \n",
    "    #build model with best C\n",
    "    svc = LinearSVC(penalty='l1', dual=False, C = best_c)\n",
    "    svc.fit(train_x, train_y)\n",
    "    \n",
    "    #Accuracy\n",
    "    train_accuracy = svc.score(train_x, train_y)\n",
    "    test_accuracy = svc.score(test_x, test_y)\n",
    "    result.append(train_accuracy)\n",
    "    result.append(test_accuracy)\n",
    "    \n",
    "    # confusion matrix\n",
    "    train_predict = svc.predict(train_x)\n",
    "    test_predict = svc.predict(test_x)\n",
    "    train_cm = confusion_matrix(train_y, train_predict)\n",
    "    test_cm = confusion_matrix(test_y, test_predict)\n",
    "    plot_inf.append(train_cm)\n",
    "    plot_inf.append(test_cm)\n",
    "    \n",
    "    #precision\n",
    "    train_tn, test_tn = train_cm[0][0], test_cm[0][0]; \n",
    "    train_fp, test_fp = train_cm[0][1], test_cm[0][1]; \n",
    "    train_fn, test_fn = train_cm[1][0], test_cm[1][0]; \n",
    "    train_tp, test_tp = train_cm[1][1], test_cm[1][1];\n",
    "    train_precision = train_tp/(train_tp + train_fp)\n",
    "    test_precision = test_tp/(test_tp + test_fp)\n",
    "    result.append(train_precision)\n",
    "    result.append(test_precision)\n",
    "    \n",
    "    #recall\n",
    "    train_recall = train_tp/(train_tp + train_fn)\n",
    "    test_recall = test_tp/(test_tp + test_fn)\n",
    "    result.append(train_recall)\n",
    "    result.append(test_recall)\n",
    "    \n",
    "    #F-score\n",
    "    train_f1 = 2 * (train_precision * train_recall) / (train_precision + train_recall)\n",
    "    test_f1 = 2 * (test_precision * test_recall) / (test_precision + test_recall)\n",
    "    result.append(train_f1)\n",
    "    result.append(test_f1)\n",
    "    \n",
    "    # AUC\n",
    "    train_predict_prob = svc.decision_function(train_x)\n",
    "    test_predict_prob = svc.decision_function(test_x)\n",
    "    plot_inf.append(train_predict_prob)\n",
    "    plot_inf.append(test_predict_prob)\n",
    "    train_auc = roc_auc_score(train_y, train_predict_prob)\n",
    "    test_auc = roc_auc_score(test_y, test_predict_prob)\n",
    "    result.append(train_auc)\n",
    "    result.append(test_auc)\n",
    "    \n",
    "    return result, plot_inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['train_accuracy', 'test_accuracy', 'train_precision', 'test_precision', \n",
    "       'train_recall', 'test_recall', 'train_f1', \n",
    "       'test_f1', 'train_auc', 'test_auc']\n",
    "inf = list();\n",
    "supervised_result = pd.DataFrame(columns=col, index=range(30))\n",
    "for i in range(30):\n",
    "    res, plot_inf = supersived_learning()\n",
    "    inf = plot_inf\n",
    "    supervised_result.loc[i] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of Supersived Learning:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "train_accuracy     0.982125\n",
       "test_accuracy      0.968129\n",
       "train_precision    0.984666\n",
       "test_precision     0.961925\n",
       "train_recall       0.967255\n",
       "test_recall        0.952381\n",
       "train_f1           0.975860\n",
       "test_f1            0.956516\n",
       "train_auc          0.998385\n",
       "test_auc           0.994907\n",
       "dtype: float64"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The result of Supersived Learning:\")\n",
    "supervised_result.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix and ROC\n",
    "#use cv to find best param\n",
    "normalize_train_x, train_y, normalize_test_x, test_y = split_dataset(normalize_dataset_x, dataset_y, \n",
    "                                                     test_size = 0.2,random = None)\n",
    "parameters = {'C': np.power(10, np.arange(-3, 7, 0.5))}\n",
    "svc = LinearSVC(penalty='l1', dual=False)\n",
    "clf = GridSearchCV(svc, parameters, cv=5)\n",
    "clf.fit(normalize_train_x, train_y)\n",
    "best_c = clf.best_params_['C']\n",
    "    \n",
    "#build model with best C\n",
    "svc = LinearSVC(penalty='l1', dual=False, C = best_c)\n",
    "svc.fit(normalize_train_x, train_y)\n",
    "\n",
    "# confusion matrix\n",
    "train_predict = svc.predict(normalize_train_x)\n",
    "test_predict = svc.predict(normalize_test_x)\n",
    "train_cm = pd.DataFrame(confusion_matrix(train_y, train_predict), columns= [\"Predict No\", \"Predict Yes\"],\n",
    "                        index = [\"Actual No\", \"Actual Yes\"] )\n",
    "test_cm = pd.DataFrame(confusion_matrix(test_y, test_predict), columns= [\"Predict No\", \"Predict Yes\"],\n",
    "                        index = [\"Actual No\", \"Actual Yes\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predict No</th>\n",
       "      <th>Predict Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Actual No</td>\n",
       "      <td>281</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Actual Yes</td>\n",
       "      <td>8</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Predict No  Predict Yes\n",
       "Actual No          281            4\n",
       "Actual Yes           8          162"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predict No</th>\n",
       "      <th>Predict Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Actual No</td>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Actual Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Predict No  Predict Yes\n",
       "Actual No           70            2\n",
       "Actual Yes           1           41"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predict_prob = svc.decision_function(normalize_train_x)\n",
    "test_predict_prob = svc.decision_function(normalize_test_x)\n",
    "train_fpr, train_tpr, train_thresholds = roc_curve(train_y, train_predict_prob)\n",
    "test_fpr, test_tpr, test_thresholds = roc_curve(test_y, test_predict_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_fpr, train_tpr, label='train ROC curve')\n",
    "plt.plot(test_fpr, test_tpr, label='test ROC curve')\n",
    "plt.title('ROC curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. Semi-Supervised Learning/ Self-training: \n",
    "    select 50% of the positive class along with 50% of the negative class in the training set as labeled data and the rest as unlabelled data. You can select them randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Train an L1-penalized SVM to classify the labeled data Use normalized data. Choose the penalty parameter using 5 fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types;\n",
    "normalize_train_x, train_y, normalize_test_x, test_y = split_dataset(normalize_dataset_x, dataset_y, \n",
    "                                                     test_size = 0.2,random = None)\n",
    "# select 50% of the positive class along with 50% of the negative class in the training set as labeled data \n",
    "label_train_x, label_train_y, unlabel_train_x, unlabel_train_y = split_dataset(normalize_train_x, train_y, \n",
    "                                                     test_size = 0.5 ,random = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=100000.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "          multi_class='ovr', penalty='l1', random_state=None, tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bulid model use labeled training data\n",
    "parameters = {'C': np.power(10, np.arange(-3, 7, 0.5))}\n",
    "svc = LinearSVC(penalty='l1', dual=False)\n",
    "clf = GridSearchCV(svc, parameters, cv=5)\n",
    "clf.fit(label_train_x, label_train_y)\n",
    "best_c = clf.best_params_['C']\n",
    "    \n",
    "#build model with best C\n",
    "svc = LinearSVC(penalty='l1', dual=False, C = best_c)\n",
    "svc.fit(label_train_x, label_train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Find the unlabeled data point that is the farthest to the decision boundary of the SVM. Let the SVM label it (ignore its true label), and add it to the labeled data, and retrain the SVM. Continue this process until all unlabeled data are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semi_supersived_learning():\n",
    "    result = list()\n",
    "    plot_inf = list()\n",
    "    #split dataset randomly\n",
    "    normalize_train_x, train_y, normalize_test_x, test_y = split_dataset(normalize_dataset_x, dataset_y, \n",
    "                                                     test_size = 0.2,random = None)\n",
    "    # select 50% of the positive class along with 50% of the negative class in the training set as labeled data \n",
    "    label_train_x, label_train_y, unlabel_train_x, unlabel_train_y = split_dataset(normalize_train_x, train_y, \n",
    "                                                     test_size = 0.5 ,random = None)\n",
    "    #bulid model use labeled training data\n",
    "    parameters = {'C': np.power(10, np.arange(-3, 7, 0.5))}\n",
    "    svc = LinearSVC(penalty='l1', dual=False)\n",
    "    clf = GridSearchCV(svc, parameters, cv=5)\n",
    "    clf.fit(label_train_x, label_train_y)\n",
    "    best_c = clf.best_params_['C']\n",
    "    \n",
    "    #build model with best C\n",
    "    svc = LinearSVC(penalty='l1', dual=False, C = best_c)\n",
    "    svc.fit(label_train_x, label_train_y)\n",
    "    \n",
    "    # add unlabel data into train data, retrain SVM\n",
    "    while len(unlabel_train_x) > 0:\n",
    "\n",
    "        unlabel_predict_prob = svc.decision_function(unlabel_train_x)# calculate abs distance\n",
    "        abs_prob = np.abs(svc.decision_function(unlabel_train_x))\n",
    "        max_idx = abs_prob.argmax() # the index which has max absolute value\n",
    "        label = int(unlabel_predict_prob[max_idx] > 0) #label of this data point\n",
    "        label_train_x = np.vstack([label_train_x, unlabel_train_x.iloc[max_idx,:]]) #add to training set\n",
    "        label_train_y = np.append(label_train_y, label)\n",
    "        unlabel_train_x = unlabel_train_x.drop(unlabel_train_x.index[max_idx]) #delete from unlabeled data\n",
    "        svc.fit(label_train_x, label_train_y) # refit the model\n",
    "        \n",
    "\n",
    "    #Accuracy\n",
    "    train_accuracy = svc.score(normalize_train_x, train_y)\n",
    "    test_accuracy = svc.score(normalize_test_x, test_y)\n",
    "    result.append(train_accuracy)\n",
    "    result.append(test_accuracy)\n",
    "    \n",
    "    # confusion matrix\n",
    "    train_predict = svc.predict(normalize_train_x)\n",
    "    test_predict = svc.predict(normalize_test_x)\n",
    "    train_cm = confusion_matrix(train_y, train_predict)\n",
    "    test_cm = confusion_matrix(test_y, test_predict)\n",
    "    plot_inf.append(train_cm)\n",
    "    plot_inf.append(test_cm)\n",
    "    \n",
    "    #precision\n",
    "    train_tn, test_tn = train_cm[0][0], test_cm[0][0]; \n",
    "    train_fp, test_fp = train_cm[0][1], test_cm[0][1]; \n",
    "    train_fn, test_fn = train_cm[1][0], test_cm[1][0]; \n",
    "    train_tp, test_tp = train_cm[1][1], test_cm[1][1];\n",
    "    train_precision = train_tp/(train_tp + train_fp)\n",
    "    test_precision = test_tp/(test_tp + test_fp)\n",
    "    result.append(train_precision)\n",
    "    result.append(test_precision)\n",
    "    \n",
    "    #recall\n",
    "    train_recall = train_tp/(train_tp + train_fn)\n",
    "    test_recall = test_tp/(test_tp + test_fn)\n",
    "    result.append(train_recall)\n",
    "    result.append(test_recall)\n",
    "    \n",
    "    #F-score\n",
    "    train_f1 = 2 * (train_precision * train_recall) / (train_precision + train_recall)\n",
    "    test_f1 = 2 * (test_precision * test_recall) / (test_precision + test_recall)\n",
    "    result.append(train_f1)\n",
    "    result.append(test_f1)\n",
    "    \n",
    "    # AUC\n",
    "    train_predict_prob = svc.decision_function(normalize_train_x)\n",
    "    test_predict_prob = svc.decision_function(normalize_test_x)\n",
    "    plot_inf.append(train_predict_prob)\n",
    "    plot_inf.append(test_predict_prob)\n",
    "    train_auc = roc_auc_score(train_y, train_predict_prob)\n",
    "    test_auc = roc_auc_score(test_y, test_predict_prob)\n",
    "    result.append(train_auc)\n",
    "    result.append(test_auc)\n",
    "    \n",
    "    return result, plot_inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['train_accuracy', 'test_accuracy', 'train_precision', 'test_precision', \n",
    "       'train_recall', 'test_recall', 'train_f1', \n",
    "       'test_f1', 'train_auc', 'test_auc']\n",
    "semi_supervised_result = pd.DataFrame(columns=col, index=range(30))\n",
    "for i in range(30):\n",
    "    res, plot_inf = semi_supersived_learning()\n",
    "    semi_supervised_result.loc[i] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of Semi-Supersived Learning:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "train_accuracy     0.969011\n",
       "test_accuracy      0.952924\n",
       "train_precision    0.967454\n",
       "test_precision     0.948441\n",
       "train_recall       0.949412\n",
       "test_recall        0.923810\n",
       "train_f1           0.958087\n",
       "test_f1            0.934980\n",
       "train_auc          0.992888\n",
       "test_auc           0.987511\n",
       "dtype: float64"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The result of Semi-Supersived Learning:\")\n",
    "semi_supervised_result.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset randomly\n",
    "normalize_train_x, train_y, normalize_test_x, test_y = split_dataset(normalize_dataset_x, dataset_y, \n",
    "                                                     test_size = 0.2,random = None)\n",
    "    # select 50% of the positive class along with 50% of the negative class in the training set as labeled data \n",
    "label_train_x, label_train_y, unlabel_train_x, unlabel_train_y = split_dataset(normalize_train_x, train_y, \n",
    "                                                     test_size = 0.5 ,random = None)\n",
    "    #bulid model use labeled training data\n",
    "parameters = {'C': np.power(10, np.arange(-1, 5, 0.5))}\n",
    "svc = LinearSVC(penalty='l1', dual=False)\n",
    "clf = GridSearchCV(svc, parameters, cv=5)\n",
    "clf.fit(label_train_x, label_train_y)\n",
    "best_c = clf.best_params_['C']\n",
    "    \n",
    "    #build model with best C\n",
    "svc = LinearSVC(penalty='l1', dual=False, C = best_c)\n",
    "svc.fit(label_train_x, label_train_y)\n",
    "    \n",
    "    # add unlabel data into train data, retrain SVM\n",
    "while len(unlabel_train_x) > 0:\n",
    "\n",
    "    unlabel_predict_prob = svc.decision_function(unlabel_train_x)# calculate abs distance\n",
    "    abs_prob = np.abs(svc.decision_function(unlabel_train_x))\n",
    "    max_idx = abs_prob.argmax() # the index which has max absolute value\n",
    "    label = int(unlabel_predict_prob[max_idx] > 0) #label of this data point\n",
    "    label_train_x = np.vstack([label_train_x, unlabel_train_x.iloc[max_idx,:]]) #add to training set\n",
    "    label_train_y = np.append(label_train_y, label)\n",
    "    unlabel_train_x = unlabel_train_x.drop(unlabel_train_x.index[max_idx]) #delete from unlabeled data\n",
    "    svc.fit(label_train_x, label_train_y) # refit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# confusion matrix\n",
    "train_predict = svc.predict(normalize_train_x)\n",
    "test_predict = svc.predict(normalize_test_x)\n",
    "train_cm = pd.DataFrame(confusion_matrix(train_y, train_predict), columns= [\"Predict No\", \"Predict Yes\"],\n",
    "                        index = [\"Actual No\", \"Actual Yes\"] )\n",
    "test_cm = pd.DataFrame(confusion_matrix(test_y, test_predict), columns= [\"Predict No\", \"Predict Yes\"],\n",
    "                        index = [\"Actual No\", \"Actual Yes\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predict No</th>\n",
       "      <th>Predict Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Actual No</td>\n",
       "      <td>282</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Actual Yes</td>\n",
       "      <td>8</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Predict No  Predict Yes\n",
       "Actual No          282            3\n",
       "Actual Yes           8          162"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predict No</th>\n",
       "      <th>Predict Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Actual No</td>\n",
       "      <td>68</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Actual Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Predict No  Predict Yes\n",
       "Actual No           68            4\n",
       "Actual Yes           1           41"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3daZgV1bn28f8tgojiwOCJgsiooVFptUXRROGoCBgRjUecguaoGMfEBN9oYowafGPUmASjJkQNxoOKs0SJGj0OiQIKyiAQmcTQTiAqIgoyPOdDFW3T9LAbeu+mu+7fdfXlrqq1Vz3VjfvZa62qtRQRmJlZdm1V3wGYmVn9ciIwM8s4JwIzs4xzIjAzyzgnAjOzjHMiMDPLOCcCM7OMcyKwRkfSQklfSPpM0vuSRkvavkKZQyT9r6TlkpZJ+qukogpldpD0W0n/Tuual263KewVmeWXE4E1VsdGxPZAMbAfcPn6A5J6A08DjwG7AZ2AacBLkjqnZZoBzwI9gP7ADsAhwFKgV76ClrR1vuo2q4oTgTVqEfE+8BRJQljveuAvEfG7iFgeER9FxBXAROCqtMxQoANwfETMioh1EbE4In4REeMrO5ekHpL+LukjSR9I+km6f7SkEeXK9ZFUWm57oaQfS5oOrJB0haQHK9T9O0kj09c7SrpD0nuS3pE0QlKTzfxVWYY5EVijJqk9MACYl263IPlm/0Alxe8HjkpfHwk8GRGf5XielsAzwJMkrYyuJC2KXJ0CHAPsBNwNDJS0Q1p3E+Ak4J607F3AmvQc+wH9gLNrcS6zDTgRWGP1qKTlwCJgMfDzdH8rkn/371XynveA9f3/rasoU5VvAe9HxK8jYmXa0phUi/ePjIhFEfFFRLwNvAYMTo/9J/B5REyU9B8kie0HEbEiIhYDvwFOrsW5zDbgRGCN1eCIaAn0Ab7OVx/wHwPrgF0rec+uwIfp66VVlKnK7sD8TYo0sajC9j0krQSAU/mqNbAH0BR4T9Inkj4B/gjsshnntoxzIrBGLSJeAEYDN6bbK4AJwH9VUvwkvurOeQY4WtJ2OZ5qEdClimMrgBbltr9WWagVth8A+qRdW8fzVSJYBKwC2kTETunPDhHRI8c4zTbiRGBZ8FvgKEnrB4wvA86QdLGklpJ2TgdzewNXp2XuJvnQfUjS1yVtJam1pJ9IGljJOR4HvibpB5K2Ses9KD02laTPv5WkrwE/qCngiFgCPA/8GXgrIman+98juePp1+ntrVtJ6iLp8E34vZgBTgSWAemH6l+An6Xb/wSOBk4gGQd4m2TQ9RsRMTcts4pkwPhfwN+BT4FXSLqYNur7j4jlJAPNxwLvA3OBvunhu0luT11I8iE+NsfQ70ljuKfC/qFAM2AWSVfXg9SuG8tsA/LCNGZm2eYWgZlZxjkRmJllnBOBmVnGORGYmWVcg5vgqk2bNtGxY8f6DsPMrEGZMmXKhxHRtrJjDS4RdOzYkcmTJ9d3GGZmDYqkt6s65q4hM7OMcyIwM8s4JwIzs4xzIjAzyzgnAjOzjMtbIpB0p6TFkt6o4rgkjUwXBJ8uaf98xWJmZlXLZ4tgNMmi31UZAHRLf4YBt+UxFjMzq0LeniOIiBcldaymyHEkC4gHMFHSTpJ2Tedb3zJN/jPMeLDmcrbJPli+kg8/W1XfYZhtkZbv1J2Dz/9Tnddbnw+UtWPD5flK030bJQJJw0haDXTo0KEgwVVqxoPw/gz42j61fqs/4HKzfOUaAFo2b3DPOpo1WPX5f5sq2Vfp4ggRMQoYBVBSUlK/Cyh8bR/47hO1ftvFf5zArI8+pWjXHfIQVONyXHE7Tj2oHhO+WcbUZyIoJVnwe732wLv1FEut3TPp3zw29Z2cy896L0kCY8/tnceozMxqrz4TwTjgQkn3AQcBy+p7fKCmD/crly4D4Jo/TmDSWx8BcFCnVjnVXbTrDhxX3G7zgzQzq2N5SwSS7gX6AG0klQI/B5oCRMQfgPHAQGAe8Dnw3XzFAtQ40PvB8pV0/nAFl1B1/3TH1QtY2LQzkCQAd2GYWWOQz7uGTqnheAAX5Ov8G6lioHf9IO76QcpObbbjP1o2r6KS/eixz4mMLXH3jpk1Htm6NaOSgd6yQdx2SdfNwf6Gb2YZk61EkCo/FuBBXDPLukzONfTY1HeY9d6ngAdxzcwy1yK4Z9K/mfTWRxzUqZVbAWZmZLBFsL5LyK0AM7NEZloE6+8OmvXlpxzUqZVv+zQzS2WmRfDhZ6v4/Mu1HhMwM6sgMy0CgBbNmnhcwMysgsy0CMzMrHJOBGZmGedEYGaWcU4EZmYZ50RgZpZxTgRmZhnnRGBmlnFOBGZmGedEYGaWcU4EZmYZ50RgZpZxTgRmZhnnRGBmlnFOBGZmGedEYGaWcU4EZmYZ50RgZpZxTgRmZhnnRGBmlnFOBGZmGedEYGaWcU4EZmYZ50RgZpZxeU0EkvpLelPSPEmXVXK8g6TnJL0uabqkgfmMx8zMNpa3RCCpCXALMAAoAk6RVFSh2BXA/RGxH3AycGu+4jEzs8rls0XQC5gXEQsi4kvgPuC4CmUC2CF9vSPwbh7jMTOzSuQzEbQDFpXbLk33lXcVcLqkUmA8cFFlFUkaJmmypMlLlizJR6xmZpmVz0SgSvZFhe1TgNER0R4YCNwtaaOYImJURJREREnbtm3zEKqZWXblMxGUAruX227Pxl0/ZwH3A0TEBKA50CaPMZmZWQX5TASvAt0kdZLUjGQweFyFMv8GjgCQ1J0kEbjvx8ysgPKWCCJiDXAh8BQwm+TuoJmSrpE0KC32I+AcSdOAe4EzI6Ji95GZmeXR1vmsPCLGkwwCl993ZbnXs4BD8xmDmZlVz08Wm5llnBOBmVnGORGYmWWcE4GZWcY5EZiZZZwTgZlZxjkRmJllnBOBmVnGORGYmWVcTolAUjNJXfMdjJmZFV6NiUDSMcAM4O/pdrGkR/IdmJmZFUYuLYJrgIOATwAiYirg1oGZWSORSyJYHRGfVNjnGULNzBqJXGYfnS3pJGArSZ2A7wMT8xuWmZkVSi4tgguBA4B1wMPASpJkYGZmjUAuLYKjI+LHwI/X75B0AklSMDOzBi6XFsEVlez7aV0HYmZm9aPKFoGko4H+QDtJN5U7tANJN5GZmTUC1XUNLQbeIBkTmFlu/3LgsnwGZWZmhVNlIoiI14HXJY2JiJUFjMnMzAool8HidpKuBYqA5ut3RsSeeYvKzMwKJpfB4tHAnwEBA4D7gfvyGJOZmRVQLomgRUQ8BRAR8yPiCqBvfsMyM7NCyaVraJUkAfMlfQ94B9glv2GZmVmh5JIILgG2By4GrgV2BP47n0GZmVnh1JgIImJS+nI58B0ASe3zGZSZmRVOtWMEkg6UNFhSm3S7h6S/4EnnzMwajSoTgaRfAmOA04AnJf0UeA6YBvjWUTOzRqK6rqHjgJ4R8YWkVsC76fabhQnNzMwKobquoZUR8QVARHwE/MtJwMys8amuRdBZ0vqppgV0LLdNRJxQU+WS+gO/A5oAt0fEdZWUOQm4imTVs2kRcWru4ZuZ2eaqLhF8u8L272tTsaQmwC3AUUAp8KqkcRExq1yZbsDlwKER8bEkP59gZlZg1U069+xm1t0LmBcRCwAk3Ucy7jCrXJlzgFsi4uP0nIs385xmZlZLuUwxsanaAYvKbZem+8rbE9hT0kuSJqZdSRuRNEzSZEmTlyxZkqdwzcyyKZ+JQJXsiwrbWwPdgD7AKcDtknba6E0RoyKiJCJK2rZtW+eBmpllWc6JQNI2tay7FNi93HZ7kltQK5Z5LCJWR8RbwJskicHMzAqkxkQgqZekGcDcdLunpJtzqPtVoJukTpKaAScD4yqUeZR0JtP06eU9gQW1iN/MzDZTLi2CkcC3gKUAETGNHKahjog1wIXAU8Bs4P6ImCnpGkmD0mJPAUslzSJ5avnSiFha+8swM7NNlcvso1tFxNvJTNRl1uZSeUSMB8ZX2HdludcB/DD9MTOzepBLIlgkqRcQ6bMBFwFz8huWmZkVSi5dQ+eRfGPvAHwAHJzuMzOzRiCXFsGaiDg575GYmVm9yKVF8Kqk8ZLOkNQy7xGZmVlB1ZgIIqILMAI4AJgh6VFJbiGYmTUSOT1QFhEvR8TFwP7ApyQL1piZWSOQywNl20s6TdJfgVeAJcAheY/MzMwKIpfB4jeAvwLXR8Q/8hyPmZkVWC6JoHNErMt7JGZmVi+qTASSfh0RPwIeklRx1tCcVigzM7MtX3UtgrHpf2u1MpmZmTUs1a1Q9kr6sntEbJAMJF0IbO4KZmZmtgXI5fbR/65k31l1HYiZmdWP6sYIhpCsIdBJ0sPlDrUEPsl3YGZmVhjVjRG8QrIGQXvglnL7lwOv5zMoMzMrnOrGCN4C3gKeKVw4ZmZWaNV1Db0QEYdL+pgNF50XyZoyrfIenZmZ5V11XUPrl6NsU4hAzMysflR511C5p4l3B5pExFqgN3AusF0BYjMzswLI5fbRR0mWqewC/AXoDtyT16jMzKxgckkE6yJiNXAC8NuIuAhol9+wzMysUHJJBGsk/RfwHeDxdF/T/IVkZmaFlOuTxX1JpqFeIKkTcG9+wzIzs0KpcRrqiHhD0sVAV0lfB+ZFxLX5D83MzAqhxkQg6ZvA3cA7JM8QfE3SdyLipXwHZ2Zm+ZfLwjS/AQZGxCwASd1JEkNJPgMzM7PCyGWMoNn6JAAQEbOBZvkLyczMCimXFsFrkv5I0goAOA1POmdm1mjkkgi+B1wM/D+SMYIXgZvzGZSZmRVOtYlA0j5AF+CRiLi+MCGZmVkhVTlGIOknJNNLnAb8XVJlK5WZmVkDV91g8WnAvhHxX8CBwHm1rVxSf0lvSpon6bJqyp0oKST5TiQzswKrLhGsiogVABGxpIayG5HUhGRlswFAEXCKpKJKyrUkGYOYVJv6zcysblQ3RtC53FrFArqUX7s4Ik6ooe5eJE8hLwCQdB9wHDCrQrlfANcDw2sTuJmZ1Y3qEsG3K2z/vpZ1twMWldsuBQ4qX0DSfsDuEfG4pCoTgaRhwDCADh061DIMMzOrTnVrFj+7mXWrsmrLDkpbkTy1fGZNFUXEKGAUQElJSdRQ3MzMaqFW/f61VEqyutl67YF3y223BPYGnpe0EDgYGOcBYzOzwspnIngV6Capk6RmwMnAuPUHI2JZRLSJiI4R0RGYCAyKiMl5jMnMzCrIORFI2qY2FUfEGuBC4ClgNnB/RMyUdI2kQbUL08zM8iWXaah7AXcAOwIdJPUEzk6XrKxWRIwHxlfYd2UVZfvkErCZmdWtXFoEI4FvAUsBImIayYplZmbWCOSSCLaKiLcr7Fubj2DMzKzwcpl9dFHaPRTp08IXAXPyG5aZmRVKLi2C84AfAh2AD0hu86z1vENmZrZlymXx+sUkt36amVkjlMtdQ3+i3BPB60XEsLxEZGZmBZXLGMEz5V43B45nwzmEzMysAcula2hs+W1JdwN/z1tEZmZWUJsyxUQnYI+6DsTMzOpHLmMEH/PVGMFWwEdAlauNmZlZw1LT4vUCegLvpLvWRYSngTYza0Sq7RpKP/QfiYi16Y+TgJlZI5PLGMErkvbPeyRmZlYvquwakrR1OpX0N4BzJM0HVpCsPBYR4eRgZtYIVDdG8AqwPzC4QLGYmVk9qC4RCCAi5hcoFjMzqwfVJYK2kn5Y1cGIuCkP8ZiZWYFVlwiaANuTtgzMzKxxqi4RvBcR1xQsEjMzqxfV3T7qloCZWQZUlwiOKFgUZmZWb6pMBBHxUSEDMTOz+rEps4+amVkj4kRgZpZxTgRmZhnnRGBmlnFOBGZmGedEYGaWcU4EZmYZ50RgZpZxeU0EkvpLelPSPEkbLXgv6YeSZkmaLulZSXvkMx4zM9tY3hKBpCbALcAAoAg4RVJRhWKvAyURsS/wIHB9vuIxM7PK5bNF0AuYFxELIuJL4D7guPIFIuK5iPg83ZwItM9jPGZmVol8JoJ2wKJy26XpvqqcBfytsgOShkmaLGnykiVL6jBEMzPLZyKobBrrqLSgdDpQAtxQ2fGIGBURJRFR0rZt2zoM0czMqluYZnOVAruX224PvFuxkKQjgZ8Ch0fEqjzGY2Zmlchni+BVoJukTpKaAScD48oXkLQf8EdgUEQszmMsZmZWhbwlgohYA1wIPAXMBu6PiJmSrpE0KC12A8m6yA9ImippXBXVmZlZnuSza4iIGA+Mr7DvynKvj8zn+c3MrGZ+stjMLOOcCMzMMs6JwMws45wIzMwyzonAzCzjnAjMzDIur7ePmlnDsXr1akpLS1m5cmV9h2KboXnz5rRv356mTZvm/B4nAjMDoLS0lJYtW9KxY0ekyqYKsy1dRLB06VJKS0vp1KlTzu9z15CZAbBy5Upat27tJNCASaJ169a1btU5EZhZGSeBhm9T/oZOBGZmGedEYGZbhE8++YRbb711k947cOBAPvnkk5zLX3XVVbRr147i4mKKioq49957y45FBCNGjKBbt27sueee9O3bl5kzZ5Yd/+yzzzj33HPp0qULPXr04LDDDmPSpEmbFPeWwonAzLYI1SWCtWvXVvve8ePHs9NOO9XqfJdccglTp07lscce49xzz2X16tUA3HLLLbz88stMmzaNOXPmcPnllzNo0KCyfvezzz6bVq1aMXfuXGbOnMno0aP58MMPa3Xu6kQE69atq7P6cuG7hsxsI1f/dSaz3v20Tuss2m0Hfn5sjyqPX3bZZcyfP5/i4mKOOuoojjnmGK6++mp23XVXpk6dyqxZsxg8eDCLFi1i5cqVfP/732fYsGEAdOzYkcmTJ/PZZ58xYMAAvvGNb/Dyyy/Trl07HnvsMbbddtsqz9utWzdatGjBxx9/zC677MKvfvUrnn/+eVq0aAFAv379OOSQQxgzZgx9+vRh0qRJjBkzhq22Sr5Hd+7cmc6dO29U75NPPslPfvIT1q5dS5s2bXj22We56qqr2H777Rk+fDgAe++9N48//jgAAwYMoG/fvkyYMIHBgwezYsUKrr/+egBGjx7NlClTuPnmm/mf//kfRo4cyZdffslBBx3ErbfeSpMmTTbhL/IVtwjMbItw3XXX0aVLF6ZOncoNNySr1r7yyitce+21zJo1C4A777yTKVOmMHnyZEaOHMnSpUs3qmfu3LlccMEFzJw5k5122omHHnqo2vO+9tprdOvWjV122YVPP/2UFStW0KVLlw3KlJSUMHPmTGbOnElxcXGNH7xLlizhnHPO4aGHHmLatGk88MADNV7/m2++ydChQ3n99dc5//zzefjhh8uOjR07liFDhjB79mzGjh3LSy+9xNSpU2nSpAljxoypse6auEVgZhup7pt7IfXq1WuD++FHjhzJI488AsCiRYuYO3curVu33uA9nTp1ori4GIADDjiAhQsXVlr3b37zG/70pz+xYMECnnzyyWrjiIha3Y0zceJEDjvssLLYW7VqVeN79thjDw4++GAA2rZtS+fOnZk4cSLdunXjzTff5NBDD+WWW25hypQpHHjggQB88cUX7LLLLjnHVRUnAjPbYm233XZlr59//nmeeeYZJkyYQIsWLejTp0+l98tvs802Za+bNGnCF198UWndl1xyCcOHD+fhhx9m6NChzJ8/nx122IHtttuOBQsWbNDd89prr3H44YfTo0cPpk2bxrp168q6hipTVeLYeuutN+j/Lx9/+WsFGDJkCPfffz9f//rXOf7445FERHDGGWfwy1/+sspzbwp3DZnZFqFly5YsX768yuPLli1j5513pkWLFvzrX/9i4sSJdXLeE044gZKSEu666y4ALr30Ui6++OKyBPLMM8/wz3/+k1NPPZUuXbpQUlLCz3/+cyICSLqiHnvssQ3q7N27Ny+88AJvvfUWAB999BGQjGW89tprQJJc1h+vKq5HH32Ue++9lyFDhgBwxBFH8OCDD7J48eKyet9+++3N/h04EZjZFqF169Yceuih7L333lx66aUbHe/fvz9r1qxh33335Wc/+1lZN0pduPLKK7nppptYt24dF110EQceeCD77LMPe+21F7/4xS82GHC+/fbbef/99+natSv77LMP55xzDrvtttsG9bVt25ZRo0Zxwgkn0LNnz7IP8m9/+9t89NFHFBcXc9ttt7HnnntWGdPOO+9MUVERb7/9Nr169QKgqKiIESNG0K9fP/bdd1+OOuoo3nvvvc2+fq3Pag1FSUlJTJ48udbvm/n/vwFAj5/8s65DMmsUZs+eTffu3es7DKsDlf0tJU2JiJLKyrtFYGaWcU4EZmYZ50RgZpZxTgRmZhnnRGBmlnFOBGZmGedEYGZbhM2Zhhrgt7/9LZ9//nmlx/r06cNee+1Fz549OfDAA5k6dWrZsWXLljF06FC6dOlCly5dGDp0KMuWLSs7PmfOHAYOHEjXrl3p3r07J510Eh988MEmx7klciIwsy1CPhMBwJgxY5g2bRrnn3/+Bg+snXXWWXTu3Jn58+czf/58OnXqxNlnnw0kU0Acc8wxnHfeecybN4/Zs2dz3nnnsWTJkk2Os6I1a9bUWV2bynMNmdnG/nYZvD+jbuv82j4w4LoqD1echvqGG27ghhtu4P7772fVqlUcf/zxXH311axYsYKTTjqJ0tJS1q5dy89+9jM++OAD3n33Xfr27UubNm147rnnqjxP7969y2Y3nTdvHlOmTGHs2LFlx6+88kq6du3K/PnzeeGFF+jduzfHHnts2fG+fftWWu/111/P3XffzVZbbcWAAQO47rrr6NOnDzfeeCMlJSV8+OGHlJSUsHDhQkaPHs0TTzzBypUrWbFiBW3btuWMM85g4MCBAJx55pkce+yxDB48mMsuu4znn3+eVatWccEFF3DuuefW6teeCycCM9siXHfddbzxxhtl3TZPP/00c+fO5ZVXXiEiGDRoEC+++CJLlixht91244knngCSrp0dd9yRm266ieeee442bdpUe54nn3ySwYMHAzBr1qyNppVu0qQJxcXFzJw5kzfeeIMDDjigxtj/9re/8eijjzJp0iRatGhRNrdQdSZMmMD06dNp1aoVjzzyCGPHjmXgwIF8+eWXPPvss9x2223ccccd7Ljjjrz66qusWrWKQw89lH79+m0wI2tdcCIws41V8829UJ5++mmefvpp9ttvPyBZInLu3Ll885vfZPjw4fz4xz/mW9/6Ft/85jdzqu+0005jxYoVrF27tmzit6pmCa3ttNPPPPMM3/3ud8sWs8ll2umjjjqqrNyAAQO4+OKLWbVqFU8++SSHHXYY2267LU8//TTTp0/nwQcfBJKkN3fu3IaVCCT1B34HNAFuj4jrKhzfBvgLcACwFBgSEQvzGZOZNQwRweWXX15pV8iUKVMYP348l19+Of369ePKK6+ssb4xY8bQs2dPLrvsMi644AIefvhhevToweuvv77BtNLr1q1j2rRpdO/encWLF/PCCy/kFGtN005XnDK7/LTTzZs3p0+fPjz11FOMHTuWU045pazem2++maOPPrrGGDZH3gaLJTUBbgEGAEXAKZKKKhQ7C/g4IroCvwF+la94zGzLVnEa6qOPPpo777yTzz77DIB33nmHxYsX8+6779KiRQtOP/10hg8fXvbtvqZprAGaNm3KiBEjmDhxIrNnz6Zr167st99+jBgxoqzMiBEj2H///enatSunnnoqL7/8clk3FCRdSzNmbDh+0q9fP+68886ywery005PmTIFoOxbfVVOPvlk/vznP/OPf/yj7IP/6KOP5rbbbitbT3nOnDmsWLGi2no2RT7vGuoFzIuIBRHxJXAfcFyFMscBd6WvHwSOUG3aY2bWaFSchrpfv36ceuqp9O7dm3322YcTTzyR5cuXM2PGDHr16kVxcTHXXnstV1xxBQDDhg0rW/e3Ottuuy0/+tGPuPHGGwG44447mDNnDl27dqVLly7MmTOHO+64o6zs448/zs0330y3bt0oKipi9OjRG60K1r9/fwYNGkRJSQnFxcVldQ8fPpzbbruNQw45pMYF7vv168eLL77IkUceSbNmzQA4++yzKSoqYv/992fvvffm3HPPzctdRnmbhlrSiUD/iDg73f4OcFBEXFiuzBtpmdJ0e35a5sMKdQ0DhgF06NDhgE1ZiGHirecAcPD5f9qk6zFr7DwNdeNR22mo8zlGUNk3+4pZJ5cyRMQoYBQk6xFsSjBOAGZmlctn11ApsHu57fbAu1WVkbQ1sCNQ831XZmZWZ/KZCF4FuknqJKkZcDIwrkKZccAZ6esTgf+NhrZkmlkj4v/9Gr5N+RvmLRFExBrgQuApYDZwf0TMlHSNpEFpsTuA1pLmAT8ELstXPGZWvebNm7N06VIngwYsIli6dCnNmzev1fsys2axmVVv9erVlJaWbnS/uzUszZs3p3379jRt2nSD/fU1WGxmDUjTpk3r/IlVaxg8+6iZWcY5EZiZZZwTgZlZxjW4wWJJS4DaP1qcaANU/5x34+NrzgZfczZszjXvERFtKzvQ4BLB5pA0uapR88bK15wNvuZsyNc1u2vIzCzjnAjMzDIua4lgVH0HUA98zdnga86GvFxzpsYIzMxsY1lrEZiZWQVOBGZmGdcoE4Gk/pLelDRP0kYzmkraRtLY9PgkSR0LH2XdyuGafyhplqTpkp6VtEd9xFmXarrmcuVOlBSSGvythrlcs6ST0r/1TEn3FDrGupbDv+0Okp6T9Hr673tgfcRZVyTdKWlxuoJjZcclaWT6+5guaf/NPmlENKofoAkwH+gMNAOmAUUVypwP/CF9fTIwtr7jLsA19wVapK/Py8I1p+VaAi8CE4GS+o67AH/nbsDrwM7p9i71HXcBrnkUcF76ughYWN9xb+Y1HwbsD7xRxfGBwN9IVng8GJi0uedsjC2CXsC8iFgQEV8C9wHHVShzHHBX+vpB4AhJlS2b2VDUeM0R8VxEfJ5uTiRZMa4hy+XvDPAL4HqgMcytnMs1nwPcEhEfA0TE4gLHWNdyueYAdkhf78jGKyE2KBHxItWv1Hgc8JdITAR2krTr5pyzMSaCdsCictul6b5Ky0SygM4yoHVBosuPXK65vLNIvlE0ZDVes6T9gN0j4t6GFoUAAAWtSURBVPFCBpZHufyd9wT2lPSSpImS+hcsuvzI5ZqvAk6XVAqMBy4qTGj1prb/v9eoMa5HUNk3+4r3yOZSpiHJ+XoknQ6UAIfnNaL8q/aaJW0F/AY4s1ABFUAuf+etSbqH+pC0+v4hae+I+CTPseVLLtd8CjA6In4tqTdwd3rN6/IfXr2o88+vxtgiKAV2L7fdno2bimVlJG1N0pysrim2pcvlmpF0JPBTYFBErCpQbPlS0zW3BPYGnpe0kKQvdVwDHzDO9d/2YxGxOiLeAt4kSQwNVS7XfBZwP0BETACak0zO1ljl9P97bTTGRPAq0E1SJ0nNSAaDx1UoMw44I319IvC/kY7CNFA1XnPaTfJHkiTQ0PuNoYZrjohlEdEmIjpGREeScZFBEdGQ1znN5d/2oyQ3BiCpDUlX0YKCRlm3crnmfwNHAEjqTpIIlhQ0ysIaBwxN7x46GFgWEe9tToWNrmsoItZIuhB4iuSOgzsjYqaka4DJETEOuIOk+TiPpCVwcv1FvPlyvOYbgO2BB9Jx8X9HxKB6C3oz5XjNjUqO1/wU0E/SLGAtcGlELK2/qDdPjtf8I+BPki4h6SI5syF/sZN0L0nXXpt03OPnQFOAiPgDyTjIQGAe8Dnw3c0+ZwP+fZmZWR1ojF1DZmZWC04EZmYZ50RgZpZxTgRmZhnnRGBmlnFOBLbFkbRW0tRyPx2rKduxqlkaa3nO59MZLqel0zPstQl1fE/S0PT1mZJ2K3fsdklFdRznq5KKc3jPDyS12NxzW+PlRGBboi8iorjcz8ICnfe0iOhJMiHhDbV9c0T8ISL+km6eCexW7tjZETGrTqL8Ks5byS3OHwBOBFYlJwJrENJv/v+Q9Fr6c0glZXpIeiVtRUyX1C3df3q5/X+U1KSG070IdE3fe0Q6z/2MdJ74bdL91+mr9R1uTPddJWm4pBNJ5nMak55z2/SbfImk8yRdXy7mMyXdvIlxTqDcZGOSbpM0Wck6BFen+y4mSUjPSXou3ddP0oT09/iApO1rOI81ck4EtiXatly30CPpvsXAURGxPzAEGFnJ+74H/C4iikk+iEvTKQeGAIem+9cCp9Vw/mOBGZKaA6OBIRGxD8mT+OdJagUcD/SIiH2BEeXfHBEPApNJvrkXR8QX5Q4/CJxQbnsIMHYT4+xPMqXEej+NiBJgX+BwSftGxEiSeWj6RkTfdNqJK4Aj09/lZOCHNZzHGrlGN8WENQpfpB+G5TUFfp/2ia8lmUOnognATyW1Bx6OiLmSjgAOAF5Np9bYliSpVGaMpC+AhSRTGe8FvBURc9LjdwEXAL8nWd/gdklPADlPcx0RSyQtSOeImZue46W03trEuR3JlAvlV6c6SdIwkv+vdyVZpGV6hfcenO5/KT1PM5Lfm2WYE4E1FJcAHwA9SVqyGy00ExH3SJoEHAM8Jelskil774qIy3M4x2nlJ6WTVOkaFen8N71IJjo7GbgQ+M9aXMtY4CTgX8AjERFKPpVzjpNkpa7rgFuAEyR1AoYDB0bEx5JGk0y+VpGAv0fEKbWI1xo5dw1ZQ7Ej8F46x/x3SL4Nb0BSZ2BB2h0yjqSL5FngREm7pGVaKff1mv8FdJTUNd3+DvBC2qe+Y0SMJxmIrezOneUkU2FX5mFgMMk8+mPTfbWKMyJWk3TxHJx2K+0ArACWSfoPYEAVsUwEDl1/TZJaSKqsdWUZ4kRgDcWtwBmSJpJ0C62opMwQ4A1JU4GvkyznN4vkA/NpSdOBv5N0m9QoIlaSzOz4gKQZwDrgDyQfqo+n9b1A0lqpaDTwh/WDxRXq/RiYBewREa+k+2odZzr28GtgeERMI1mreCZwJ0l303qjgL9Jei4ilpDc0XRvep6JJL8ryzDPPmpmlnFuEZiZZZwTgZlZxjkRmJllnBOBmVnGORGYmWWcE4GZWcY5EZiZZdz/AbPrLUpW2VUiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#ROC\n",
    "train_predict_prob = svc.decision_function(normalize_train_x)\n",
    "test_predict_prob = svc.decision_function(normalize_test_x)\n",
    "train_fpr, train_tpr, train_thresholds = roc_curve(train_y, train_predict_prob)\n",
    "test_fpr, test_tpr, test_thresholds = roc_curve(test_y, test_predict_prob)\n",
    "plt.plot(train_fpr, train_tpr, label='train ROC curve')\n",
    "plt.plot(test_fpr, test_tpr, label='test ROC curve')\n",
    "plt.title('ROC curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii. Unsupervised Learning: \n",
    "    Run k-means algorithm on the whole training set. Ignore the labels of the data, and assume k = 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Run the k-means algorithm multiple times. \n",
    "    Make sure that you initialize the algoritm randomly.\n",
    "    How do you make sure that the algorithm was not trapped in a local minimum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset to do k-means\n",
    "train_x, train_y, test_x, test_y = split_dataset(dataset_x, dataset_y, test_size = 0.2, random = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    We can run K-means several times to avoid to trapped in a local minimum. Besides, it's important to choose random seed and cluster number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build kmeans model\n",
    "#Use the global random state from np.random, run 20 times\n",
    "kmeans = KMeans(n_clusters=2, n_init = 50, random_state = None).fit(train_x)\n",
    "labels = kmeans.labels_\n",
    "centers = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Compute the centers of the two clusters and find the closest 30 datapoints to each center. \n",
    "    Read the true labels of those 30 data points and take a majority poll within them. The majority poll becomes the label predicted by k-means for the members of each cluster. Then compare the labels provided by k-means with the true labels of the training data and report the average accuracy, precision, recall, F-score, and AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_label(dataset_x, dataset_y, cluster_index, center):\n",
    "    #convert into arrays to calculate easily\n",
    "    dataset_x = np.array(dataset_x)\n",
    "    dataset_y = np.array(dataset_y)\n",
    "    cluster = dataset_x[cluster_index]\n",
    "    cluster_label = dataset_y[cluster_index] \n",
    "    cluster_distances = np.empty(0)# create a array to store distance\n",
    "    for point in cluster:\n",
    "        distance = np.linalg.norm(point - center)\n",
    "        cluster_distances = np.append(cluster_distances, distance) ## add to list\n",
    "    ranks = cluster_distances.argsort()\n",
    "    cluster_label = cluster_label[ranks[:30]]\n",
    "    counts = np.bincount(cluster_label)\n",
    "    classify_cluster = np.argmax(counts)\n",
    "    return classify_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train accuracy is:  0.8901\n"
     ]
    }
   ],
   "source": [
    "#cluster index\n",
    "cluster_index0 = np.where(labels==0)\n",
    "cluster_index1 = np.where(labels==1)\n",
    "\n",
    "#center\n",
    "center0 = centers[0]\n",
    "center1 = centers[1]\n",
    "\n",
    "#find the majority label for each cluster\n",
    "majoritylabel0 = majority_label(train_x, train_y, cluster_index0, center0)\n",
    "majoritylabel1 = majority_label(train_x, train_y, cluster_index1, center1)\n",
    "\n",
    "train_predict_labels = labels\n",
    "train_predict_labels[cluster_index0] = majoritylabel0\n",
    "train_predict_labels[cluster_index1] = majoritylabel1\n",
    "\n",
    "train_accuracy = accuracy_score(train_y, train_predict_labels)\n",
    "\n",
    "print(\"The train accuracy is: \",round(train_accuracy,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predict No</th>\n",
       "      <th>Predict Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Actual No</td>\n",
       "      <td>280</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Actual Yes</td>\n",
       "      <td>45</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Predict No  Predict Yes\n",
       "Actual No          280            5\n",
       "Actual Yes          45          125"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cm = pd.DataFrame(confusion_matrix(train_y, train_predict_labels), columns= [\"Predict No\", \"Predict Yes\"],\n",
    "                        index = [\"Actual No\", \"Actual Yes\"] )\n",
    "train_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train precision is:  0.9615\n",
      "Train recall is:  0.7353\n",
      "Train F1-score is:  0.8333\n"
     ]
    }
   ],
   "source": [
    "#precision\n",
    "train_cm = confusion_matrix(train_y, train_predict_labels)\n",
    "train_tn = train_cm[0][0]; \n",
    "train_fp = train_cm[0][1]; \n",
    "train_fn = train_cm[1][0]; \n",
    "train_tp = train_cm[1][1];\n",
    "train_precision = train_tp/(train_tp + train_fp)\n",
    "print(\"Train precision is: \", round(train_precision,4))\n",
    "    \n",
    "#recall\n",
    "train_recall = train_tp/(train_tp + train_fn)\n",
    "print(\"Train recall is: \", round(train_recall,4))\n",
    "    \n",
    "#F-score\n",
    "train_f1 = 2 * (train_precision * train_recall) / (train_precision + train_recall)\n",
    "print(\"Train F1-score is: \", round(train_f1,4))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Classify test data based on their proximity to the centers of the clusters.\n",
    "    Report the average accuracy, precision, recall, F-score, and AUC over M runs, and ROC and the confusion matrix for one of the runs for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = kmeans.predict(test_x)\n",
    "test_cluster_index0 = np.where(test_labels==0)\n",
    "test_cluster_index1 = np.where(test_labels==1)\n",
    "\n",
    "\n",
    "test_predict_labels = test_labels\n",
    "test_predict_labels[test_cluster_index0] = majoritylabel0\n",
    "test_predict_labels[test_cluster_index1] = majoritylabel1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy is:  0.886\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predict No</th>\n",
       "      <th>Predict Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Actual No</td>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Actual Yes</td>\n",
       "      <td>11</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Predict No  Predict Yes\n",
       "Actual No           70            2\n",
       "Actual Yes          11           31"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy = accuracy_score(test_y, test_predict_labels)\n",
    "print(\"The test accuracy is: \",round(test_accuracy,4))\n",
    "\n",
    "test_cm = pd.DataFrame(confusion_matrix(test_y, test_predict_labels), columns= [\"Predict No\", \"Predict Yes\"],\n",
    "                        index = [\"Actual No\", \"Actual Yes\"] )\n",
    "test_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test precision is:  0.9394\n",
      "test recall is:  0.7381\n",
      "test F1-score is:  0.8267\n"
     ]
    }
   ],
   "source": [
    "#precision\n",
    "test_cm = confusion_matrix(test_y, test_predict_labels)\n",
    "test_tn = test_cm[0][0]; \n",
    "test_fp = test_cm[0][1]; \n",
    "test_fn = test_cm[1][0]; \n",
    "test_tp = test_cm[1][1];\n",
    "test_precision = test_tp/(test_tp + test_fp)\n",
    "print(\"test precision is: \", round(test_precision,4))\n",
    "    \n",
    "#recall\n",
    "test_recall = test_tp/(test_tp + test_fn)\n",
    "print(\"test recall is: \", round(test_recall,4))\n",
    "    \n",
    "#F-score\n",
    "test_f1 = 2 * (test_precision * test_recall) / (test_precision + test_recall)\n",
    "print(\"test F1-score is: \", round(test_f1,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run KMeans cluster 30 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unsupervised():\n",
    "    result = list()\n",
    "    #split dataset to do k-means\n",
    "    train_x, train_y, test_x, test_y = split_dataset(normalize_dataset_x, dataset_y, test_size = 0.2, random = None)\n",
    "    #build kmeans model\n",
    "    #Use the global random state from np.random, run 20 times\n",
    "    kmeans = KMeans(n_clusters=2, n_init=50, random_state = None).fit(train_x)\n",
    "    labels = kmeans.labels_\n",
    "    centers = kmeans.cluster_centers_\n",
    "    \n",
    "    #cluster index\n",
    "    cluster_index0 = np.where(labels==0)\n",
    "    cluster_index1 = np.where(labels==1)\n",
    "\n",
    "    #center\n",
    "    center0 = centers[0]\n",
    "    center1 = centers[1]\n",
    "\n",
    "    #find the majority label for each cluster\n",
    "    majoritylabel0 = majority_label(train_x, train_y, cluster_index0, center0)\n",
    "    majoritylabel1 = majority_label(train_x, train_y, cluster_index1, center1)\n",
    "\n",
    "    train_predict_labels = labels.copy()\n",
    "    train_predict_labels[cluster_index0] = majoritylabel0\n",
    "    train_predict_labels[cluster_index1] = majoritylabel1\n",
    "    \n",
    "    #train scores\n",
    "    train_accuracy = accuracy_score(train_y, train_predict_labels)\n",
    "    \n",
    "    #precision\n",
    "    train_cm = confusion_matrix(train_y, train_predict_labels)\n",
    "    train_tn = train_cm[0][0]; \n",
    "    train_fp = train_cm[0][1]; \n",
    "    train_fn = train_cm[1][0]; \n",
    "    train_tp = train_cm[1][1];\n",
    "    train_precision = train_tp/(train_tp + train_fp)\n",
    "    \n",
    "\n",
    "    #recall\n",
    "    train_recall = train_tp/(train_tp + train_fn)\n",
    "    \n",
    "    \n",
    "    #F-score\n",
    "    train_f1 = 2 * (train_precision * train_recall) / (train_precision + train_recall)\n",
    "    \n",
    "    \n",
    "    #test ont test data\n",
    "    test_labels = kmeans.predict(test_x)\n",
    "    test_cluster_index0 = np.where(test_labels==0)\n",
    "    test_cluster_index1 = np.where(test_labels==1)\n",
    "\n",
    "\n",
    "    test_predict_labels = test_labels.copy()\n",
    "    test_predict_labels[test_cluster_index0] = majoritylabel0\n",
    "    test_predict_labels[test_cluster_index1] = majoritylabel1\n",
    "    #scores\n",
    "    test_accuracy = accuracy_score(test_y, test_predict_labels)\n",
    "    \n",
    "    #precision\n",
    "    test_cm = confusion_matrix(test_y, test_predict_labels)\n",
    "    test_tn = test_cm[0][0]; \n",
    "    test_fp = test_cm[0][1]; \n",
    "    test_fn = test_cm[1][0]; \n",
    "    test_tp = test_cm[1][1];\n",
    "    test_precision = test_tp/(test_tp + test_fp)\n",
    "    \n",
    "    \n",
    "    #recall\n",
    "    test_recall = test_tp/(test_tp + test_fn)\n",
    "    \n",
    "    \n",
    "    #F-score\n",
    "    test_f1 = 2 * (test_precision * test_recall) / (test_precision + test_recall)\n",
    "    \n",
    "    # AUC\n",
    "    if(majoritylabel0==0):\n",
    "        train_auc = roc_auc_score(train_y, pd.DataFrame(kmeans.transform(train_x)).iloc[:,0])\n",
    "        test_auc = roc_auc_score(test_y, pd.DataFrame(kmeans.transform(test_x)).iloc[:,0])\n",
    "    \n",
    "\n",
    "    else:\n",
    "        train_auc = roc_auc_score(train_y, pd.DataFrame(kmeans.transform(train_x)).iloc[:,1])\n",
    "        test_auc = roc_auc_score(test_y, pd.DataFrame(kmeans.transform(test_x)).iloc[:,1])\n",
    "    \n",
    "    result.append(train_accuracy)\n",
    "    result.append(test_accuracy)\n",
    "    result.append(train_precision)\n",
    "    result.append(test_precision)\n",
    "    result.append(train_recall)\n",
    "    result.append(test_recall)\n",
    "    result.append(train_f1)\n",
    "    result.append(test_f1)\n",
    "    result.append(train_auc)\n",
    "    result.append(test_auc)\n",
    "    \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['train_accuracy', 'test_accuracy', 'train_precision', 'test_precision', \n",
    "       'train_recall', 'test_recall', 'train_f1', \n",
    "       'test_f1', 'train_auc', 'test_auc']\n",
    "unsupervised_result = pd.DataFrame(columns=col, index=range(30))\n",
    "for i in range(30):\n",
    "    res = unsupervised()\n",
    "    unsupervised_result.loc[i] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of Kmeans Clustering:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "train_accuracy     0.886154\n",
       "test_accuracy      0.888012\n",
       "train_precision    0.960898\n",
       "test_precision     0.952797\n",
       "train_recall       0.725098\n",
       "test_recall        0.732540\n",
       "train_f1           0.826164\n",
       "test_f1            0.827172\n",
       "train_auc          0.908832\n",
       "test_auc           0.916986\n",
       "dtype: float64"
      ]
     },
     "execution_count": 605,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The result of Kmeans Clustering:\")\n",
    "unsupervised_result.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predict No</th>\n",
       "      <th>Predict Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Actual No</td>\n",
       "      <td>280</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Actual Yes</td>\n",
       "      <td>45</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Predict No  Predict Yes\n",
       "Actual No          280            5\n",
       "Actual Yes          45          125"
      ]
     },
     "execution_count": 606,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cm = pd.DataFrame(confusion_matrix(train_y, train_predict_labels), columns= [\"Predict No\", \"Predict Yes\"],\n",
    "                        index = [\"Actual No\", \"Actual Yes\"] )\n",
    "train_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predict No</th>\n",
       "      <th>Predict Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Actual No</td>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Actual Yes</td>\n",
       "      <td>11</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Predict No  Predict Yes\n",
       "Actual No           70            2\n",
       "Actual Yes          11           31"
      ]
     },
     "execution_count": 607,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cm = pd.DataFrame(confusion_matrix(test_y, test_predict_labels), columns= [\"Predict No\", \"Predict Yes\"],\n",
    "                        index = [\"Actual No\", \"Actual Yes\"] )\n",
    "test_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_fpr, train_tpr, train_thresholds = roc_curve(train_y, pd.DataFrame(kmeans.transform(train_x)).iloc[:,0])\n",
    "test_fpr, test_tpr, test_thresholds = roc_curve(test_y, pd.DataFrame(kmeans.transform(test_x)).iloc[:,0])\n",
    "plt.plot(train_fpr, train_tpr, label='train ROC curve')\n",
    "plt.plot(test_fpr, test_tpr, label='test ROC curve')\n",
    "plt.title('ROC curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iv. Spectral Clustering:\n",
    "    Repeat 1(b)iii using spectral clustering, which is clustering based on kernels. Research what spectral clustering is. Use RBF kernel with gamma=1 or find a gamma for which the two clutsres have the same balance as the one in original data set (if the positive class has p and the negative class has n samples, the two clusters must have p and n members). Do not label data based on their proximity to cluster center, because spectral clustering may give you non-convex clusters . Instead, use fit - predict method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_clustering():\n",
    "    result = []\n",
    "    train_x, train_y, test_x, test_y = split_dataset(normalize_dataset_x, dataset_y, \n",
    "                                                     test_size = 0.2,random = None)\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "    \n",
    "    \n",
    "\n",
    "    clustering = SpectralClustering(n_clusters=2, gamma=1, affinity='rbf').fit(train_x)\n",
    "    labels = clustering.labels_\n",
    "    \n",
    "    #cluster index\n",
    "    cluster_index0 = np.where(labels==0)\n",
    "    cluster_index1 = np.where(labels==1)\n",
    "    #find the majority label for each cluster\n",
    "    majoritylabel0 = np.argmax(np.bincount(train_y[cluster_index0[0]]))\n",
    "    majoritylabel1 = np.argmax(np.bincount(train_y[cluster_index1]))\n",
    "\n",
    "    train_predict_labels = labels\n",
    "    train_predict_labels[cluster_index0] = majoritylabel0\n",
    "    train_predict_labels[cluster_index1] = majoritylabel1\n",
    "    \n",
    "    #train scores\n",
    "    train_accuracy = accuracy_score(train_y, train_predict_labels)\n",
    "   \n",
    "    #precision\n",
    "    train_cm = confusion_matrix(train_y, train_predict_labels)\n",
    "    train_tn = train_cm[0][0]; \n",
    "    train_fp = train_cm[0][1]; \n",
    "    train_fn = train_cm[1][0]; \n",
    "    train_tp = train_cm[1][1];\n",
    "    train_precision = train_tp/(train_tp + train_fp)\n",
    "\n",
    "    #recall\n",
    "    train_recall = train_tp/(train_tp + train_fn)\n",
    "    \n",
    "    #F-score\n",
    "    train_f1 = 2 * (train_precision * train_recall) / (train_precision + train_recall)\n",
    "\n",
    "    \n",
    "    #test ont test data\n",
    "    test_labels = clustering.fit_predict(test_x)\n",
    "    \n",
    "    test_cluster_index0 = np.where(test_labels==0)\n",
    "    test_cluster_index1 = np.where(test_labels==1)\n",
    "    \n",
    "    test_predict_labels = test_labels\n",
    "    test_predict_labels[test_cluster_index0] = majoritylabel0\n",
    "    test_predict_labels[test_cluster_index1] = majoritylabel1\n",
    "    #scores\n",
    "    test_accuracy = accuracy_score(test_y, test_predict_labels)\n",
    "\n",
    "    #precision\n",
    "    test_cm = confusion_matrix(test_y, test_predict_labels)\n",
    "    test_tn = test_cm[0][0]; \n",
    "    test_fp = test_cm[0][1]; \n",
    "    test_fn = test_cm[1][0]; \n",
    "    test_tp = test_cm[1][1];\n",
    "    test_precision = test_tp/(test_tp + test_fp)\n",
    "    \n",
    "    \n",
    "    #recall\n",
    "    test_recall = test_tp/(test_tp + test_fn)\n",
    "    \n",
    "    #F-score\n",
    "    test_f1 = 2 * (test_precision * test_recall) / (test_precision + test_recall)\n",
    "    \n",
    "    #AUC score\n",
    "    test_dis = pairwise_distances(test_x, train_x, n_jobs=10)\n",
    "    train_dis = pairwise_distances(train_x, train_x, n_jobs=10)\n",
    "    train_min_0 = np.amin(train_dis[:,cluster_index0[0]], axis = 1).reshape(-1,1)\n",
    "    train_min_1 = np.amin(train_dis[:,cluster_index1[0]], axis = 1).reshape(-1, 1)\n",
    "    train_min = np.concatenate((train_min_0, train_min_1), axis=1)\n",
    "    train_pro_class0 = (train_min[:, 0] / np.sum(train_min, axis=1))\n",
    "    test_min_0 = np.amin(test_dis[:,cluster_index0[0]], axis = 1).reshape(-1,1)\n",
    "    test_min_1 = np.amin(test_dis[:,cluster_index1[0]], axis = 1).reshape(-1, 1)\n",
    "    test_min = np.concatenate((test_min_0, test_min_1), axis=1)\n",
    "    test_pro_class0 = (test_min[:, 0] / np.sum(test_min, axis=1))\n",
    "    train_auc = roc_auc_score(train_y, train_pro_class0)\n",
    "    test_auc = roc_auc_score(test_y, test_pro_class0)\n",
    "\n",
    "    \n",
    "    result.append(train_accuracy)\n",
    "    result.append(test_accuracy)\n",
    "    result.append(train_precision)\n",
    "    result.append(test_precision)\n",
    "    result.append(train_recall)\n",
    "    result.append(test_recall)\n",
    "    result.append(train_f1)\n",
    "    result.append(test_f1)\n",
    "    result.append(train_auc)\n",
    "    result.append(test_auc)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['train_accuracy', 'test_accuracy', 'train_precision', 'test_precision', \n",
    "       'train_recall', 'test_recall', 'train_f1', 'test_f1','train_auc',\"test_auc\"]\n",
    "spectral_result = pd.DataFrame(columns=col, index=range(30))\n",
    "for i in range(30):\n",
    "    res = spectral_clustering()\n",
    "    spectral_result.loc[i] = res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of Spectral Learning:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "train_accuracy     0.881832\n",
       "test_accuracy      0.520760\n",
       "train_precision    0.958319\n",
       "test_precision     0.564505\n",
       "train_recall       0.714902\n",
       "test_recall        0.507143\n",
       "train_f1           0.818764\n",
       "test_f1            0.516316\n",
       "train_auc          0.594460\n",
       "test_auc           0.615002\n",
       "dtype: float64"
      ]
     },
     "execution_count": 704,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The result of Spectral Learning:\")\n",
    "spectral_result.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, test_x, test_y = split_dataset(normalize_dataset_x, dataset_y, \n",
    "                                                     test_size = 0.2,random = None)\n",
    "train_y = np.array(train_y)\n",
    "test_y = np.array(test_y)\n",
    "    \n",
    "clustering = SpectralClustering(n_clusters=2, gamma=1, affinity='rbf').fit(train_x)\n",
    "labels = clustering.labels_\n",
    "    \n",
    "    #cluster index\n",
    "cluster_index0 = np.where(labels==0)\n",
    "cluster_index1 = np.where(labels==1)\n",
    "\n",
    "    #find the majority label for each cluster\n",
    "majoritylabel0 = np.argmax(np.bincount(train_y[cluster_index0]))\n",
    "majoritylabel1 = np.argmax(np.bincount(train_y[cluster_index1]))\n",
    "\n",
    "train_predict_labels = labels\n",
    "train_predict_labels[cluster_index0] = majoritylabel0\n",
    "train_predict_labels[cluster_index1] = majoritylabel1\n",
    "    \n",
    "#confusion matrix of training data\n",
    "train_cm = confusion_matrix(train_y, train_predict_labels)\n",
    "    \n",
    "#test ont test data\n",
    "test_labels = clustering.fit_predict(test_x)\n",
    "test_cluster_index0 = np.where(test_labels==0)\n",
    "test_cluster_index1 = np.where(test_labels==1)\n",
    "\n",
    "\n",
    "test_predict_labels = test_labels\n",
    "test_predict_labels[test_cluster_index0] = majoritylabel0\n",
    "test_predict_labels[test_cluster_index1] = majoritylabel1\n",
    "\n",
    "test_cm = confusion_matrix(test_y, test_predict_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predict No</th>\n",
       "      <th>Predict Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Actual No</td>\n",
       "      <td>280</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Actual Yes</td>\n",
       "      <td>46</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Predict No  Predict Yes\n",
       "Actual No          280            5\n",
       "Actual Yes          46          124"
      ]
     },
     "execution_count": 569,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cm = pd.DataFrame(train_cm, columns= [\"Predict No\", \"Predict Yes\"],\n",
    "                        index = [\"Actual No\", \"Actual Yes\"] )\n",
    "train_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predict No</th>\n",
       "      <th>Predict Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Actual No</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Actual Yes</td>\n",
       "      <td>17</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Predict No  Predict Yes\n",
       "Actual No           72            0\n",
       "Actual Yes          17           25"
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cm = pd.DataFrame(test_cm, columns= [\"Predict No\", \"Predict Yes\"],\n",
    "                        index = [\"Actual No\", \"Actual Yes\"] )\n",
    "test_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC curve\n",
    "test_dis = pairwise_distances(test_x, train_x, n_jobs=10)\n",
    "train_dis = pairwise_distances(train_x, train_x, n_jobs=10)\n",
    "train_min_0 = np.amin(train_dis[:,cluster_index0[0]], axis = 1).reshape(-1,1)\n",
    "train_min_1 = np.amin(train_dis[:,cluster_index1[0]], axis = 1).reshape(-1, 1)\n",
    "train_min = np.concatenate((train_min_0, train_min_1), axis=1)\n",
    "train_pro_class0 = (train_min[:, 0] / np.sum(train_min, axis=1))\n",
    "test_min_0 = np.amin(test_dis[:,cluster_index0[0]], axis = 1).reshape(-1,1)\n",
    "test_min_1 = np.amin(test_dis[:,cluster_index1[0]], axis = 1).reshape(-1, 1)\n",
    "test_min = np.concatenate((test_min_0, test_min_1), axis=1)\n",
    "test_pro_class0 = (test_min[:, 0] / np.sum(test_min, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deXwV5dn/8c9FBCKrsimL7KAEkYgRRVuFR0XAimh9xK2oj4q1Lq0t/qqttWrxVatWW6zaUhesDyqKG1Xq+ri0KigoiITKppYICiJrICHL9ftjJoeTkOWEZE5ycr7v1ysvz8yZM3NNgnOd+7pn7tvcHRERSV/NGjoAERFpWEoEIiJpTolARCTNKRGIiKQ5JQIRkTSnRCAikuaUCERE0pwSgTQ5Zva5me00s+1m9pWZzTCzNhW2OcbM/s/MtpnZFjP7u5llVdimnZn9wcz+E+5rZbjcKblnJBItJQJpqk519zZANnA4cH3ZG2Y2AngFeB7oBvQBFgPvmFnfcJsWwOvAYGAM0A44BtgIDI8qaDPbJ6p9i1RFiUCaNHf/CniZICGUuR34m7v/0d23ufu37n4DMA+4KdxmEtATON3dc9291N3Xu/tv3H1uZccys8Fm9qqZfWtmX5vZL8L1M8xsatx2I80sL275czP7uZl9DOSb2Q1mNrvCvv9oZtPC1+3N7EEzW2dmX5rZVDPLqOOvStKYEoE0aWbWAxgLrAyXWxF8s3+qks2fBE4KX58IvOTu2xM8TlvgNeAlglZGf4IWRaLOAU4B9gMeBcaZWbtw3xnAWcBj4baPAMXhMQ4HRgOX1OJYIuUoEUhT9ZyZbQPWAOuBX4frOxD8u19XyWfWAWX1/45VbFOV7wFfufvv3b0gbGnMr8Xnp7n7Gnff6e5fAB8CE8L3/gvY4e7zzOwAgsT2E3fPd/f1wN3A2bU4lkg5SgTSVE1w97bASOAQdl/gNwGlQNdKPtMV+CZ8vbGKbapyELBqryINrKmw/BhBKwHgXHa3BnoBzYF1ZrbZzDYDfwG61OHYkuaUCKRJc/e3gBnAneFyPvAe8N+VbH4Wu8s5rwEnm1nrBA+1BuhXxXv5QKu45QMrC7XC8lPAyLC0dTq7E8EaoBDo5O77hT/t3H1wgnGK7EGJQNLBH4CTzKysw/g64AIzu9rM2prZ/mFn7gjg5nCbRwkuuk+b2SFm1szMOprZL8xsXCXHeAE40Mx+YmYtw/0eFb63iKDm38HMDgR+UlPA7r4BeBN4GPjM3ZeF69cR3PH0+/D21mZm1s/Mjt+L34sIoEQgaSC8qP4N+FW4/C/gZOAMgn6ALwg6Xb/j7ivCbQoJOoz/DbwKbAXeJygx7VH7d/dtBB3NpwJfASuAUeHbjxLcnvo5wUV8VoKhPxbG8FiF9ZOAFkAuQalrNrUrY4mUY5qYRkQkvalFICKS5pQIRETSnBKBiEiaUyIQEUlzKTfAVadOnbx3794NHYaISEpZuHDhN+7eubL3Ui4R9O7dmwULFjR0GCIiKcXMvqjqPZWGRETSnBKBiEiaUyIQEUlzSgQiImlOiUBEJM1FlgjM7CEzW29mn1TxvpnZtHBC8I/NbFhUsYiISNWibBHMIJj0uypjgQHhz2Tg/ghjERGRKkT2HIG7v21mvavZ5DSCCcQdmGdm+5lZ13C8dZHUt+BhWDK75u1EquA4u4pLyd9Vwo7CYjJ7ZtPpzLvr/TgN+UBZd8pPz5cXrtsjEZjZZIJWAz179kxKcCJ1tmQ2fLUEDhzS0JFICih1Z2dRCTt2lZBfWMyOXcXk7yqhpHT3VAHt9y+MzblanxoyEVgl6yqdHMHdpwPTAXJycjSBgqSOA4fARS82dBTSyGzZWcSydVvJXbuV3PC/K9Zvo6gkuLy13KcZh3RtR1bXdmR1C/57yIFtad0ymkt2QyaCPIIJv8v0ANY2UCwiIvXO3fly885yF/zcdVvJ27Qztk3H1i3I6taO7w7sQ1bXdgzu1o7eHVuzT0bybupsyEQwB7jSzJ4AjgK2qH9ARFLVruJSVq7fHnfB30Lu2q1sLSgGwAz6dGzN0IP245zhPcnq1o7BXdvRuW1LzCorkCRPZInAzB4HRgKdzCwP+DXQHMDd/wzMBcYBK4EdwEVRxdLkqVOycVL/QJNVU2kns3kzDjmwHd8b2i1W3jnkwLa0atE4x/mM8q6hc2p434Erojp+WlGnZON04BAYcmZDRyF1kEhpp1ObFmR1a89xAzvH6vl9OrUmo1nDfsuvjcaZnqT21CkpUicJlXY6tSb7oP0496iesW/6XdpmNnDkdadEICJpp6mVduqqaZ6ViAjpU9qpKyUCEWkS0rm0U1dKBCKSclTaqV/6rYhIo+Xu5G3aWa6sk7t2K19uji/ttCSrW7u0Lu3UlRKBiDQKu4pLWbF+W7kL/rJ15Us7fTu1Zliv/Tn/6F5kdWvHoK5tVdqpB0oE9a0hHu7SMwSSYrbsKAou9nHf9FfGlXb2bZ7BIV3bcurQbrFv+QertBMZ/VbrW0M83KUHl6SRSqS007ltS7K6tmPkwZ1j9fzeHVXaSSYlgijo4S5JQ5WVdnLXbWWbSjuNnhKBiNRaoqWd8SrtpAT9VUSkSirtpAclgrqorGNYHbeSolTaSV9KBHVRWcewOm4lBai0I/H0V60rdQxLI1ZW2lkaXuyXVVLa6dI2eCBr1MG7H8jqpdJOWlEiEGkiCotLWPH19nLf8pfFlXaaGfTt3IYjeu3PD0b0IqtrOwaFM2RJelMiEElBm3fs2qMDd+X67RSX7i7tDOraltOyu5HVtT1Z3dpx8AFt2bdFRgNHLo2REoFII1axtFM27EJlpZ3/OqSLSjuyV5QIRBoJlXakoSgRiDQAlXakMVEiEImQSjuSCpQIROpJpaWdtVvZVqjSjjRuSgQie6Gm0k6rFhkM6tqOCYd3j33LH6jSjjRSSgQi1UiktHNAu2CsnRMGdYnV83t1aEUzlXYkRSgRiIQSKe3069yGnN77M6lr2Vg77ejURqUdSW1KBJKWNu/Ytbusk2Bp5+AD25LZXKUdaXqUCGqj4mijGmm00XN31ny7k9x1W8rV89duKYhto9KOpDslgtqoONqoRhptVGKlnQr1/IqlnSP7dIiNm6/SjogSQe1ptNFGYVP+rmAkzXLDKKu0I7I3lAikUVNpRyR6SgTSaKi0I9IwIk0EZjYG+COQATzg7rdVeL8n8AiwX7jNde4+N8qYakWdw5GpqbTTWqUdkaSJLBGYWQZwL3ASkAd8YGZz3D03brMbgCfd/X4zywLmAr2jiqnW1DlcZ6WlZZOfV13aObBdJlnd2nHioANiF/2eKu2IJE2ULYLhwEp3Xw1gZk8ApwHxicCBduHr9sDaCOPZO+ocTlhBUQkr11dd2sloZvTr3JrhfTqEF/z2DOralo4q7Yg0qCgTQXdgTdxyHnBUhW1uAl4xs6uA1sCJle3IzCYDkwF69uxZ74FK7SVa2jl9WPdYPX/gASrtiDRGUSaCytr1XmH5HGCGu//ezEYAj5rZoe5eWu5D7tOB6QA5OTkV9yERKi111mzaUe5bfu66raxTaUekyYgyEeQBB8Ut92DP0s/FwBgAd3/PzDKBTsD6COOSKhQUlY21syVuhqxtbK9Q2jlKpR2RJiXKRPABMMDM+gBfAmcD51bY5j/ACcAMMxsEZAIbIoxJQpvyKxlGecN2SiqUds5QaUekyYssEbh7sZldCbxMcGvoQ+6+1MxuARa4+xzgZ8BfzewagrLRhe6u0k89qk1p56QslXZE0lGkzxGEzwTMrbDuxrjXucCxUcaQThIp7fTv3Iaj+3ZkUNe2Ku2ICJDuTxZXfGCsokb8ANm3ZXftVFPayerWju8P6x6r5w84oI1KOyKyh/ROBBUfGKuoETxAlkhpp2v7TLK6tmP04ANi9fyD9ldpR0QSk96JAJLywNiydVu5942VlNay+2PDtsIqSzvxY+10aN0iirBFJE0oESQgb9MOZi/Mo7R07/qx53/2LfM/+5b+XdpU+nBFVfZr1VylHRGJnBJBnKVrt7BqQ/4e61/+5CteXLIOq0OlpV/n1rx6zXFYXXYiIhKBtE4EhcUlFJc6/1m3FYCLHv6A9dsKK912/1bN+ejG0ckMT0QkKdI2EeRt2kHems0AnP3Hf8bWn5XTg8nH9dtj+05tVIcXkaYpbRPB1p1BB2zX9pn8+eRh4VpjRN+OtG/VvOECExFJsrRNBGXatmzOmEO7NnQYIiINplkiG5lZCzPrH3UwIiKSfDUmAjM7BVgCvBouZ5vZs1EHJiIiyZFIi+AWggllNgO4+yJArQMRkSYikURQ5O6bK6zTCKEiIk1EIp3Fy8zsLKBZOLfAj4F50YYlIiLJkkiL4ErgCKAUeAYoIEgGIiLSBCTSIjjZ3X8O/LxshZmdQZAUREQkxSXSIrihknW/rO9ARESkYVTZIjCzkwkmlu9uZnfFvdWOoEwkIiJNQHWlofXAJwR9Akvj1m8DrosyKBERSZ4qE4G7fwR8ZGYz3b2gqu1ERCS1JdJZ3N3MbgWygMyyle4+MLKoREQkaRLpLJ4BPAwYMBZ4EngiwphERCSJEkkErdz9ZQB3X+XuNwCjog1LRESSJZHSUKEF8yuuMrMfAl8CXaINK3quUTJERIDEEsE1QBvgauBWoD3wP1EGlQwl4UT0mkJYRNJdjYnA3eeHL7cBPwAwsx5RBpUMBUXBoxDNlAlEJM1V20dgZkea2QQz6xQuDzazv9EEBp0rLC4B1CIQEakyEZjZb4GZwHnAS2b2S+ANYDGQ8reOqkUgIhKorjR0GjDU3XeaWQdgbbj8aXJCi1ZhcQltgWYJTdYpItJ0VZcICtx9J4C7f2tm/07pJLDgYVgyO7Z41PZCWtoXNLOhDRiUiEjDq+77cF8zeyb8eRboHbec0BDUZjbGzD41s5VmVun4RGZ2lpnlmtlSM3tsb04iIUtmw1dLYoul7uR6L3YNOiOyQ4qIpILqWgTfr7D8p9rs2MwygHuBk4A84AMzm+PuuXHbDACuB451901mFu3zCQcOgYteBOCFf33Gb17IZfGw0ZEeUkSksatu0LnX67jv4cBKd18NYGZPEPQ75MZtcylwr7tvCo+5vo7HTFjZXUMtm6uTQETSW5RXwe7AmrjlvHBdvIHAQDN7x8zmmdmYynZkZpPNbIGZLdiwYUO9BFd211DLfZQIRCS9RXkVrOy+zIrjOuwDDABGAucAD5jZfnt8yH26u+e4e07nzp3rJbjCohJa7tMM0+2jIpLmEk4EZtaylvvOAw6KW+5BcAtqxW2ed/cid/8M+JQgMUSusLhUrQERERJIBGY23MyWACvC5aFmdk8C+/4AGGBmfcysBXA2MKfCNs8RjmQaPr08EFhdi/j3WkFRCZnNM5JxKBGRRi2Rr8TTgO8BGwHcfTEJDEPt7sXAlcDLwDLgSXdfama3mNn4cLOXgY1mlkvw1PK17r6x9qdRe4XFpeooFhEhsdFHm7n7FxVq6SWJ7Nzd5wJzK6y7Me61Az8Nf5KqoKiEzH3UIhARSSQRrDGz4YCHzwZcBSyPNqzoFRaXqjQkIkJipaHLCb6x9wS+Bo4O16W0gvCuIRGRdJdIi6DY3c+OPJIkKywuZV+1CEREEmoRfGBmc83sAjNrG3lESaIWgYhIoMYrobv3A6YCRwBLzOw5M0v5FoJuHxURCST0ldjd33X3q4FhwFaCCWtSmh4oExEJJPJAWRszO8/M/g68D2wAjok8sogVFJXSUi0CEZGEOos/Af4O3O7u/4w4nqQpLFYfgYgIJJYI+rp7aeSRJFlhkZ4jEBGBahKBmf3e3X8GPG1mFUcNxd1Tdmqv0lJnV4n6CEREoPoWwazwv7WamSwVFBYHDRy1CEREqp+h7P3w5SB3L5cMzOxKoK4zmDWYstnJMjXonIhIQreP/k8l6y6u70CSaffsZGoRiIhU10cwkWAOgT5m9kzcW22BzVEHFqWCIrUIRETKVNdH8D7BHAQ9gHvj1m8DPooyqKiV9RGoRSAiUn0fwWfAZ8BryQsnOdQiEBHZrbrS0FvufryZbaL8pPNGMKdMh8iji4haBCIiu1VXGiqbjrJTMgJJJrUIRER2q/JKGPc08UFAhruXACOAy4DWSYgtMmoRiIjslshX4ucIpqnsB/wNGAQ8FmlUEVOLQERkt0SuhKXuXgScAfzB3a8CukcbVrT0ZLGIyG6JJIJiM/tv4AfAC+G65tGFFL2yFoHGGhIRSfzJ4lEEw1CvNrM+wOPRhhWtWB+BWgQiIjUPQ+3un5jZ1UB/MzsEWOnut0YfWnTUIhAR2a3GRGBm3wUeBb4keIbgQDP7gbu/E3VwUSlUIhARiUlkYpq7gXHungtgZoMIEkNOlIFFqWy+YjNr6FBERBpcIl+JW5QlAQB3Xwa0iC6k6BUUleiOIRGRUCItgg/N7C8ErQCA82gCg86pLCQiEkgkEfwQuBr4fwR9BG8D90QZVNTUIhAR2a3aRGBmQ4B+wLPufntyQoqeWgQiIrtVeTU0s18QDC9xHvCqmVU2U1lKUotARGS36r4Wnwcc5u7/DRwJXF7bnZvZGDP71MxWmtl11Wx3ppm5mSXlTqTC4lKNMyQiEqrualjo7vkA7r6hhm33YGYZBDObjQWygHPMLKuS7doS9EHMr83+66KgqEQjj4qIhKrrI+gbN1exAf3i5y529zNq2PdwgqeQVwOY2RPAaUBuhe1+A9wOTKlN4HVRUFRKh9ZqEYiIQPWJ4PsVlv9Uy313B9bELecBR8VvYGaHAwe5+wtmVmUiMLPJwGSAnj171jKMPRUWq0UgIlKmujmLX6/jvit7bDc25aWZNSN4avnCmnbk7tOB6QA5OTlew+Y1KigqpaX6CEREgFrW/Wspj2B2szI9gLVxy22BQ4E3zexz4GhgTjI6jIPbR9UiEBGBaBPBB8AAM+tjZi2As4E5ZW+6+xZ37+Tuvd29NzAPGO/uCyKMCQgGndNdQyIigYSvhmbWsjY7dvdi4ErgZWAZ8KS7LzWzW8xsfO3CrF9qEYiI7JbIMNTDgQeB9kBPMxsKXBJOWVktd58LzK2w7sYqth2ZSMB1VVLq7CrRcwQiImUSuRpOA74HbARw98UEM5alpF2ar1hEpJxEEkEzd/+iwrqSKIJJBs1OJiJSXiKjj64Jy0MePi18FbA82rCiU6gWgYhIOYl8Lb4c+CnQE/ia4DbPWo871FioRSAiUl4ik9evJ7j1s0koKA4SgVoEIiKBRO4a+itxTwSXcffJkUQUscKioDSkFoGISCCRPoLX4l5nAqdTfgyhlFJWGlKLQEQkkEhpaFb8spk9CrwaWUQRK+ssVotARCSwN1fDPkCv+g4kWdQiEBEpL5E+gk3s7iNoBnwLVDnbWGOnFoGISHk1TV5vwFDgy3BVqbvXeRjohqQWgYhIedV+LQ4v+s+6e0n4k9JJAOJaBBprSEQESKyP4H0zGxZ5JEmy+4EytQhERKCa0pCZ7RMOJf0d4FIzWwXkE8w85u6ekslh9xATahGIiED1fQTvA8OACUmKJSkKi0owgxYZSgQiIlB9IjAAd1+VpFiSoqC4lJb7NCPoBxcRkeoSQWcz+2lVb7r7XRHEE7nCohL1D4iIxKkuEWQAbQhbBk1FQZFmJxMRiVddIljn7rckLZIkKSxWi0BEJF51X42bVEugjFoEIiLlVXdFPCFpUSSRWgQiIuVVmQjc/dtkBpIsahGIiJSXdlfEwuISjTMkIhIn7RJBQVGpRh4VEYmTdlfEguISWqpFICISk3aJoFAtAhGRctLuiqg+AhGR8tIvEahFICJSTtpdEQvUIhARKSetEoHjFJW4WgQiInEivSKa2Rgz+9TMVprZHhPem9lPzSzXzD42s9fNrFeU8ZSGE22qRSAisltkicDMMoB7gbFAFnCOmWVV2OwjIMfdDwNmA7dHFQ9AaTjlcqZaBCIiMVFeEYcDK919tbvvAp4ATovfwN3fcPcd4eI8oEeE8cQSgZ4jEBHZLcpE0B1YE7ecF66rysXAPyp7w8wmm9kCM1uwYcOGvQ6oNJiuWGMNiYjEifKKWNkw1l7phmbnAznAHZW97+7T3T3H3XM6d+681wF5WYtAo4+KiMRUNzFNXeUBB8Ut9wDWVtzIzE4Efgkc7+6FEcYT11msFoGISJkor4gfAAPMrI+ZtQDOBubEb2BmhwN/Aca7+/oIYwHi+gjUIhARiYksEbh7MXAl8DKwDHjS3Zea2S1mNj7c7A6CeZGfMrNFZjanit3Vi9hdQ2oRiIjERFkawt3nAnMrrLsx7vWJUR6/orLSkFoEIiK7pdVXY7UIRET2lFZXxNJS9RGIiFSUXomgrDSkFoGISExaXRE9VhpSi0BEpExaJYLdncVpddoiItVKqytiqTtm0CIjrU5bRKRakd4+2tiUejAXgVllo1+IpLeioiLy8vIoKCho6FCkDjIzM+nRowfNmzdP+DNplgjUPyBSlby8PNq2bUvv3r31ZSlFuTsbN24kLy+PPn36JPy5tKqRlJZqdjKRqhQUFNCxY0clgRRmZnTs2LHWrbq0uiqWuqtFIFINJYHUtzd/w7RKBO66Y0hEpKK0uiqqRSDSeG3evJn77rtvrz47btw4Nm/enPD2N910E927dyc7O5usrCwef/zx2HvuztSpUxkwYAADBw5k1KhRLF26NPb+9u3bueyyy+jXrx+DBw/muOOOY/78+XsVd2ORZokAMjW8hEijVF0iKCkpqfazc+fOZb/99qvV8a655hoWLVrE888/z2WXXUZRUREA9957L++++y6LFy9m+fLlXH/99YwfPz5Wd7/kkkvo0KEDK1asYOnSpcyYMYNvvvmmVseujrtTWjadYpKk2V1DruElRBJw89+Xkrt2a73uM6tbO3596uAq37/uuutYtWoV2dnZnHTSSZxyyincfPPNdO3alUWLFpGbm8uECRNYs2YNBQUF/PjHP2by5MkA9O7dmwULFrB9+3bGjh3Ld77zHd599126d+/O888/z7777lvlcQcMGECrVq3YtGkTXbp04Xe/+x1vvvkmrVq1AmD06NEcc8wxzJw5k5EjRzJ//nxmzpxJs2bBtaRv37707dt3j/2+9NJL/OIXv6CkpIROnTrx+uuvc9NNN9GmTRumTJkCwKGHHsoLL7wAwNixYxk1ahTvvfceEyZMID8/n9tvvx2AGTNmsHDhQu655x7+93//l2nTprFr1y6OOuoo7rvvPjIy6vYFN62uisFzBGoRiDRGt912G/369WPRokXccUcwa+3777/PrbfeSm5uLgAPPfQQCxcuZMGCBUybNo2NGzfusZ8VK1ZwxRVXsHTpUvbbbz+efvrpao/74YcfMmDAALp06cLWrVvJz8+nX79+5bbJyclh6dKlLF26lOzs7BovvBs2bODSSy/l6aefZvHixTz11FM1nv+nn37KpEmT+Oijj/jRj37EM888E3tv1qxZTJw4kWXLljFr1izeeecdFi1aREZGBjNnzqxx3zVJsxaBBpwTSUR139yTafjw4eXuh582bRrPPvssAGvWrGHFihV07Nix3Gf69OlDdnY2AEcccQSff/55pfu+++67+etf/8rq1at56aWXqo3D3Wt1N868efM47rjjYrF36NChxs/06tWLo48+GoDOnTvTt29f5s2bx4ABA/j000859thjuffee1m4cCFHHnkkADt37qRLly4Jx1WV9EoEpa4+ApEU0rp169jrN998k9dee4333nuPVq1aMXLkyErvl2/ZsmXsdUZGBjt37qx039dccw1TpkzhmWeeYdKkSaxatYp27drRunVrVq9eXa7c8+GHH3L88cczePBgFi9eTGlpaaw0VJmqEsc+++xTrv4fH3/8uQJMnDiRJ598kkMOOYTTTz8dM8PdueCCC/jtb39b5bH3Rlp9PXb1EYg0Wm3btmXbtm1Vvr9lyxb2339/WrVqxb///W/mzZtXL8c944wzyMnJ4ZFHHgHg2muv5eqrr44lkNdee41//etfnHvuufTr14+cnBx+/etfx0YzXrFiBc8//3y5fY4YMYK33nqLzz77DIBvv/0WCPoyPvzwQyBILmXvVxXXc889x+OPP87EiRMBOOGEE5g9ezbr16+P7feLL76o8+8gra6KumtIpPHq2LEjxx57LIceeijXXnvtHu+PGTOG4uJiDjvsMH71q1/Fyij14cYbb+Suu+6itLSUq666iiOPPJIhQ4Zw8MEH85vf/KZch/MDDzzAV199Rf/+/RkyZAiXXnop3bp1K7e/zp07M336dM444wyGDh0au5B///vf59tvvyU7O5v777+fgQMHVhnT/vvvT1ZWFl988QXDhw8HICsri6lTpzJ69GgOO+wwTjrpJNatW1fn87eyrJYqcnJyfMGCBbX/4MOnMP+zjbx5zAx+PuaQ+g9MJMUtW7aMQYMGNXQYUg8q+1ua2UJ3z6ls+7RpETiOoxaBiEhFaZMINE2liEjl0uaqWFo2TaXGGhIRKSdtroqlpZqvWESkMumTCFQaEhGpVNpcFXeXhtQiEBGJlzaJwNUiEGnU6jIMNcAf/vAHduzYUel7I0eO5OCDD2bo0KEceeSRLFq0KPbeli1bmDRpEv369aNfv35MmjSJLVu2xN5fvnw548aNo3///gwaNIizzjqLr7/+eq/jbIzS5qqoFoFI4xZlIgCYOXMmixcv5kc/+lG5B9Yuvvhi+vbty6pVq1i1ahV9+vThkksuAYIhIE455RQuv/xyVq5cybJly7j88svZsGHDXsdZUXFxcb3ta2+lzVhDZYlALQKRBPzjOvhqSf3u88AhMPa2Kt+uOAz1HXfcwR133MGTTz5JYWEhp59+OjfffDP5+fmcddZZ5OXlUVJSwq9+9Su+/vpr1q5dy6hRo+jUqRNvvPFGlccZMWJEbHTTlStXsnDhQmbNmhV7/8Ybb6R///6sWrWKt956ixEjRnDqqafG3h81alSl+7399tt59NFHadasGWPHjuW2225j5MiR3HnnneTk5PDNN9+Qk5PD559/zowZM3jxxRcpKCggPz+fzp07c8EFFzBu3DgALrzwQk499VQmTJjAddddx3Aao4EAAAxqSURBVJtvvklhYSFXXHEFl112Wa1+7YlIo0QQ/FfDUIs0TrfddhuffPJJrGzzyiuvsGLFCt5//33cnfHjx/P222+zYcMGunXrxosvvggEpZ327dtz11138cYbb9CpU6dqj/PSSy8xYcIEAHJzc/cYVjojI4Ps7GyWLl3KJ598whFHHFFj7P/4xz947rnnmD9/Pq1atYqNLVSd9957j48//pgOHTrw7LPPMmvWLMaNG8euXbt4/fXXuf/++3nwwQdp3749H3zwAYWFhRx77LGMHj263Iis9SGNEkHZ7aNqEYjUqJpv7snyyiuv8Morr3D44YcDwRSRK1as4Lvf/S5Tpkzh5z//Od/73vf47ne/m9D+zjvvPPLz8ykpKYkN/FbVKKG1HXb6tdde46KLLopNZpPIsNMnnXRSbLuxY8dy9dVXU1hYyEsvvcRxxx3HvvvuyyuvvMLHH3/M7NmzgSDprVixIrUSgZmNAf4IZAAPuPttFd5vCfwNOALYCEx098+jiKXsOQK1CERSg7tz/fXXV1oKWbhwIXPnzuX6669n9OjR3HjjjTXub+bMmQwdOpTrrruOK664gmeeeYbBgwfz0UcflRtWurS0lMWLFzNo0CDWr1/PW2+9lVCsNQ07XXHI7PhhpzMzMxk5ciQvv/wys2bN4pxzzont95577uHkk0+uMYa6iOzrsZllAPcCY4Es4Bwzy6qw2cXAJnfvD9wN/C6qePQcgUjjVnEY6pNPPpmHHnqI7du3A/Dll1+yfv161q5dS6tWrTj//POZMmVK7Nt9TcNYAzRv3pypU6cyb948li1bRv/+/Tn88MOZOnVqbJupU6cybNgw+vfvz7nnnsu7774bK0NBUFpasqR8/8no0aN56KGHYp3V8cNOL1y4ECD2rb4qZ599Ng8//DD//Oc/Yxf+k08+mfvvvz82n/Ly5cvJz8+vdj97I8qr4nBgpbuvdvddwBPAaRW2OQ14JHw9GzjBatMeqwV3PVks0phVHIZ69OjRnHvuuYwYMYIhQ4Zw5plnsm3bNpYsWcLw4cPJzs7m1ltv5YYbbgBg8uTJsXl/q7Pvvvvys5/9jDvvvBOABx98kOXLl9O/f3/69evH8uXLefDBB2PbvvDCC9xzzz0MGDCArKwsZsyYscesYGPGjGH8+PHk5OSQnZ0d2/eUKVO4//77OeaYY2qc4H706NG8/fbbnHjiibRo0QKASy65hKysLIYNG8ahhx7KZZddFsldRpENQ21mZwJj3P2ScPkHwFHufmXcNp+E2+SFy6vCbb6psK/JwGSAnj17HrE3EzF8MfMqVq3fzneuepAWGm9IZA8ahrrpqO0w1FH2EVT2zb5i1klkG9x9OjAdgvkI9iaYXufdQ6+9+aCISBMX5VfjPOCguOUewNqqtjGzfYD2QM33XYmISL2JMhF8AAwwsz5m1gI4G5hTYZs5wAXh6zOB//NUmzJNpAnR/36pb2/+hpElAncvBq4EXgaWAU+6+1Izu8XMxoebPQh0NLOVwE+B66KKR0Sql5mZycaNG5UMUpi7s3HjRjIzM2v1ufSZs1hEqlVUVEReXt4e97tLasnMzKRHjx40b9683PqG6iwWkRTSvHnzen9iVVKD7qMUEUlzSgQiImlOiUBEJM2lXGexmW0Aav9ocaATUP1z3k2Pzjk96JzTQ13OuZe7d67sjZRLBHVhZguq6jVvqnTO6UHnnB6iOmeVhkRE0pwSgYhImku3RDC9oQNoADrn9KBzTg+RnHNa9RGIiMie0q1FICIiFSgRiIikuSaZCMxsjJl9amYrzWyPEU3NrKWZzQrfn29mvZMfZf1K4Jx/ama5Zvaxmb1uZik/T09N5xy33Zlm5maW8rcaJnLOZnZW+LdeamaPJTvG+pbAv+2eZvaGmX0U/vse1xBx1hcze8jM1oczOFb2vpnZtPD38bGZDavzQd29Sf0AGcAqoC/QAlgMZFXY5kfAn8PXZwOzGjruJJzzKKBV+PrydDjncLu2wNvAPCCnoeNOwt95APARsH+43KWh407COU8HLg9fZwGfN3TcdTzn44BhwCdVvD8O+AfBDI9HA/Presym2CIYDqx099Xuvgt4AjitwjanAY+Er2cDJ5hZZdNmpooaz9nd33D3HeHiPIIZ41JZIn9ngN8AtwNNYWzlRM75UuBed98E4O7rkxxjfUvknB1oF75uz54zIaYUd3+b6mdqPA34mwfmAfuZWde6HLMpJoLuwJq45bxwXaXbeDCBzhagY1Kii0Yi5xzvYoJvFKmsxnM2s8OBg9z9hWQGFqFE/s4DgYFm9o6ZzTOzMUmLLhqJnPNNwPlmlgfMBa5KTmgNprb/v9eoKc5HUNk3+4r3yCayTSpJ+HzM7HwgBzg+0oiiV+05m1kz4G7gwmQFlASJ/J33ISgPjSRo9f3TzA51980RxxaVRM75HGCGu//ezEYAj4bnXBp9eA2i3q9fTbFFkAccFLfcgz2birFtzGwfguZkdU2xxi6Rc8bMTgR+CYx398IkxRaVms65LXAo8KaZfU5QS52T4h3Gif7bft7di9z9M+BTgsSQqhI554uBJwHc/T0gk2BwtqYqof/fa6MpJoIPgAFm1sfMWhB0Bs+psM0c4ILw9ZnA/3nYC5OiajznsEzyF4IkkOp1Y6jhnN19i7t3cvfe7t6boF9kvLun8jynifzbfo7gxgDMrBNBqWh1UqOsX4mc83+AEwDMbBBBItiQ1CiTaw4wKbx76Ghgi7uvq8sOm1xpyN2LzexK4GWCOw4ecvelZnYLsMDd5wAPEjQfVxK0BM5uuIjrLsFzvgNoAzwV9ov/x93HN1jQdZTgOTcpCZ7zy8BoM8sFSoBr3X1jw0VdNwme88+Av5rZNQQlkgtT+YudmT1OUNrrFPZ7/BpoDuDufyboBxkHrAR2ABfV+Zgp/PsSEZF60BRLQyIiUgtKBCIiaU6JQEQkzSkRiIikOSUCEZE0p0QgjY6ZlZjZorif3tVs27uqURprecw3wxEuF4fDMxy8F/v4oZlNCl9faGbd4t57wMyy6jnOD8wsO4HP/MTMWtX12NJ0KRFIY7TT3bPjfj5P0nHPc/ehBAMS3lHbD7v7n939b+HihUC3uPcucffceolyd5z3kVicPwGUCKRKSgSSEsJv/v80sw/Dn2Mq2Wawmb0ftiI+NrMB4frz49b/xcwyajjc20D/8LMnhOPcLwnHiW8Zrr/Nds/vcGe47iYzm2JmZxKM5zQzPOa+4Tf5HDO73Mxuj4v5QjO7Zy/jfI+4wcbM7H4zW2DBPAQ3h+uuJkhIb5jZG+G60Wb2Xvh7fMrM2tRwHGnilAikMdo3riz0bLhuPXCSuw8DJgLTKvncD4E/uns2wYU4LxxyYCJwbLi+BDivhuOfCiwxs0xgBjDR3YcQPIl/uZl1AE4HBrv7YcDU+A+7+2xgAcE392x33xn39mzgjLjlicCsvYxzDMGQEmV+6e45wGHA8WZ2mLtPIxiHZpS7jwqHnbgBODH8XS4AflrDcaSJa3JDTEiTsDO8GMZrDvwprImXEIyhU9F7wC/NrAfwjLuvMLMTgCOAD8KhNfYlSCqVmWlmO4HPCYYyPhj4zN2Xh+8/AlwB/IlgfoMHzOxFIOFhrt19g5mtDseIWREe451wv7WJszXBkAvxs1OdZWaTCf6/7kowScvHFT57dLj+nfA4LQh+b5LGlAgkVVwDfA0MJWjJ7jHRjLs/ZmbzgVOAl83sEoIhex9x9+sTOMZ58YPSmVmlc1SE498MJxjo7GzgSuC/anEus4CzgH8Dz7q7W3BVTjhOgpm6bgPuBc4wsz7AFOBId99kZjMIBl+ryIBX3f2cWsQrTZxKQ5Iq2gPrwjHmf0DwbbgcM+sLrA7LIXMISiSvA2eaWZdwmw6W+HzN/wZ6m1n/cPkHwFthTb29u88l6Iit7M6dbQRDYVfmGWACwTj6s8J1tYrT3YsISjxHh2WldkA+sMXMDgDGVhHLPODYsnMys1ZmVlnrStKIEoGkivuAC8xsHkFZKL+SbSYCn5jZIuAQgun8cgkumK+Y2cfAqwRlkxq5ewHByI5PmdkSoBT4M8FF9YVwf28RtFYqmgH8uayzuMJ+NwG5QC93fz9cV+s4w76H3wNT3H0xwVzFS4GHCMpNZaYD/zCzN9x9A8EdTY+Hx5lH8LuSNKbRR0VE0pxaBCIiaU6JQEQkzSkRiIikOSUCEZE0p0QgIpLmlAhERNKcEoGISJr7/3lz1SX0NayBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_fpr, train_tpr, train_thresholds = roc_curve(train_y, train_pro_class0)\n",
    "test_fpr, test_tpr, test_thresholds = roc_curve(test_y, test_pro_class0)\n",
    "plt.plot(train_fpr, train_tpr, label='train ROC curve')\n",
    "plt.plot(test_fpr, test_tpr, label='test ROC curve')\n",
    "plt.title('ROC curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v. One can expect that supervised learning on the full data set works better than semi-supervised learning with half of the data set labeled.One can expect that unsupervised learning underperforms in such situations. Compare the results you obtained by those methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>supervised</th>\n",
       "      <th>semi_supervised</th>\n",
       "      <th>unsupervised</th>\n",
       "      <th>spectral_unsupervised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>train_accuracy</td>\n",
       "      <td>0.9821</td>\n",
       "      <td>0.9690</td>\n",
       "      <td>0.8862</td>\n",
       "      <td>0.8818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>test_accuracy</td>\n",
       "      <td>0.9681</td>\n",
       "      <td>0.9529</td>\n",
       "      <td>0.8880</td>\n",
       "      <td>0.5208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>train_precision</td>\n",
       "      <td>0.9847</td>\n",
       "      <td>0.9675</td>\n",
       "      <td>0.9609</td>\n",
       "      <td>0.9583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>test_precision</td>\n",
       "      <td>0.9619</td>\n",
       "      <td>0.9484</td>\n",
       "      <td>0.9528</td>\n",
       "      <td>0.5645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>train_recall</td>\n",
       "      <td>0.9673</td>\n",
       "      <td>0.9494</td>\n",
       "      <td>0.7251</td>\n",
       "      <td>0.7149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>test_recall</td>\n",
       "      <td>0.9524</td>\n",
       "      <td>0.9238</td>\n",
       "      <td>0.7325</td>\n",
       "      <td>0.5071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>train_f1</td>\n",
       "      <td>0.9759</td>\n",
       "      <td>0.9581</td>\n",
       "      <td>0.8262</td>\n",
       "      <td>0.8188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>test_f1</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.9350</td>\n",
       "      <td>0.8272</td>\n",
       "      <td>0.5163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>train_auc</td>\n",
       "      <td>0.9984</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9088</td>\n",
       "      <td>0.5945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>test_auc</td>\n",
       "      <td>0.9949</td>\n",
       "      <td>0.9875</td>\n",
       "      <td>0.9170</td>\n",
       "      <td>0.6150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 supervised  semi_supervised  unsupervised  \\\n",
       "train_accuracy       0.9821           0.9690        0.8862   \n",
       "test_accuracy        0.9681           0.9529        0.8880   \n",
       "train_precision      0.9847           0.9675        0.9609   \n",
       "test_precision       0.9619           0.9484        0.9528   \n",
       "train_recall         0.9673           0.9494        0.7251   \n",
       "test_recall          0.9524           0.9238        0.7325   \n",
       "train_f1             0.9759           0.9581        0.8262   \n",
       "test_f1              0.9565           0.9350        0.8272   \n",
       "train_auc            0.9984           0.9929        0.9088   \n",
       "test_auc             0.9949           0.9875        0.9170   \n",
       "\n",
       "                 spectral_unsupervised  \n",
       "train_accuracy                  0.8818  \n",
       "test_accuracy                   0.5208  \n",
       "train_precision                 0.9583  \n",
       "test_precision                  0.5645  \n",
       "train_recall                    0.7149  \n",
       "test_recall                     0.5071  \n",
       "train_f1                        0.8188  \n",
       "test_f1                         0.5163  \n",
       "train_auc                       0.5945  \n",
       "test_auc                        0.6150  "
      ]
     },
     "execution_count": 705,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_table = pd.DataFrame({'supervised':supervised_result.mean(), \n",
    "                              'semi_supervised':semi_supervised_result.mean(),\n",
    "                              'unsupervised':unsupervised_result.mean(), \n",
    "                              'spectral_unsupervised':spectral_result.mean()})\n",
    "round(compare_table,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion:\n",
    "    From the reslut table, I think supervised learning is better than semi-supervised learning. However, compare to unsupervised learning, supervised and semi-supervised learning are outperforms. In this dataset, all models are better to predict class0, but it's poorly to perdict class1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Active Learning Using Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Download the banknote authentication Data Set from: \n",
    "    https://archive.ics.uci.edu/ml/datasets/banknote+authentication. Choose 472 data points randomly as the test set, and the remaining 900 points as the training set. This is abinary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.62160</td>\n",
       "      <td>8.66610</td>\n",
       "      <td>-2.8073</td>\n",
       "      <td>-0.44699</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.54590</td>\n",
       "      <td>8.16740</td>\n",
       "      <td>-2.4586</td>\n",
       "      <td>-1.46210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.86600</td>\n",
       "      <td>-2.63830</td>\n",
       "      <td>1.9242</td>\n",
       "      <td>0.10645</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.45660</td>\n",
       "      <td>9.52280</td>\n",
       "      <td>-4.0112</td>\n",
       "      <td>-3.59440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.32924</td>\n",
       "      <td>-4.45520</td>\n",
       "      <td>4.5718</td>\n",
       "      <td>-0.98880</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1367</td>\n",
       "      <td>0.40614</td>\n",
       "      <td>1.34920</td>\n",
       "      <td>-1.4501</td>\n",
       "      <td>-0.55949</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1368</td>\n",
       "      <td>-1.38870</td>\n",
       "      <td>-4.87730</td>\n",
       "      <td>6.4774</td>\n",
       "      <td>0.34179</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1369</td>\n",
       "      <td>-3.75030</td>\n",
       "      <td>-13.45860</td>\n",
       "      <td>17.5932</td>\n",
       "      <td>-2.77710</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>-3.56370</td>\n",
       "      <td>-8.38270</td>\n",
       "      <td>12.3930</td>\n",
       "      <td>-1.28230</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1371</td>\n",
       "      <td>-2.54190</td>\n",
       "      <td>-0.65804</td>\n",
       "      <td>2.6842</td>\n",
       "      <td>1.19520</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1372 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1        2        3  4\n",
       "0     3.62160   8.66610  -2.8073 -0.44699  0\n",
       "1     4.54590   8.16740  -2.4586 -1.46210  0\n",
       "2     3.86600  -2.63830   1.9242  0.10645  0\n",
       "3     3.45660   9.52280  -4.0112 -3.59440  0\n",
       "4     0.32924  -4.45520   4.5718 -0.98880  0\n",
       "...       ...       ...      ...      ... ..\n",
       "1367  0.40614   1.34920  -1.4501 -0.55949  1\n",
       "1368 -1.38870  -4.87730   6.4774  0.34179  1\n",
       "1369 -3.75030 -13.45860  17.5932 -2.77710  1\n",
       "1370 -3.56370  -8.38270  12.3930 -1.28230  1\n",
       "1371 -2.54190  -0.65804   2.6842  1.19520  1\n",
       "\n",
       "[1372 rows x 5 columns]"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_2 = pd.read_csv('data_banknote_authentication.txt', header=None)\n",
    "dataset_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.62160</td>\n",
       "      <td>8.6661</td>\n",
       "      <td>-2.8073</td>\n",
       "      <td>-0.44699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.54590</td>\n",
       "      <td>8.1674</td>\n",
       "      <td>-2.4586</td>\n",
       "      <td>-1.46210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.86600</td>\n",
       "      <td>-2.6383</td>\n",
       "      <td>1.9242</td>\n",
       "      <td>0.10645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.45660</td>\n",
       "      <td>9.5228</td>\n",
       "      <td>-4.0112</td>\n",
       "      <td>-3.59440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.32924</td>\n",
       "      <td>-4.4552</td>\n",
       "      <td>4.5718</td>\n",
       "      <td>-0.98880</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1       2        3\n",
       "0  3.62160  8.6661 -2.8073 -0.44699\n",
       "1  4.54590  8.1674 -2.4586 -1.46210\n",
       "2  3.86600 -2.6383  1.9242  0.10645\n",
       "3  3.45660  9.5228 -4.0112 -3.59440\n",
       "4  0.32924 -4.4552  4.5718 -0.98880"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_x2 = dataset_2.iloc[:,:-1]\n",
    "dataset_y2 = dataset_2[4]\n",
    "dataset_x2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.50680</td>\n",
       "      <td>1.158800</td>\n",
       "      <td>3.92490</td>\n",
       "      <td>0.125850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-2.59120</td>\n",
       "      <td>-0.105540</td>\n",
       "      <td>1.27980</td>\n",
       "      <td>1.041400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.38214</td>\n",
       "      <td>8.390900</td>\n",
       "      <td>2.16240</td>\n",
       "      <td>-3.740500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.04660</td>\n",
       "      <td>2.030000</td>\n",
       "      <td>2.17610</td>\n",
       "      <td>-0.083634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.96470</td>\n",
       "      <td>6.938300</td>\n",
       "      <td>0.57722</td>\n",
       "      <td>0.663770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>895</td>\n",
       "      <td>-1.00500</td>\n",
       "      <td>0.084831</td>\n",
       "      <td>-0.24620</td>\n",
       "      <td>0.456880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>896</td>\n",
       "      <td>-0.39816</td>\n",
       "      <td>5.978100</td>\n",
       "      <td>1.39120</td>\n",
       "      <td>-1.162100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>897</td>\n",
       "      <td>-0.98540</td>\n",
       "      <td>-6.661000</td>\n",
       "      <td>5.82450</td>\n",
       "      <td>0.546100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>898</td>\n",
       "      <td>0.34340</td>\n",
       "      <td>0.124150</td>\n",
       "      <td>-0.28733</td>\n",
       "      <td>0.146540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>899</td>\n",
       "      <td>4.17110</td>\n",
       "      <td>8.722000</td>\n",
       "      <td>-3.02240</td>\n",
       "      <td>-0.596990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1        2         3\n",
       "0    2.50680  1.158800  3.92490  0.125850\n",
       "1   -2.59120 -0.105540  1.27980  1.041400\n",
       "2   -0.38214  8.390900  2.16240 -3.740500\n",
       "3    2.04660  2.030000  2.17610 -0.083634\n",
       "4    1.96470  6.938300  0.57722  0.663770\n",
       "..       ...       ...      ...       ...\n",
       "895 -1.00500  0.084831 -0.24620  0.456880\n",
       "896 -0.39816  5.978100  1.39120 -1.162100\n",
       "897 -0.98540 -6.661000  5.82450  0.546100\n",
       "898  0.34340  0.124150 -0.28733  0.146540\n",
       "899  4.17110  8.722000 -3.02240 -0.596990\n",
       "\n",
       "[900 rows x 4 columns]"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split dataset 2 into training data and test data\n",
    "train_x2, train_y2, test_x2, test_y2 = split_dataset(dataset_x2, dataset_y2, \n",
    "                                                     test_size = (472/(472+900)), random = None)\n",
    "\n",
    "train_x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Repeat each of the following two procedures 50 times. You will have 50 errors for 90 SVMs per each procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i. Train a SVM with a pool of 10 randomly selected data points from the training set using linear kernel and L1 penalty.\n",
    "    Select the penalty parameter using 10-fold cross validation. Repeat this process by adding 10 other randomly selected data points to the pool, until you use all the 900 points. Do NOT replace the samples back into the training set at each step. Calculate the test error for each SVM. You will have 90 SVMs that were trained using 10, 20, 30, ... , 900 data points and their 90 test errors. You have implemented passive learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_10_sample(train_x2, train_y2):\n",
    "    select_x = pd.DataFrame()\n",
    "    select_y = pd.DataFrame()\n",
    "    #try to keep balance of select sample\n",
    "    while len(select_y) < 1 or len(np.bincount(select_y)) < 2 or np.bincount(select_y).min() < 2:\n",
    "        rand_index = np.random.randint(0,train_x2.shape[0],10)\n",
    "        select_x = train_x2.iloc[rand_index]\n",
    "        select_y = train_y2.iloc[rand_index]\n",
    "        \n",
    "    unselect_x = train_x2.drop(train_x2.index[rand_index])\n",
    "    unselect_y = train_y2.drop(train_y2.index[rand_index])\n",
    "    \n",
    "    #reset index\n",
    "    select_x = select_x.reset_index(drop=True)\n",
    "    select_y = select_y.reset_index(drop=True)\n",
    "    unselect_x = unselect_x.reset_index(drop=True)\n",
    "    unselect_y = unselect_y.reset_index(drop=True)\n",
    "    return select_x, select_y, unselect_x, unselect_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "def passive_learning():\n",
    "    train_x2, train_y2, test_x2, test_y2 = split_dataset(dataset_x2, dataset_y2, \n",
    "                                                     test_size = (472/(472+900)), random = None)\n",
    "    #first select, get 10 sample as the first training data\n",
    "    select_x, select_y, unselect_x, unselect_y = select_10_sample(train_x2, train_y2)\n",
    "    passive_errors = list()\n",
    "    \n",
    "        \n",
    "    for i in range(90):\n",
    "        CV=5\n",
    "        parameters = {'C':np.power(10, np.arange(-3, 7,0.5))}\n",
    "        svc = LinearSVC(penalty='l1', dual=False)\n",
    "        clf = GridSearchCV(svc, parameters, cv=CV)\n",
    "        clf.fit(select_x, select_y)\n",
    "        best_C = clf.best_params_['C']\n",
    "\n",
    "        # Refit the model with the best params\n",
    "        svc = LinearSVC(penalty='l1', dual=False, C=best_C)\n",
    "        svc.fit(select_x, select_y)\n",
    "        \n",
    "        passive_errors.append(1-svc.score(test_x2, test_y2))\n",
    "        \n",
    "        if(i!=89):\n",
    "            new_select_x = unselect_x.sample(n=10, replace=False, random_state=None, axis=0)\n",
    "            rand_index = np.array(new_select_x.index)\n",
    "            new_select_y = unselect_y.iloc[rand_index]\n",
    "            unselect_x = unselect_x.drop(train_x2.index[rand_index])\n",
    "            unselect_y = unselect_y.drop(train_y2.index[rand_index])\n",
    "            \n",
    "            #reset index\n",
    "            unselect_x = unselect_x.reset_index(drop=True)\n",
    "            unselect_y = unselect_y.reset_index(drop=True)\n",
    "            select_x = pd.concat([select_x, new_select_x],axis=0, ignore_index=True)\n",
    "            select_y = pd.concat([select_y, new_select_y],axis=0, ignore_index=True)\n",
    "            \n",
    "    return passive_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete 0 times\n",
      "complete 1 times\n",
      "complete 2 times\n",
      "complete 3 times\n",
      "complete 4 times\n",
      "complete 5 times\n",
      "complete 6 times\n",
      "complete 7 times\n",
      "complete 8 times\n",
      "complete 9 times\n",
      "complete 10 times\n",
      "complete 11 times\n",
      "complete 12 times\n",
      "complete 13 times\n",
      "complete 14 times\n",
      "complete 15 times\n",
      "complete 16 times\n",
      "complete 17 times\n",
      "complete 18 times\n",
      "complete 19 times\n",
      "complete 20 times\n",
      "complete 21 times\n",
      "complete 22 times\n",
      "complete 23 times\n",
      "complete 24 times\n",
      "complete 25 times\n",
      "complete 26 times\n",
      "complete 27 times\n",
      "complete 28 times\n",
      "complete 29 times\n",
      "complete 30 times\n",
      "complete 31 times\n",
      "complete 32 times\n",
      "complete 33 times\n",
      "complete 34 times\n",
      "complete 35 times\n",
      "complete 36 times\n",
      "complete 37 times\n",
      "complete 38 times\n",
      "complete 39 times\n",
      "complete 40 times\n",
      "complete 41 times\n",
      "complete 42 times\n",
      "complete 43 times\n",
      "complete 44 times\n",
      "complete 45 times\n",
      "complete 46 times\n",
      "complete 47 times\n",
      "complete 48 times\n",
      "complete 49 times\n"
     ]
    }
   ],
   "source": [
    "passive_result = list()\n",
    "for i in range(50):\n",
    "    print(\"complete\", i, \"times\")\n",
    "    error = passive_learning()\n",
    "    passive_result.append(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.055085</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.052966</td>\n",
       "      <td>0.052966</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.141949</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.093220</td>\n",
       "      <td>0.080508</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.052966</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.076271</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.139831</td>\n",
       "      <td>0.129237</td>\n",
       "      <td>0.129237</td>\n",
       "      <td>0.110169</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.065678</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.207627</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.076271</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.171610</td>\n",
       "      <td>0.080508</td>\n",
       "      <td>0.055085</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.341102</td>\n",
       "      <td>0.055085</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.105932</td>\n",
       "      <td>0.180085</td>\n",
       "      <td>0.137712</td>\n",
       "      <td>0.076271</td>\n",
       "      <td>0.065678</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.188559</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.101695</td>\n",
       "      <td>0.120763</td>\n",
       "      <td>0.118644</td>\n",
       "      <td>0.052966</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.154661</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.065678</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.065678</td>\n",
       "      <td>0.080508</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.296610</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.315678</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.078390</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.129237</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.139831</td>\n",
       "      <td>0.103814</td>\n",
       "      <td>0.086864</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.097458</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.338983</td>\n",
       "      <td>0.141949</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.095339</td>\n",
       "      <td>0.078390</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.163136</td>\n",
       "      <td>0.139831</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.108051</td>\n",
       "      <td>0.095339</td>\n",
       "      <td>0.082627</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.082627</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.103814</td>\n",
       "      <td>0.088983</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.052966</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.129237</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.165254</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.114407</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.088983</td>\n",
       "      <td>0.088983</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.065678</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.002119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.065678</td>\n",
       "      <td>0.065678</td>\n",
       "      <td>0.052966</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.065678</td>\n",
       "      <td>0.052966</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.167373</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.055085</td>\n",
       "      <td>0.105932</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.175847</td>\n",
       "      <td>0.052966</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.052966</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.088983</td>\n",
       "      <td>0.065678</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.122881</td>\n",
       "      <td>0.057203</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.108051</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.061441</td>\n",
       "      <td>0.105932</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.163136</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.220339</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.061441</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.052966</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.197034</td>\n",
       "      <td>0.091102</td>\n",
       "      <td>0.146186</td>\n",
       "      <td>0.118644</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   0.055085  0.019068  0.023305  0.019068  0.019068  0.016949  0.016949   \n",
       "1   0.050847  0.052966  0.052966  0.046610  0.036017  0.141949  0.031780   \n",
       "2   0.093220  0.080508  0.023305  0.025424  0.019068  0.019068  0.052966   \n",
       "3   0.036017  0.040254  0.012712  0.010593  0.025424  0.023305  0.033898   \n",
       "4   0.084746  0.069915  0.076271  0.059322  0.044492  0.044492  0.021186   \n",
       "5   0.139831  0.129237  0.129237  0.110169  0.044492  0.046610  0.033898   \n",
       "6   0.038136  0.046610  0.014831  0.014831  0.014831  0.012712  0.012712   \n",
       "7   0.207627  0.084746  0.076271  0.021186  0.027542  0.027542  0.021186   \n",
       "8   0.171610  0.080508  0.055085  0.016949  0.016949  0.025424  0.025424   \n",
       "9   0.019068  0.050847  0.044492  0.016949  0.016949  0.012712  0.014831   \n",
       "10  0.341102  0.055085  0.059322  0.025424  0.027542  0.027542  0.027542   \n",
       "11  0.105932  0.180085  0.137712  0.076271  0.065678  0.031780  0.031780   \n",
       "12  0.059322  0.036017  0.027542  0.033898  0.023305  0.023305  0.042373   \n",
       "13  0.188559  0.031780  0.029661  0.029661  0.021186  0.031780  0.038136   \n",
       "14  0.101695  0.120763  0.118644  0.052966  0.016949  0.012712  0.029661   \n",
       "15  0.154661  0.044492  0.033898  0.044492  0.023305  0.010593  0.008475   \n",
       "16  0.033898  0.042373  0.065678  0.027542  0.027542  0.027542  0.027542   \n",
       "17  0.065678  0.080508  0.038136  0.042373  0.042373  0.038136  0.012712   \n",
       "18  0.296610  0.042373  0.021186  0.019068  0.012712  0.012712  0.012712   \n",
       "19  0.315678  0.033898  0.078390  0.027542  0.023305  0.025424  0.021186   \n",
       "20  0.129237  0.036017  0.021186  0.021186  0.016949  0.025424  0.025424   \n",
       "21  0.139831  0.103814  0.086864  0.014831  0.012712  0.019068  0.021186   \n",
       "22  0.097458  0.135593  0.027542  0.027542  0.019068  0.016949  0.023305   \n",
       "23  0.338983  0.141949  0.040254  0.014831  0.010593  0.033898  0.004237   \n",
       "24  0.059322  0.021186  0.095339  0.078390  0.016949  0.021186  0.016949   \n",
       "25  0.163136  0.139831  0.008475  0.008475  0.010593  0.010593  0.006356   \n",
       "26  0.108051  0.095339  0.082627  0.029661  0.082627  0.008475  0.012712   \n",
       "27  0.040254  0.014831  0.103814  0.088983  0.019068  0.016949  0.014831   \n",
       "28  0.052966  0.025424  0.012712  0.036017  0.036017  0.021186  0.012712   \n",
       "29  0.129237  0.016949  0.014831  0.012712  0.014831  0.010593  0.010593   \n",
       "30  0.165254  0.048729  0.021186  0.012712  0.008475  0.006356  0.006356   \n",
       "31  0.114407  0.050847  0.088983  0.088983  0.040254  0.021186  0.033898   \n",
       "32  0.125000  0.065678  0.021186  0.027542  0.016949  0.012712  0.002119   \n",
       "33  0.125000  0.046610  0.065678  0.065678  0.052966  0.048729  0.019068   \n",
       "34  0.069915  0.048729  0.065678  0.052966  0.010593  0.014831  0.023305   \n",
       "35  0.167373  0.029661  0.016949  0.012712  0.014831  0.008475  0.008475   \n",
       "36  0.055085  0.105932  0.023305  0.023305  0.023305  0.023305  0.023305   \n",
       "37  0.029661  0.135593  0.067797  0.014831  0.012712  0.012712  0.012712   \n",
       "38  0.175847  0.052966  0.048729  0.048729  0.048729  0.027542  0.052966   \n",
       "39  0.088983  0.065678  0.012712  0.012712  0.008475  0.008475  0.014831   \n",
       "40  0.122881  0.057203  0.048729  0.042373  0.031780  0.031780  0.031780   \n",
       "41  0.135593  0.012712  0.044492  0.033898  0.012712  0.016949  0.016949   \n",
       "42  0.125000  0.108051  0.059322  0.061441  0.105932  0.038136  0.021186   \n",
       "43  0.023305  0.023305  0.027542  0.027542  0.036017  0.031780  0.031780   \n",
       "44  0.040254  0.036017  0.067797  0.067797  0.014831  0.008475  0.010593   \n",
       "45  0.163136  0.008475  0.008475  0.008475  0.008475  0.008475  0.008475   \n",
       "46  0.220339  0.031780  0.029661  0.025424  0.040254  0.029661  0.031780   \n",
       "47  0.067797  0.061441  0.031780  0.052966  0.025424  0.046610  0.025424   \n",
       "48  0.031780  0.023305  0.023305  0.027542  0.027542  0.036017  0.025424   \n",
       "49  0.197034  0.091102  0.146186  0.118644  0.027542  0.019068  0.040254   \n",
       "\n",
       "          7         8         9   ...        80        81        82        83  \\\n",
       "0   0.019068  0.016949  0.012712  ...  0.008475  0.008475  0.008475  0.008475   \n",
       "1   0.029661  0.019068  0.014831  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "2   0.010593  0.014831  0.012712  ...  0.008475  0.008475  0.008475  0.008475   \n",
       "3   0.023305  0.023305  0.021186  ...  0.008475  0.006356  0.006356  0.006356   \n",
       "4   0.014831  0.014831  0.010593  ...  0.006356  0.006356  0.006356  0.006356   \n",
       "5   0.031780  0.031780  0.065678  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "6   0.016949  0.014831  0.016949  ...  0.006356  0.006356  0.006356  0.006356   \n",
       "7   0.019068  0.012712  0.016949  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "8   0.025424  0.019068  0.019068  ...  0.004237  0.004237  0.006356  0.006356   \n",
       "9   0.010593  0.010593  0.010593  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "10  0.019068  0.010593  0.012712  ...  0.004237  0.004237  0.004237  0.004237   \n",
       "11  0.025424  0.014831  0.021186  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "12  0.040254  0.042373  0.042373  ...  0.008475  0.008475  0.008475  0.008475   \n",
       "13  0.038136  0.021186  0.025424  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "14  0.010593  0.012712  0.010593  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "15  0.008475  0.016949  0.010593  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "16  0.027542  0.027542  0.027542  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "17  0.010593  0.012712  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "18  0.012712  0.012712  0.014831  ...  0.010593  0.012712  0.012712  0.010593   \n",
       "19  0.021186  0.016949  0.023305  ...  0.014831  0.012712  0.012712  0.012712   \n",
       "20  0.016949  0.029661  0.010593  ...  0.004237  0.004237  0.004237  0.004237   \n",
       "21  0.016949  0.016949  0.014831  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "22  0.016949  0.023305  0.025424  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "23  0.014831  0.025424  0.025424  ...  0.008475  0.008475  0.008475  0.008475   \n",
       "24  0.019068  0.019068  0.010593  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "25  0.016949  0.019068  0.016949  ...  0.006356  0.006356  0.006356  0.012712   \n",
       "26  0.014831  0.023305  0.023305  ...  0.008475  0.008475  0.008475  0.008475   \n",
       "27  0.025424  0.027542  0.023305  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "28  0.012712  0.010593  0.012712  ...  0.006356  0.006356  0.006356  0.006356   \n",
       "29  0.012712  0.021186  0.006356  ...  0.006356  0.008475  0.008475  0.008475   \n",
       "30  0.004237  0.008475  0.006356  ...  0.006356  0.006356  0.008475  0.008475   \n",
       "31  0.019068  0.033898  0.033898  ...  0.008475  0.008475  0.008475  0.008475   \n",
       "32  0.010593  0.010593  0.010593  ...  0.002119  0.002119  0.002119  0.002119   \n",
       "33  0.021186  0.059322  0.059322  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "34  0.008475  0.023305  0.010593  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "35  0.014831  0.008475  0.008475  ...  0.006356  0.006356  0.006356  0.006356   \n",
       "36  0.019068  0.016949  0.021186  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "37  0.019068  0.019068  0.019068  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "38  0.048729  0.044492  0.027542  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "39  0.014831  0.014831  0.014831  ...  0.006356  0.006356  0.006356  0.006356   \n",
       "40  0.040254  0.040254  0.010593  ...  0.006356  0.006356  0.006356  0.006356   \n",
       "41  0.014831  0.012712  0.025424  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "42  0.016949  0.016949  0.012712  ...  0.008475  0.008475  0.008475  0.008475   \n",
       "43  0.031780  0.031780  0.031780  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "44  0.010593  0.010593  0.010593  ...  0.008475  0.008475  0.008475  0.008475   \n",
       "45  0.006356  0.048729  0.048729  ...  0.008475  0.008475  0.008475  0.008475   \n",
       "46  0.021186  0.014831  0.016949  ...  0.002119  0.002119  0.004237  0.004237   \n",
       "47  0.023305  0.021186  0.016949  ...  0.008475  0.008475  0.008475  0.008475   \n",
       "48  0.025424  0.025424  0.027542  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "49  0.010593  0.027542  0.027542  ...  0.006356  0.006356  0.006356  0.006356   \n",
       "\n",
       "          84        85        86        87        88        89  \n",
       "0   0.008475  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "1   0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "2   0.008475  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "3   0.008475  0.008475  0.008475  0.008475  0.008475  0.006356  \n",
       "4   0.006356  0.006356  0.006356  0.006356  0.006356  0.006356  \n",
       "5   0.016949  0.016949  0.016949  0.016949  0.016949  0.016949  \n",
       "6   0.006356  0.006356  0.006356  0.006356  0.006356  0.006356  \n",
       "7   0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "8   0.004237  0.004237  0.004237  0.004237  0.008475  0.008475  \n",
       "9   0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "10  0.004237  0.004237  0.004237  0.004237  0.004237  0.004237  \n",
       "11  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "12  0.008475  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "13  0.016949  0.016949  0.016949  0.016949  0.016949  0.016949  \n",
       "14  0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "15  0.010593  0.010593  0.016949  0.010593  0.010593  0.010593  \n",
       "16  0.016949  0.016949  0.016949  0.016949  0.016949  0.016949  \n",
       "17  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "18  0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "19  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "20  0.004237  0.004237  0.004237  0.004237  0.004237  0.004237  \n",
       "21  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "22  0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "23  0.008475  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "24  0.012712  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "25  0.012712  0.014831  0.006356  0.006356  0.006356  0.006356  \n",
       "26  0.008475  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "27  0.010593  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "28  0.006356  0.006356  0.006356  0.006356  0.006356  0.006356  \n",
       "29  0.008475  0.008475  0.008475  0.008475  0.008475  0.006356  \n",
       "30  0.008475  0.008475  0.006356  0.006356  0.006356  0.006356  \n",
       "31  0.008475  0.008475  0.008475  0.008475  0.010593  0.010593  \n",
       "32  0.002119  0.002119  0.002119  0.002119  0.002119  0.002119  \n",
       "33  0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "34  0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "35  0.008475  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "36  0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "37  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "38  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "39  0.006356  0.006356  0.006356  0.006356  0.006356  0.006356  \n",
       "40  0.006356  0.006356  0.006356  0.004237  0.004237  0.004237  \n",
       "41  0.008475  0.008475  0.008475  0.012712  0.008475  0.010593  \n",
       "42  0.008475  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "43  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "44  0.008475  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "45  0.008475  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "46  0.004237  0.006356  0.004237  0.004237  0.004237  0.004237  \n",
       "47  0.008475  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "48  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "49  0.006356  0.006356  0.006356  0.006356  0.006356  0.006356  \n",
       "\n",
       "[50 rows x 90 columns]"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passive_result = pd.DataFrame(passive_result).T\n",
    "passive_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deXxU9b3/8dcne2Tf3AANVhYRQsCwtFivirdq69Jfi4pVL97aorW2Ll2kWqza2z6s19ZerdpqtXXpVdFeLVVaV1q1VQuoZZFF1AgRtBEBTRAC5PP743smmQwDmcScTMi8n4/HPGbmnO855zOHYT75fL9nMXdHREQkVV62AxARkc5JCUJERNJSghARkbSUIEREJC0lCBERSasg2wG0l/79+3tZWVm2wxAR2aMsXLjwPXcfkG5el0kQZWVlLFiwINthiIjsUczsrV3NUxeTiIikpQQhIiJpKUGIiEhaXWYMQkTaz7Zt26iurmbLli3ZDkXaSUlJCYMGDaKwsDDjZZQgRGQn1dXV9OjRg7KyMsws2+HIx+TurF+/nurqaoYMGZLxcupiEpGdbNmyhX79+ik5dBFmRr9+/VpdESpBiEhaSg5dS1v+PWNNEGZ2nJmtMLNVZjYzzfwjzOwlM9tuZlOTpleY2fNmttTMFpnZabEF+eGH8IMfwIsvxrYJEZE9UWwJwszygZuA44GRwOlmNjKl2WrgbOB/U6ZvBv7D3Q8FjgN+bma9Ywm0vh6uvloJQiRHrF27lqlTp7bcsAVVVVWMGjWqHSLavV/+8pfcddddsW8nnTgHqScAq9z9DQAzuw84GXg10cDdq6J5DckLuvvKpNdrzexfwABgY7tHWVISnnW0hkhO2H///XnwwQezHUYjd8fdyctL//f6eeed18ERNYmzi2kgsCbpfXU0rVXMbAJQBLyeZt4MM1tgZgtqamraFmUiQXz0UduWF5F2V1VVxYgRI5g+fTrl5eVMnTqVzZs3A3D11Vczfvx4Ro0axYwZM0jcFfOGG25g5MiRlJeXM23aNAD++te/UlFRQUVFBWPHjuXDDz9s9pf/xIkTWbp0aeN2jzzySBYuXEhdXR1f/vKXGT9+PGPHjuUPf/jDbuPdsWMH3/nOdxg/fjzl5eX86le/AqC2tpYpU6Ywbtw4Ro8e3bieqqoqDjnkEM4//3zGjRvHmjVr6N69O5dffjljxoxh0qRJvPvuuwBceeWVXHfddY3xXXrppUyYMIFhw4bx7LPPArB582ZOPfVUysvLOe2005g4cWK7XHoozgoi3YhIq+5vamb7AXcD0929IXW+u98K3ApQWVnZtnun5udDYaEqCJFduegieOWV9l1nRQX8/Oe7bbJixQpuv/12Jk+ezJe//GVuvvlmvv3tb3PBBRdwxRVXAHDWWWfxyCOPcOKJJ3LNNdfw5ptvUlxczMaNobPhuuuu46abbmLy5MnU1tZSkviDMDJt2jRmz57NVVddxbp161i7di2HHXYYl112GUcffTR33HEHGzduZMKECRxzzDF069Ytbay33347vXr1Yv78+WzdupXJkyfzmc98hsGDB/PQQw/Rs2dP3nvvPSZNmsRJJ53U+Pl+85vfcPPNNwNQV1fHpEmT+NGPfsR3v/tdbrvtNr7//e/vtK3t27fzj3/8g7lz53LVVVfx5JNPcvPNN9OnTx8WLVrEkiVLqKioaN2/xy7EWUFUA4OT3g8C1ma6sJn1BB4Fvu/uL7RzbM2VlqqCEOlkBg8ezOTJkwE488wzee655wCYN28eEydOZPTo0Tz99NONFUB5eTlnnHEG99xzDwUF4W/fyZMnc8kll3DDDTewcePGxukJp556Kg888AAAs2fP5pRTTgHg8ccf55prrqGiooIjjzySLVu2sHr16l3G+vjjj3PXXXdRUVHBxIkTWb9+Pa+99hruzmWXXUZ5eTnHHHMMb7/9dmNlcOCBBzJp0qTGdRQVFXHCCScAcNhhh1FVVZV2W1/4whd2avPcc881Vk2jRo2ivLw8gz3csjgriPnAUDMbArwNTAO+lMmCZlYEPATc5e4PxBdipKREFYTIrrTwl35cUg/LNDO2bNnC+eefz4IFCxg8eDBXXnll47H9jz76KM888wxz5szhhz/8IUuXLmXmzJl87nOfY+7cuUyaNIknn3yyWRUxcOBA+vXrx6JFi7j//vsbu4bcnd///vcMHz48o1jdnRtvvJFjjz222fTf/va31NTUsHDhQgoLCykrK2uMN7UaKSwsbPzM+fn5bN++Pe22iouLd2qT6GZrb7FVEO6+HbgAeAxYBsx296VmdrWZnQRgZuPNrBo4BfiVmSU6A08FjgDONrNXokf71EzpqIIQ6XRWr17N888/D8C9997L4Ycf3vjj2r9/f2praxsHmxsaGlizZg1HHXUU1157LRs3bqS2tpbXX3+d0aNHc+mll1JZWcny5ct32s60adO49tpr2bRpE6NHjwbg2GOP5cYbb2z84X355Zd3G+uxxx7LLbfcwrZt2wBYuXIldXV1bNq0ib333pvCwkLmzZvHW2/t8sraH8vhhx/O7NmzAXj11VdZvHhxu6w31kttuPtcYG7KtCuSXs8ndD2lLncPcE+csTWjCkKk0znkkEO48847Offccxk6dChf+9rX2GuvvfjqV7/K6NGjKSsrY/z48UAYJD7zzDPZtGkT7s7FF19M7969mTVrFvPmzSM/P5+RI0dy/PHHs27dumbbmTp1KhdeeCGzZs1qnDZr1iwuuugiysvLcXfKysp45JFHdhnrV77yFaqqqhg3bhzuzoABA3j44Yc544wzOPHEE6msrKSiooIRI0bEsq/OP//8xgH9sWPHUl5eTq9evT72ei2u0qSjVVZWeptH7ceOhcGDYc6c9g1KZA+1bNkyDjnkkKxtv6qqihNOOIElS5ZkLYY9yY4dO9i2bRslJSW8/vrrTJkyhZUrV1JUVNSsXbp/VzNb6O6V6dari/WBKggR2aNt3ryZo446im3btuHu3HLLLTslh7ZQggCNQYh0MmVlZaoeWqFHjx6x3HJZF+uDkCBUQYg001W6nyVoy7+nEgSELiZVECKNSkpKWL9+vZJEF5G4H0TqiYItURcTqItJJMWgQYOorq6mzZewkU4ncUe51lCCAA1Si6QoLCxs1Z3HpGtSFxOoghARSUMJAlRBiIikoQQBTRWEBuRERBopQUDTPSHq67Mbh4hIJ6IEAaGCAI1DiIgkUYIA3XZURCQNJQhQBSEikoYSBKiCEBFJQwkCVEGIiKShBAFNCUIVhIhIIyUIaOpiUgUhItJICQJUQYiIpKEEAaogRETSUIIADVKLiKShBAE6zFVEJA0lCFAFISKShhIEqIIQEUlDCQI0SC0ikoYSBEB+PhQWqoIQEUmiBJGg246KiDQTa4Iws+PMbIWZrTKzmWnmH2FmL5nZdjObmjJvupm9Fj2mxxknoNuOioikiC1BmFk+cBNwPDASON3MRqY0Ww2cDfxvyrJ9gR8AE4EJwA/MrE9csQKqIEREUsRZQUwAVrn7G+5eD9wHnJzcwN2r3H0R0JCy7LHAE+7+vrtvAJ4Ajosx1pAgVEGIiDSKM0EMBNYkva+OprXbsmY2w8wWmNmCmpqaNgcKhC4mVRAiIo3iTBCWZpq357Lufqu7V7p75YABA1oV3E5UQYiINBNngqgGBie9HwSs7YBl20YVhIhIM3EmiPnAUDMbYmZFwDRgTobLPgZ8xsz6RIPTn4mmxUcVhIhIM7ElCHffDlxA+GFfBsx296VmdrWZnQRgZuPNrBo4BfiVmS2Nln0f+CEhycwHro6mxUcVhIhIMwVxrtzd5wJzU6ZdkfR6PqH7KN2ydwB3xBlfMzrMVUSkGZ1JnaAT5UREmlGCSFAFISLSjBJEgioIEZFmlCASEhWEZ3qqhohI16YEkZC4J0R9fXbjEBHpJJQgEnTbURGRZpQgEhIJQuMQIiKAEkQT3XZURKQZJYgEVRAiIs0oQSSoghARaUYJIkEVhIhIM0oQCaogRESaUYJIUAUhItKMEkSCKggRkWaUIBJ0opyISDNKEAmJCkJdTCIigBJEE1UQIiLNKEEkqIIQEWlGCSJBFYSISDNKEAl5eVBUpApCRCSiBJGspEQVhIhIRAkiWWmpKggRkYgSRDJVECIijZQgkqmCEBFppASRTBWEiEgjJYhkqiBERBrFmiDM7DgzW2Fmq8xsZpr5xWZ2fzT/RTMri6YXmtmdZrbYzJaZ2ffijLORKggRkUaxJQgzywduAo4HRgKnm9nIlGbnABvc/WDgeuAn0fRTgGJ3Hw0cBpybSB6xUgUhItIozgpiArDK3d9w93rgPuDklDYnA3dGrx8EppiZAQ50M7MCoBSoBz6IMdZAFYSISKM4E8RAYE3S++poWto27r4d2AT0IySLOmAdsBq4zt3fT92Amc0wswVmtqCmpubjR1xaqgQhIhKJM0FYmmmeYZsJwA5gf2AI8C0zO2inhu63unulu1cOGDDg48YbKgh1MYmIAPEmiGpgcNL7QcDaXbWJupN6Ae8DXwL+7O7b3P1fwN+AyhhjDVRBiIg0ijNBzAeGmtkQMysCpgFzUtrMAaZHr6cCT7u7E7qVjragGzAJWB5jrIEGqUVEGmWUIMzsQDM7JnpdamY9WlomGlO4AHgMWAbMdvelZna1mZ0UNbsd6Gdmq4BLgMShsDcB3YElhETzG3df1IrP1TaJQWpP7QkTEck9BS01MLOvAjOAvsAnCF1FvwSmtLSsu88F5qZMuyLp9RbCIa2py9Wmmx67xD0h6uuhuLjDNy8i0plkUkF8HZhMdJipu78G7B1nUFmTuKucxiFERDJKEFuj8xiAxsHkrtkHk6ggNA4hIpJRgvirmV0GlJrZvwMPAH+MN6wsUQUhItIokwQxE6gBFgPnAnPd/fJYo8oWVRAiIo1aHKQGvuHu/wPclphgZhdG07oWVRAiIo0yqSCmp5l2djvH0TmoghARabTLCsLMTiec0TzEzJJPcOsBrI87sKxQBSEi0mh3XUx/J1wsrz/w06TpHwLxn7SWDaogREQa7TJBuPtbwFvAJzsunCxLJAhVECIiLY9BmNkkM5tvZrVmVm9mO8ws/nszZIO6mEREGmUySP0L4HTgNcLNe74C3BhnUFmjLiYRkUaZHOaKu68ys3x33wH8xsz+HnNc2aEKQkSkUSYJYnN0ue5XzOxawsB1t3jDyhJVECIijTLpYjorancB4Tagg4EvxhlU1qiCEBFptNsKwszygR+5+5nAFuCqDokqW/LyoKhIFYSICC1UENGYw4Coiyk3JG4aJCKS4zIZg6gC/hadTV2XmOjuP4srqKzSbUdFRIDMEsTa6JFHuMxG16YKQkQEyCBBuHvXHndIpQpCRATI7Cim3KIKQkQEUILYmSoIERGghQRhZvlmdnFHBdMplJaqghARIbPDXE/uoFg6h5ISVRAiImR2FNPfzOwXwP00P8z1pdiiyqbSUqira7mdiEgXl0mC+FT0fHXSNAeObv9wOoFBg+BPf4KGhnBmtYhIjsrkMNejOiKQTmPECNi8Gd5+GwYPznY0IiJZk8kNg3qZ2c/MbEH0+KmZ9cpk5WZ2nJmtMLNVZjYzzfxiM7s/mv+imZUlzSs3s+fNbKmZLTazktZ8sDYbMSI8r1jRIZsTEemsMulDuYNwH+pTo8cHwG9aWii60N9NwPHASOB0MxuZ0uwcYIO7HwxcD/wkWrYAuAc4z90PBY4EtmUQ68c3fHh4Xr68QzYnItJZZZIgPuHuP3D3N6LHVcBBGSw3AVgVLVMP3MfOR0SdDNwZvX4QmGJmBnwGWOTu/wRw9/XREVXx23df6NlTCUJEcl4mCeIjMzs88cbMJgOZnCgwEFiT9L46mpa2jbtvBzYB/YBhgJvZY2b2kpl9N90GzGxGouurpqYmg5AyYBa6mdTFJCI5LpOjmM4D7koad9gATM9gOUszzTNsUwAcDowHNgNPmdlCd3+qWUP3W4FbASorK1PX3XbDh8O8ee22OhGRPVFLZ1LnAcPdfQxQDpS7+1h3X5TBuqsJd59LGES4KmzaNtG4Qy/g/Wj6X939PXffDMwFxmWwzfYxYgRUV0NtbYdtUkSks2npTOoGwq1GcfcP3P2DVqx7PjDUzIZENxyaBsxJaTOHpmpkKvC0uzvwGFBuZntFiePfgFdbse2PJ3Ek08qVHbZJEZHOJpMxiCfM7NtmNtjM+iYeLS0UjSlcQPixXwbMdvelZna1mZ0UNbsd6Gdmq4BLgJnRshuAnxGSzCvAS+7+aKs/XVvpSCYRkYzGIL4cPX89aZqTwZFM7j6X0D2UPO2KpNdbgFN2sew9hENdO97BB4ezqJUgRCSH7TZBRGMQZ7r73zoons6huBgOOkgJQkRyWiZjENd1UCydy/DhOtRVRHJaJmMQj5vZF6MT2HLHiBFhkHpHx5yfJyLS2WQyBnEJ0A3YYWYfEc5dcHfvGWtk2TZiRLgvxOrVMGRItqMREelwLVYQ7t7D3fPcvdDde0bvu3ZygKYjmdTNJCI5KpOruZqZnWlms6L3g81sQvyhZVniXAgNVItIjspkDOJm4JPAl6L3tYSrtHZt/ftD375KECKSszIZg5jo7uPM7GUIJ7FFZ0Z3bWY6kklEclomFcS26N4ODmBmA4CGWKPqLEaMUAUhIjkrkwRxA/AQsLeZ/Qh4DvhxrFF1FiNGwDvvwKZN2Y5ERKTDZXJP6t+Z2UJgCuEQ18+7+7LYI+sMkq/JNHFidmMREelgmYxB4O7Lgdzra0kkiJUrlSBEJOdk0sWUuw46KFy0T5f9FpEcpASxO0VF4SxqJQgRyUFKEC0ZNkwJQkRykhJESxIJwtvvltciInsCJYiWDBsGmzfD2tTbaYuIdG1KEC0ZNiw8q5tJRHKMEkRLlCBEJEcpQbRk0CAoKVGCEJGcowTRkrw8GDpUCUJEco4SRCZ0qKuI5CAliEwMGwZvvAHbtmU7EhGRDqMEkYlhw2D7dqiqynYkIiIdRgkiEzqSSURykBJEJpQgRCQHxZogzOw4M1thZqvMbGaa+cVmdn80/0UzK0uZf4CZ1ZrZt+OMs0X9+kGfPkoQIpJTYksQ0W1KbwKOB0YCp5vZyJRm5wAb3P1g4HrgJynzrwf+FFeMGTPTkUwiknPirCAmAKvc/Q13rwfuA05OaXMycGf0+kFgipkZgJl9HngDWBpjjJlTghCRHBNnghgIrEl6Xx1NS9vG3bcDm4B+ZtYNuBS4Ksb4WmfYMKiuhrq6bEciItIh4kwQlmZa6jWzd9XmKuB6d6/d7QbMZpjZAjNbUFNT08YwM5QYqF61Kt7tiIh0EnEmiGpgcNL7QUDqNbMb25hZAdALeB+YCFxrZlXARcBlZnZB6gbc/VZ3r3T3ygEDBrT/J0imI5lEJMcUxLju+cBQMxsCvA1MA76U0mYOMB14HpgKPO3uDnw60cDMrgRq3f0XMcbasoMPDs9KECKSI2JLEO6+Pfqr/zEgH7jD3Zea2dXAAnefA9wO3G1mqwiVw7S44vnYuneHgQNhxYpsRyIi0iHirCBw97nA3JRpVyS93gKc0sI6rowluLY49FBYsiTbUYiIdAidSd0aY8bA0qW6aJ+I5AQliNYYMwbq62H58mxHIiISOyWI1qioCM///Gd24xAR6QBKEK0xfDgUF8Mrr2Q7EhGR2ClBtEZBAYwapQpCRHKCEkRrjRkTEoSnnhQuItK1KEG01pgxUFMD69ZlOxIRkVgpQbSWBqpFJEcoQbRWeXl4VoIQkS5OCaK1eveGsjIdySQiXZ4SRFskBqpFRLowJYi2GDMmXNV18+ZsRyIiEhsliLaoqICGBl24T0S6NCWIthgzJjyrm0lEujAliLYoK4MePTRQLSJdmhJEW+TlaaBaRLo8JYi2SiSIhoZsRyIiEgsliLaaPBlqa+HHP852JCIisVCCaKtp0+Css2DWLLjjjmxHIyLS7mK9J3WXZga33w7/+hfMmAH77AOf+1y2oxIRaTeqID6OwkJ48MFwXsQpp8D8+dmOSESk3ShBfFzdu8Ojj8Lee8Ppp4dxCRGRLkAJoj3ssw/ceSe88QZcemm2oxERaRdKEO3l3/4NLroIbr4Znngi29GIiHxsShDt6Uc/gkMOgf/8T9i4MdvRiIh8LEoQ7am0FO66C955B775zWxHIyLysShBtLfKSrj8crj7bnjooZ3nr1ypaziJyB4h1gRhZseZ2QozW2VmM9PMLzaz+6P5L5pZWTT9381soZktjp6PjjPOdnf55TB2LJx7bjhPImHpUpg4McybMQPefz97MYqItCC2BGFm+cBNwPHASOB0MxuZ0uwcYIO7HwxcD/wkmv4ecKK7jwamA3fHFWcsiopCV9OmTXDeeeAO1dVw3HFQUgIXXBDOvh4+PDxv357tiEVEdhJnBTEBWOXub7h7PXAfcHJKm5OBO6PXDwJTzMzc/WV3XxtNXwqUmFlxjLG2v1Gj4L/+K3Qz/eIXcPzxIWH86U9w443w0kswbBiccw4cfDD89Kca2BaRTiXOBDEQWJP0vjqalraNu28HNgH9Utp8EXjZ3bembsDMZpjZAjNbUFNT026Bt5tLLgkX9fvmN2HFCnj44XDWNUB5OTz7bEggZWXw7W/DoEHhMFkRkU4gzgRhaaZ5a9qY2aGEbqdz023A3W9190p3rxwwYECbA41Nfn44ga6iAu65B45OGUrJy4PPfx7+8hd4+WU4/HD4+tfhvvuyEq6ISLI4L9ZXDQxOej8IWLuLNtVmVgD0At4HMLNBwEPAf7j76zHGGa9PfCL8+LekoiJUGMceC9Onw377hZPvRESyJM4KYj4w1MyGmFkRMA2Yk9JmDmEQGmAq8LS7u5n1Bh4Fvufuf4sxxs6lpCR0OR10UKgsXn012xGJSA6LLUFEYwoXAI8By4DZ7r7UzK42s5OiZrcD/cxsFXAJkDgU9gLgYGCWmb0SPfaOK9ZOpW/fMJBdXAxHHQXf/344PDbBHdavhw8+yF6MIpITzD11WGDPVFlZ6QsWLMh2GO1n0aIwcP3UU+G2poceCgUF8OabITl06xbmf+tb0KNHtqMVkT2UmS1098q085QgOrl334XZs8P4RGkpDBkSHi+8AA88EC4zPmsWHHFEOAqqT59wMyMRkQwoQXRVL74I3/lOOFw2obQU9t8/JI699w4J46OPwn0qNm8O4xtHHAGf/nQ4vFbJRCSnKUF0Ze7hpLs33ghna1dXh4sFvvtuuMzHxo0haXTrFgbBly+HDRvCssOHw3//N5xwQlOi+OgjuO22sN5vfCMciisiXdbuEoTuSb2nM4PDDguPTDQ0hEHvZ56Bm26Ck04KZ3lfcw08/TT85CchwQA8+WS46GDv3k3LL1sWEs2QIe3/WUSkU1GCyDV5eTB6dHjMmBEu+3HllTBmTJh/1FHhRL2lS+HCC2H8+HBdqcWLQ2WRqNLGjYMvfjGct1FaGqaZhfM+ioqy8tFEpH2pi0lCxXDHHeGyIMkn5/3tb3DKKbBuXXg/ahR85Svh4oIPPhgGylP17RuWOf30MM6hLiqRTk1jENJ269aFy4QccQRMmNB8UHvNmjBQ3tAQ3m/dCn/+czjiavNm2Gsv6NcvDJT36hXm19WFAfMDDoBp00Iy2dVlUurrwzYKCkJVUlIS1iUi7UYJQjpWXR3MmQP/+EcYJN+wIVzJtrg4DJZ36xYG1pcuDderOvLIMGA+cGA4AuvNN8MYyYsvhkHzZGPHwle/Cl/6Ukg67iGJrV4dDvMdOLApiTU0hOk1NeFWsN27Z/4Z1q8PA/2FhSE5desG/fu32y4S6SyUIKRzWrwY7r03nDm+enXTDZTy8sK1qT796TA24h6qiQ0b4P774Z//DOMeI0bAa6+FiiShe/dwGfVt28K8LVvCdLOQhMaODVVNQn5+U9IqLAwxvfBCWDbVoEHhhk8TJ4ZutsWLw2PDhjB2c9xxYUympKTpKLLa2pBgCgtDJfTRRyGB1tWF7rixY8OzSJYoQcieYfNmePtt2Gcf6NkzfRt3WLgQfv3rUGkMHx4eBxwQDvFdvjxcWr2gICSQ4cNDQliyJFQtL7/cPKFs3x5+rHfsCO/33RcmTQqPAw8M8+vrQyW0YEFIHm++GdoecEAY7O/ePZzx/t57bfvcZWUhIR54YEhCiRMeu3UL695rr6ZKpqAgJKREAsrPDwnx4INDYhJpJSUIkd1JVChbtoTE1NLJgzU14ce6V6+maQ0NIXE99VSogPbZJ5yo2KNHqGbq60OyKSlpqljeeSckrZdeCpXImjXNk1dr5OXB4MHhkOTE+vPymrbtHuLt0ye02bGjaTwoLy90zSWS0777htj32Se0z89vvq2GhlDtuYftlJa27YTLd99t2pdFRWF9773XNL2+vqlt9+4hiQ4fHpKktBslCJE9xQcfhEpo48amH/DNm8MPfeLRp0/TmfL19bByZaiaXn89LJ/ownJvqjygaTxo48bwI5uoUHbsCNv88MOd4zELSbNPn1DJrF8ffsQTFVeiTa9eTUlm4MCQVOrrQ7xFRU3Jp1evUIU9+WTzi1BmqrQ0dDsOHx7OxTnooLC9Pn2aHj16ND96zr3psycrKGjaP++9F+J64YXQhdm7d1PMffs2tSsuDvshkYSTH8mJsqEh7M8NG8Ij+d8l8aitbf5cVxf2VyKRJz5LYntFRc3/2MjPb0qu++4bujfbQAlCRFqWSE6J7qt33w0JIfHjWlcXuusSySkvr+kHbuPG0D1YXR2ek5PTRx+FAwkSR7uVloabY02ZEn7gk6ucAQPCugcMaN5ltn596B5MdBO+/nrTdlLl5TX9yG7dGj7Ltm2Z7YOePUOlUlfXtC/ikkjSiUf37mHaBx80JZZM4540CZ5/vk1hKEGISHZt3x661NavD2NDxe1wi/mtW6GqKiSfRBJLPBLvi4qauvv69GmqLNxDFZSocrp3DwcfjBjRvPrYujVUAokktnVrqOhqa5v/5V9X13RARELPnqESSRzmnZoMWjqhNNH1mVh/fX1TxVBQ0Dz+/PwwJtYGutSGiGRXQUFTl017KS5uOkghLsXF7ZPM2sKsaftZOtJNp3M8B4wAAAgxSURBVLmKiEhaShAiIpKWEoSIiKSlBCEiImkpQYiISFpKECIikpYShIiIpKUEISIiaXWZM6nNrAZ4qxWL9AfaePnNLkv7pDntj+a0P3bWFfbJge6e9q5dXSZBtJaZLdjV6eW5SvukOe2P5rQ/dtbV94m6mEREJC0lCBERSSuXE8St2Q6gE9I+aU77ozntj5116X2Ss2MQIiKye7lcQYiIyG4oQYiISFo5mSDM7DgzW2Fmq8xsZrbj6QhmNtjM5pnZMjNbamYXRtP7mtkTZvZa9Nwnmm5mdkO0jxaZ2bjsfoJ4mFm+mb1sZo9E74eY2YvR/rjfzIqi6cXR+1XR/LJsxh0XM+ttZg+a2fLou/LJXP6OmNnF0f+XJWZ2r5mV5NJ3JOcShJnlAzcBxwMjgdPNbGR2o+oQ24FvufshwCTg69Hnngk85e5Dgaei9xD2z9DoMQO4peND7hAXAsuS3v8EuD7aHxuAc6Lp5wAb3P1g4PqoXVf0P8Cf3X0EMIawb3LyO2JmA4FvApXuPgrIB6aRS98Rd8+pB/BJ4LGk998DvpftuLKwH/4A/DuwAtgvmrYfsCJ6/Svg9KT2je26ygMYRPjBOxp4BDDCWbEFqd8V4DHgk9HrgqidZfsztPP+6Am8mfq5cvU7AgwE1gB9o3/zR4Bjc+k7knMVBE3/6AnV0bScEZW+Y4EXgX3cfR1A9Lx31CwX9tPPge8CDdH7fsBGd98evU/+zI37I5q/KWrflRwE1AC/ibrdfm1m3cjR74i7vw1cB6wG1hH+zReSQ9+RXEwQlmZazhzra2bdgd8DF7n7B7trmmZal9lPZnYC8C93X5g8OU1Tz2BeV1EAjANucfexQB1N3UnpdOl9Eo21nAwMAfYHuhG61VJ12e9ILiaIamBw0vtBwNosxdKhzKyQkBx+5+7/F01+18z2i+bvB/wrmt7V99Nk4CQzqwLuI3Qz/RzobWYFUZvkz9y4P6L5vYD3OzLgDlANVLv7i9H7BwkJI1e/I8cAb7p7jbtvA/4P+BQ59B3JxQQxHxgaHYlQRBh0mpPlmGJnZgbcDixz958lzZoDTI9eTyeMTSSm/0d0pMokYFOim6ErcPfvufsgdy8jfAeedvczgHnA1KhZ6v5I7KepUfs9+q/DVO7+DrDGzIZHk6YAr5Kj3xFC19IkM9sr+v+T2B+58x3J9iBINh7AZ4GVwOvA5dmOp4M+8+GEcncR8Er0+Cyhj/Qp4LXouW/U3ghHe70OLCYcyZH1zxHTvjkSeCR6fRDwD2AV8ABQHE0vid6viuYflO24Y9oXFcCC6HvyMNAnl78jwFXAcmAJcDdQnEvfEV1qQ0RE0srFLiYREcmAEoSIiKSlBCEiImkpQYiISFpKECIikpYShHQ5ZvYXM4v9RvJm9s3oiqe/S5leYWafbcP69jezBzNoN9fMerd2/Rms97dmNrWFNmeb2f7tvW3pnJQgRJIknSGbifOBz3o4wS5ZBeEck1at393Xuvtuf6Cjdp91942tiLM9nU247ITkACUIyQozK4v++r4tut7+42ZWGs1rrADMrH90OYzEX68Pm9kfzexNM7vAzC6JLiz3gpn1TdrEmWb29+g6/hOi5buZ2R1mNj9a5uSk9T5gZn8EHk8T6yXRepaY2UXRtF8STpiaY2YXJ7UtAq4GTjOzV8zsNDO70sxuNbPHgbuiz/6smb0UPT6VtE+WJMX0f2b25+i+A9cmbaMq2i+724fjLdyj4Xkz++/EelM+l5nZL8zsVTN7lKaL8GFmV0T7aUkUu0XVRSXwu+izlaZr18qvgnRm2T5TT4/cfABlhHtUVETvZwNnRq//QnRWLtAfqIpen004S7UHMIBwtczzonnXEy5AmFj+tuj1EcCS6PWPk7bRm3A2fbdovdVEZwinxHkY4SzhbkB3YCkwNppXBfRPs8zZwC+S3l9JuApoafR+L6Akej0UWJC0T5YkreMNwvV8SoC3gMHJ221hHy4BPhW9viax3pQ4vwA8QbjPwf7ARmBqNK9vUru7gRNT/212106PrvFQBSHZ9Ka7vxK9Xkj4wWvJPHf/0N1rCAnij9H0xSnL3wvg7s8APaM++88AM83sFcIPXQlwQNT+CXdPd2G1w4GH3L3O3WsJF2z7dGYfr5k57v5R9LoQuM3MFhMuzbCrG1Y95e6b3H0L4RpAB6Zps9M+jD5rD3f/ezT9f3ex/iOAe919h7uvBZ5OmneUhbuiLSZcyPDQXawj03ayB2pNf6tIe9ua9HoHUBq93k5T92fJbpZpSHrfQPPvc+o1ZJxw7aAvuvuK5BlmNpFwaet02qvLJHn9FwPvEu7Ylgds2cUyqfsn3f/XdPuwNTHvdK0dMysBbiZUCmvM7Ep2/nfIuJ3suVRBSGdURejagaarZrbWaQBmdjjhKqObCHf8+kain9zMxmawnmeAz1u4omc34P8Bz7awzIeEbrBd6QWsc/cG4CxCF0+7cfcNwIcWrrAK4Wq16TwDTLNwX+79gKOi6Ykf+fcs3D8k+d8g+bPtrp10AUoQ0hldB3zNzP5O6Gtviw3R8r+k6Z7BPyR07yyKBm1/2NJK3P0l4LeEq3O+CPza3V9uYbF5wMjEIHWa+TcD083sBWAYu65ePo5zgFvN7HlCRbEpTZuHCFdoXUy4n/RfATwcIXVbNP1hwiXyE34L/DLqptu6m3bSBehqriJdkJl1j8ZMMLOZhHtFX5jlsGQPozEIka7pc2b2PcL/8bcIR0WJtIoqCBERSUtjECIikpYShIiIpKUEISIiaSlBiIhIWkoQIiKS1v8HxeAdpo38RhAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the average result\n",
    "plt.plot(np.arange(1,91)*10, passive_result.mean(), c='r',label='passive learning')\n",
    "plt.xlabel('number of training data')\n",
    "plt.ylabel('error rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  ii. Train a SVM with a pool of 10 randomly selected data points from the training set using linear kernel and L1 penalty.\n",
    "    Select the parameters of the SVM with 10-fold cross validation. Choose the 10 closest data points in the training set to the hyperplane of the SVM and add them to the pool. Do not replace the samples back into the training set. Train a new SVM using the pool. Repeat this process until all training data is used. You will have 90 SVMs that were trained using 10, 20, 30,..., 900 data points and their 90 test errors. You have implemented active learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_learning():\n",
    "    train_x2, train_y2, test_x2, test_y2 = split_dataset(dataset_x2, dataset_y2, \n",
    "                                                     test_size = (472/(472+900)), random = None)\n",
    "    #first select, get 10 sample as the first training data\n",
    "    select_x, select_y, unselect_x, unselect_y = select_10_sample(train_x2, train_y2)\n",
    "    active_errors = list()\n",
    "    \n",
    "        \n",
    "    for i in range(90):\n",
    "        CV=5\n",
    "        parameters = {'C':np.power(10, np.arange(-3, 7,0.5))}\n",
    "        svc = LinearSVC(penalty='l1', dual=False)\n",
    "        clf = GridSearchCV(svc, parameters, cv=CV)\n",
    "        clf.fit(select_x, select_y)\n",
    "        best_C = clf.best_params_['C']\n",
    "\n",
    "        # Refit the model with the best params\n",
    "        svc = LinearSVC(penalty='l1', dual=False, C=best_C)\n",
    "        svc.fit(select_x, select_y)\n",
    "        \n",
    "        active_errors.append(1-svc.score(test_x2, test_y2))\n",
    "        \n",
    "        if(i!=89):\n",
    "            distances = svc.decision_function(unselect_x)\n",
    "            closest = np.argsort(abs(distances))[:10]\n",
    "            new_select_x = unselect_x.iloc[closest,:]\n",
    "            new_select_y = unselect_y.iloc[closest]\n",
    "            unselect_x = unselect_x.drop(train_x2.index[closest])\n",
    "            unselect_y = unselect_y.drop(train_y2.index[closest])\n",
    "            \n",
    "            #reset index\n",
    "            unselect_x = unselect_x.reset_index(drop=True)\n",
    "            unselect_y = unselect_y.reset_index(drop=True)\n",
    "            select_x = pd.concat([select_x, new_select_x],axis=0, ignore_index=True)\n",
    "            select_y = pd.concat([select_y, new_select_y],axis=0, ignore_index=True)\n",
    "            \n",
    "    return active_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete 0 times\n",
      "complete 1 times\n",
      "complete 2 times\n",
      "complete 3 times\n",
      "complete 4 times\n",
      "complete 5 times\n",
      "complete 6 times\n",
      "complete 7 times\n",
      "complete 8 times\n",
      "complete 9 times\n",
      "complete 10 times\n",
      "complete 11 times\n",
      "complete 12 times\n",
      "complete 13 times\n",
      "complete 14 times\n",
      "complete 15 times\n",
      "complete 16 times\n",
      "complete 17 times\n",
      "complete 18 times\n",
      "complete 19 times\n",
      "complete 20 times\n",
      "complete 21 times\n",
      "complete 22 times\n",
      "complete 23 times\n",
      "complete 24 times\n",
      "complete 25 times\n",
      "complete 26 times\n",
      "complete 27 times\n",
      "complete 28 times\n",
      "complete 29 times\n",
      "complete 30 times\n",
      "complete 31 times\n",
      "complete 32 times\n",
      "complete 33 times\n",
      "complete 34 times\n",
      "complete 35 times\n",
      "complete 36 times\n",
      "complete 37 times\n",
      "complete 38 times\n",
      "complete 39 times\n",
      "complete 40 times\n",
      "complete 41 times\n",
      "complete 42 times\n",
      "complete 43 times\n",
      "complete 44 times\n",
      "complete 45 times\n",
      "complete 46 times\n",
      "complete 47 times\n",
      "complete 48 times\n",
      "complete 49 times\n"
     ]
    }
   ],
   "source": [
    "active_result = list()\n",
    "for i in range(50):\n",
    "    print(\"complete\", i, \"times\")\n",
    "    error = active_learning()\n",
    "    active_result.append(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.091102</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.120763</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.175847</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.220339</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.101695</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.110169</td>\n",
       "      <td>0.116525</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.184322</td>\n",
       "      <td>0.091102</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.139831</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.074153</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.141949</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.074153</td>\n",
       "      <td>0.061441</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.154661</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.141949</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.131356</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.061441</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.180085</td>\n",
       "      <td>0.091102</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.137712</td>\n",
       "      <td>0.052966</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.074153</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.171610</td>\n",
       "      <td>0.095339</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.230932</td>\n",
       "      <td>0.122881</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.144068</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.065678</td>\n",
       "      <td>0.065678</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.158898</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.148305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.245763</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.004237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.076271</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.156780</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.082627</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.021186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.292373</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.078390</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.334746</td>\n",
       "      <td>0.108051</td>\n",
       "      <td>0.055085</td>\n",
       "      <td>0.057203</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.120763</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.057203</td>\n",
       "      <td>0.072034</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.088983</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.120763</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.385593</td>\n",
       "      <td>0.074153</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.444915</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.095339</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.180085</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.057203</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.349576</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.006356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.072034</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.444915</td>\n",
       "      <td>0.105932</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.052966</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.167373</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.114407</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.186441</td>\n",
       "      <td>0.095339</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   0.135593  0.023305  0.025424  0.021186  0.016949  0.006356  0.006356   \n",
       "1   0.091102  0.038136  0.014831  0.008475  0.014831  0.006356  0.008475   \n",
       "2   0.120763  0.010593  0.031780  0.012712  0.008475  0.012712  0.006356   \n",
       "3   0.175847  0.014831  0.019068  0.010593  0.010593  0.008475  0.008475   \n",
       "4   0.220339  0.029661  0.016949  0.101695  0.044492  0.025424  0.023305   \n",
       "5   0.110169  0.116525  0.023305  0.025424  0.025424  0.016949  0.012712   \n",
       "6   0.184322  0.091102  0.023305  0.016949  0.029661  0.019068  0.008475   \n",
       "7   0.029661  0.139831  0.019068  0.010593  0.074153  0.029661  0.014831   \n",
       "8   0.141949  0.025424  0.074153  0.061441  0.029661  0.023305  0.021186   \n",
       "9   0.154661  0.021186  0.040254  0.019068  0.008475  0.008475  0.006356   \n",
       "10  0.135593  0.059322  0.141949  0.050847  0.027542  0.021186  0.021186   \n",
       "11  0.050847  0.036017  0.019068  0.010593  0.008475  0.006356  0.008475   \n",
       "12  0.131356  0.025424  0.036017  0.023305  0.016949  0.014831  0.016949   \n",
       "13  0.061441  0.016949  0.006356  0.002119  0.002119  0.010593  0.012712   \n",
       "14  0.180085  0.091102  0.010593  0.016949  0.010593  0.008475  0.008475   \n",
       "15  0.010593  0.084746  0.038136  0.023305  0.019068  0.006356  0.006356   \n",
       "16  0.137712  0.052966  0.021186  0.074153  0.008475  0.023305  0.010593   \n",
       "17  0.171610  0.095339  0.029661  0.031780  0.021186  0.016949  0.012712   \n",
       "18  0.050847  0.031780  0.016949  0.023305  0.014831  0.010593  0.016949   \n",
       "19  0.230932  0.122881  0.050847  0.027542  0.014831  0.012712  0.014831   \n",
       "20  0.144068  0.040254  0.008475  0.010593  0.059322  0.010593  0.008475   \n",
       "21  0.046610  0.027542  0.027542  0.016949  0.014831  0.012712  0.012712   \n",
       "22  0.135593  0.019068  0.010593  0.016949  0.008475  0.008475  0.012712   \n",
       "23  0.065678  0.065678  0.040254  0.036017  0.027542  0.019068  0.014831   \n",
       "24  0.158898  0.044492  0.069915  0.012712  0.016949  0.012712  0.008475   \n",
       "25  0.148305  0.023305  0.021186  0.016949  0.010593  0.012712  0.021186   \n",
       "26  0.025424  0.245763  0.029661  0.014831  0.012712  0.006356  0.004237   \n",
       "27  0.040254  0.084746  0.044492  0.014831  0.012712  0.016949  0.014831   \n",
       "28  0.076271  0.023305  0.008475  0.010593  0.006356  0.006356  0.006356   \n",
       "29  0.156780  0.021186  0.044492  0.019068  0.012712  0.010593  0.027542   \n",
       "30  0.082627  0.014831  0.016949  0.025424  0.021186  0.016949  0.016949   \n",
       "31  0.292373  0.067797  0.027542  0.023305  0.014831  0.012712  0.012712   \n",
       "32  0.031780  0.078390  0.029661  0.021186  0.010593  0.014831  0.016949   \n",
       "33  0.334746  0.108051  0.055085  0.057203  0.046610  0.033898  0.023305   \n",
       "34  0.120763  0.038136  0.010593  0.012712  0.010593  0.010593  0.008475   \n",
       "35  0.044492  0.040254  0.016949  0.014831  0.012712  0.012712  0.014831   \n",
       "36  0.050847  0.016949  0.040254  0.033898  0.027542  0.014831  0.016949   \n",
       "37  0.057203  0.072034  0.038136  0.016949  0.033898  0.025424  0.016949   \n",
       "38  0.088983  0.067797  0.027542  0.014831  0.021186  0.014831  0.010593   \n",
       "39  0.120763  0.033898  0.008475  0.029661  0.010593  0.010593  0.008475   \n",
       "40  0.385593  0.074153  0.010593  0.012712  0.006356  0.010593  0.012712   \n",
       "41  0.444915  0.016949  0.036017  0.021186  0.008475  0.010593  0.010593   \n",
       "42  0.095339  0.031780  0.010593  0.010593  0.008475  0.012712  0.012712   \n",
       "43  0.042373  0.029661  0.025424  0.023305  0.012712  0.010593  0.012712   \n",
       "44  0.180085  0.023305  0.057203  0.021186  0.008475  0.008475  0.023305   \n",
       "45  0.349576  0.084746  0.019068  0.040254  0.031780  0.012712  0.006356   \n",
       "46  0.072034  0.014831  0.025424  0.019068  0.016949  0.010593  0.012712   \n",
       "47  0.444915  0.105932  0.012712  0.052966  0.014831  0.014831  0.006356   \n",
       "48  0.167373  0.031780  0.040254  0.027542  0.016949  0.012712  0.014831   \n",
       "49  0.114407  0.016949  0.186441  0.095339  0.038136  0.038136  0.016949   \n",
       "\n",
       "          7         8         9   ...        80        81        82        83  \\\n",
       "0   0.010593  0.010593  0.010593  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "1   0.006356  0.006356  0.006356  ...  0.006356  0.006356  0.006356  0.006356   \n",
       "2   0.014831  0.012712  0.012712  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "3   0.008475  0.008475  0.008475  ...  0.010593  0.008475  0.008475  0.008475   \n",
       "4   0.019068  0.012712  0.014831  ...  0.023305  0.021186  0.014831  0.014831   \n",
       "5   0.008475  0.010593  0.008475  ...  0.008475  0.010593  0.012712  0.010593   \n",
       "6   0.008475  0.004237  0.010593  ...  0.008475  0.004237  0.004237  0.004237   \n",
       "7   0.012712  0.012712  0.014831  ...  0.016949  0.016949  0.016949  0.012712   \n",
       "8   0.016949  0.010593  0.010593  ...  0.008475  0.010593  0.008475  0.010593   \n",
       "9   0.006356  0.006356  0.006356  ...  0.006356  0.006356  0.006356  0.006356   \n",
       "10  0.016949  0.016949  0.019068  ...  0.016949  0.019068  0.019068  0.019068   \n",
       "11  0.006356  0.006356  0.006356  ...  0.006356  0.006356  0.006356  0.006356   \n",
       "12  0.016949  0.016949  0.016949  ...  0.010593  0.010593  0.012712  0.012712   \n",
       "13  0.012712  0.012712  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "14  0.008475  0.008475  0.008475  ...  0.008475  0.008475  0.008475  0.006356   \n",
       "15  0.004237  0.004237  0.004237  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "16  0.008475  0.008475  0.008475  ...  0.008475  0.008475  0.008475  0.008475   \n",
       "17  0.019068  0.010593  0.010593  ...  0.014831  0.014831  0.014831  0.010593   \n",
       "18  0.027542  0.016949  0.016949  ...  0.016949  0.021186  0.021186  0.021186   \n",
       "19  0.014831  0.014831  0.014831  ...  0.014831  0.016949  0.016949  0.016949   \n",
       "20  0.008475  0.008475  0.008475  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "21  0.012712  0.012712  0.014831  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "22  0.012712  0.012712  0.012712  ...  0.012712  0.012712  0.016949  0.016949   \n",
       "23  0.012712  0.010593  0.010593  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "24  0.012712  0.012712  0.012712  ...  0.010593  0.012712  0.012712  0.012712   \n",
       "25  0.021186  0.021186  0.014831  ...  0.021186  0.021186  0.025424  0.025424   \n",
       "26  0.008475  0.004237  0.004237  ...  0.006356  0.004237  0.006356  0.004237   \n",
       "27  0.012712  0.012712  0.012712  ...  0.021186  0.021186  0.021186  0.021186   \n",
       "28  0.008475  0.008475  0.006356  ...  0.008475  0.008475  0.008475  0.006356   \n",
       "29  0.012712  0.008475  0.008475  ...  0.008475  0.008475  0.008475  0.008475   \n",
       "30  0.016949  0.016949  0.016949  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "31  0.010593  0.010593  0.014831  ...  0.016949  0.012712  0.012712  0.016949   \n",
       "32  0.016949  0.016949  0.016949  ...  0.019068  0.016949  0.016949  0.016949   \n",
       "33  0.019068  0.019068  0.012712  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "34  0.008475  0.008475  0.008475  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "35  0.014831  0.014831  0.014831  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "36  0.016949  0.016949  0.016949  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "37  0.019068  0.016949  0.014831  ...  0.021186  0.021186  0.021186  0.021186   \n",
       "38  0.012712  0.014831  0.014831  ...  0.012712  0.014831  0.014831  0.014831   \n",
       "39  0.014831  0.008475  0.008475  ...  0.008475  0.008475  0.008475  0.008475   \n",
       "40  0.010593  0.014831  0.010593  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "41  0.010593  0.010593  0.010593  ...  0.010593  0.010593  0.010593  0.014831   \n",
       "42  0.012712  0.010593  0.010593  ...  0.010593  0.012712  0.012712  0.012712   \n",
       "43  0.010593  0.012712  0.019068  ...  0.012712  0.012712  0.010593  0.012712   \n",
       "44  0.010593  0.008475  0.008475  ...  0.008475  0.008475  0.010593  0.010593   \n",
       "45  0.008475  0.006356  0.006356  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "46  0.006356  0.006356  0.006356  ...  0.006356  0.006356  0.006356  0.006356   \n",
       "47  0.008475  0.008475  0.008475  ...  0.008475  0.008475  0.008475  0.008475   \n",
       "48  0.010593  0.012712  0.012712  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "49  0.012712  0.012712  0.012712  ...  0.019068  0.019068  0.019068  0.019068   \n",
       "\n",
       "          84        85        86        87        88        89  \n",
       "0   0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "1   0.006356  0.006356  0.006356  0.006356  0.006356  0.006356  \n",
       "2   0.016949  0.016949  0.016949  0.016949  0.012712  0.012712  \n",
       "3   0.008475  0.008475  0.008475  0.010593  0.010593  0.010593  \n",
       "4   0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "5   0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "6   0.004237  0.004237  0.004237  0.004237  0.004237  0.008475  \n",
       "7   0.016949  0.016949  0.016949  0.012712  0.012712  0.012712  \n",
       "8   0.010593  0.010593  0.021186  0.010593  0.010593  0.010593  \n",
       "9   0.006356  0.006356  0.006356  0.006356  0.006356  0.006356  \n",
       "10  0.019068  0.019068  0.019068  0.019068  0.023305  0.016949  \n",
       "11  0.006356  0.006356  0.006356  0.006356  0.006356  0.006356  \n",
       "12  0.010593  0.010593  0.023305  0.023305  0.023305  0.023305  \n",
       "13  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "14  0.006356  0.006356  0.006356  0.006356  0.006356  0.006356  \n",
       "15  0.000000  0.004237  0.004237  0.004237  0.004237  0.000000  \n",
       "16  0.008475  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "17  0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "18  0.016949  0.016949  0.016949  0.016949  0.016949  0.016949  \n",
       "19  0.016949  0.016949  0.014831  0.014831  0.014831  0.014831  \n",
       "20  0.010593  0.008475  0.008475  0.010593  0.010593  0.010593  \n",
       "21  0.012712  0.012712  0.012712  0.014831  0.014831  0.014831  \n",
       "22  0.012712  0.016949  0.012712  0.012712  0.012712  0.012712  \n",
       "23  0.012712  0.012712  0.012712  0.012712  0.010593  0.010593  \n",
       "24  0.010593  0.010593  0.010593  0.016949  0.010593  0.010593  \n",
       "25  0.025424  0.025424  0.025424  0.025424  0.025424  0.012712  \n",
       "26  0.004237  0.006356  0.004237  0.006356  0.006356  0.004237  \n",
       "27  0.021186  0.021186  0.021186  0.021186  0.021186  0.021186  \n",
       "28  0.006356  0.006356  0.006356  0.006356  0.006356  0.006356  \n",
       "29  0.008475  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "30  0.016949  0.021186  0.025424  0.016949  0.016949  0.021186  \n",
       "31  0.016949  0.016949  0.012712  0.010593  0.010593  0.010593  \n",
       "32  0.016949  0.016949  0.016949  0.016949  0.016949  0.016949  \n",
       "33  0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "34  0.014831  0.014831  0.008475  0.008475  0.008475  0.008475  \n",
       "35  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "36  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "37  0.021186  0.021186  0.014831  0.014831  0.014831  0.014831  \n",
       "38  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "39  0.008475  0.010593  0.008475  0.014831  0.014831  0.014831  \n",
       "40  0.010593  0.010593  0.010593  0.010593  0.019068  0.019068  \n",
       "41  0.019068  0.019068  0.019068  0.019068  0.019068  0.019068  \n",
       "42  0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "43  0.012712  0.012712  0.012712  0.010593  0.010593  0.010593  \n",
       "44  0.010593  0.006356  0.008475  0.008475  0.008475  0.008475  \n",
       "45  0.014831  0.014831  0.014831  0.006356  0.014831  0.006356  \n",
       "46  0.006356  0.006356  0.006356  0.006356  0.006356  0.006356  \n",
       "47  0.008475  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "48  0.016949  0.016949  0.016949  0.016949  0.016949  0.016949  \n",
       "49  0.019068  0.019068  0.012712  0.012712  0.012712  0.012712  \n",
       "\n",
       "[50 rows x 90 columns]"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_result_df = pd.DataFrame(active_result)\n",
    "active_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the average result\n",
    "plt.plot(np.arange(1,91)*10, active_result_df.mean(), c='b',label='active learning')\n",
    "plt.xlabel('number of training data')\n",
    "plt.ylabel('error rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Average the 50 test errors for each of the incrementally trained 90 SVMs in 2(b)i and 2(b)ii. \n",
    "    By doing so, you are performing a Monte Carlo simulation. Plot average test error versus number of training instances for both active and passive learners on the same figure and report your conclusions. Here, you are actually obtaining a learning curve by Monte-Carlo simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(1,91)*10, passive_result.mean() , c='r',label='passive learning')\n",
    "plt.plot(np.arange(1,91)*10, active_result_df.mean(), c='b', label='active learning')\n",
    "plt.xlabel('number of training data')\n",
    "plt.ylabel('error rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### conclusion:\n",
    "    After ploting two curves in one plot. We can directly see thay active learning is better when number of trianging data is small. Howeverm, with the increase of number of training data, the passive learning will become as well as active learning. Due to the randomly select of training and test data, the last error rate is not same. If we always use the same trainging data and test data, we might gain the same error rate while using the whole training data to train model. This result just base on my own experiment. It might be influenced by the select of training data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
