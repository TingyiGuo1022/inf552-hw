{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Multi-class and Multi-Label Classi\f",
    "cation Using Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Choose 70% of the data randomly as the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dataset\n",
    "dataset = pd.read_csv(\"Frogs_MFCCs.csv\")\n",
    "dataset_x = dataset.iloc[:,:22]\n",
    "dataset_y = dataset.iloc[:,22:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset into training and test\n",
    "train_x, test_x, train_y, test_y = train_test_split(dataset_x, dataset_y, test_size=0.3, random_state=30)\n",
    "#reindex\n",
    "train_x = train_x.sort_index().reset_index(drop=True)\n",
    "train_y = train_y.sort_index().reset_index(drop=True)\n",
    "\n",
    "test_x = test_x.sort_index().reset_index(drop=True)\n",
    "test_y = test_y.sort_index().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Each instance has three labels: Families, Genus, and Species. Each of the labels has multiple classes. We wish to solve a multi-class and multi-label problem. One of the most important approaches to multi-class classification is to train a classifier for each label. We first try this approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. Research exact match and hamming score/ loss methods for evaluating multi-label classification and use them in evaluating the classifiers in this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Exact matches: the instances that are correctly classified in each labels\n",
    "    \n",
    "    Hamming score: the number of extract matches samples/ total number of samples.\n",
    "    \n",
    "    Hamming loss:Hamming loss: the fraction of the wrong labels to the total number of labels. This is a loss function, so the optimal value is zero. Hamming Loss =1- Hamming score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. Train a SVM for each of the labels, using Gaussian kernels and one versus all classifiers. Determine the weight of the SVM penalty and the width of the Gaussian Kernel using 10 fold cross validation. You are welcome to try to solve the problem with both standardized and raw attributes and report the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize data\n",
    "scaler= StandardScaler()\n",
    "scaler.fit(dataset_x)\n",
    "standard_train_x = pd.DataFrame(scaler.transform(train_x))\n",
    "standard_test_x = pd.DataFrame(scaler.transform(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split one y set into three to solve the multi-class and nulti-label problem\n",
    "train_y1, train_y2, train_y3 = train_y[\"Family\"], train_y[\"Genus\"], train_y[\"Species\"]\n",
    "test_y1, test_y2, test_y3 = test_y['Family'], test_y['Genus'], test_y['Species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#range of the weight of the SVM penalty and the width of the Gaussian Kernel\n",
    "param = {'C':np.logspace(-3,6,10),'gamma':np.linspace(.1,2,20)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one finished.\n",
      "two filished\n",
      "three finished\n"
     ]
    }
   ],
   "source": [
    "svc = SVC(kernel='rbf', random_state=1)\n",
    "#build model for each label\n",
    "clf1 = GridSearchCV(svc, param, cv=10)\n",
    "clf1.fit(standard_train_x, train_y1)\n",
    "print('one finished.')\n",
    "\n",
    "clf2 = GridSearchCV(svc, param, cv=10)\n",
    "clf2.fit(standard_train_x, train_y2)\n",
    "print('two filished')\n",
    "\n",
    "clf3 = GridSearchCV(svc, param, cv=10)\n",
    "clf3.fit(standard_train_x, train_y3)\n",
    "print('three finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best parameters of model1:  1.0 0.1\n",
      "the best parameters of model2:  10.0 0.1\n",
      "the best parameters of model3:  10.0 0.1\n"
     ]
    }
   ],
   "source": [
    "## Get the best parameters for each models\n",
    "C1, gamma1 = clf1.best_params_['C'], clf1.best_params_['gamma']\n",
    "C2, gamma2 = clf2.best_params_['C'], clf2.best_params_['gamma']\n",
    "C3, gamma3 = clf3.best_params_['C'], clf3.best_params_['gamma']\n",
    "\n",
    "print(\"the best parameters of model1: \",C1, gamma1)\n",
    "print(\"the best parameters of model2: \",C2, gamma2)\n",
    "print(\"the best parameters of model3: \",C3, gamma3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Refit the models with the parameters\n",
    "svc1 = SVC(C=C1, gamma=gamma1, kernel='rbf', random_state=1).fit(standard_train_x, train_y1)\n",
    "svc2 = SVC(C=C2, gamma=gamma2, kernel='rbf', random_state=1).fit(standard_train_x, train_y2)\n",
    "svc3 = SVC(C=C3, gamma=gamma3, kernel='rbf', random_state=1).fit(standard_train_x, train_y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict with the models\n",
    "test_predict_y1 = pd.Series(svc1.predict(standard_test_x))\n",
    "test_predict_y2 = pd.Series(svc2.predict(standard_test_x))\n",
    "test_predict_y3 = pd.Series(svc3.predict(standard_test_x))\n",
    "\n",
    "#contact three results\n",
    "test_predict_y = pd.concat([test_predict_y1, test_predict_y2, test_predict_y3], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hamming loss of Family :  0.0111\n",
      "hamming score of Family :  0.9889 \n",
      "\n",
      "hamming loss of Genus  :  0.012\n",
      "hamming score of Genus  :  0.988 \n",
      "\n",
      "hamming loss of Species:  0.0148\n",
      "hamming score of Species:  0.9852 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import hamming_loss\n",
    "hamming_loss_Family =hamming_loss(test_predict_y1, test_y1)\n",
    "hamming_score_Family = 1- hamming_loss_Family\n",
    "hamming_loss_Genus =hamming_loss(test_predict_y2, test_y2)\n",
    "hamming_score_Genus = 1- hamming_loss_Genus\n",
    "hamming_loss_Species =hamming_loss(test_predict_y3, test_y3)\n",
    "hamming_score_Species = 1- hamming_loss_Species\n",
    "print(\"hamming loss of Family : \", round(hamming_loss_Family ,4))\n",
    "print(\"hamming score of Family : \", round(hamming_score_Family ,4),\"\\n\")\n",
    "print(\"hamming loss of Genus  : \", round(hamming_loss_Genus ,4))\n",
    "print(\"hamming score of Genus  : \", round(hamming_score_Genus ,4),\"\\n\")\n",
    "print(\"hamming loss of Species: \", round(hamming_loss_Species ,4))\n",
    "print(\"hamming score of Species: \", round(hamming_score_Species ,4),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_score_multilabel(predict_y, true_y):\n",
    "    predict_y = predict_y.sort_index().reset_index(drop=True)\n",
    "    true_y = true_y.sort_index().reset_index(drop=True)\n",
    "    total_instance = predict_y.shape[0]\n",
    "    exact_match = 0\n",
    "    for i, row in enumerate(predict_y.iterrows()):\n",
    "        if predict_y.loc[i].tolist() == true_y.loc[i].tolist(): \n",
    "            exact_match += 1\n",
    "    return exact_match/total_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total test hamming score is :  0.9819\n",
      "total test hamming loss is :  0.0181\n"
     ]
    }
   ],
   "source": [
    "test_hamming_score = hamming_score_multilabel(test_predict_y,test_y)\n",
    "test_hamming_loss =1- test_hamming_score\n",
    "print(\"total test hamming score is : \", round(test_hamming_score,4))\n",
    "print(\"total test hamming loss is : \", round(test_hamming_loss,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii. Repeat 1(b)ii with L1-penalized SVM Remember to standardize the attributes. Determine the weight of the SVM penalty using 10 fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'C':np.logspace(-3,6,10)}\n",
    "linear_svc_l1 = LinearSVC(penalty='l1', dual=False, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one finished.\n",
      "two filished\n",
      "three finished\n"
     ]
    }
   ],
   "source": [
    "#build model for each label\n",
    "Linear_clf1 = GridSearchCV(linear_svc_l1, param, cv=10)\n",
    "Linear_clf1.fit(standard_train_x, train_y1)\n",
    "print('one finished.')\n",
    "\n",
    "Linear_clf2 = GridSearchCV(linear_svc_l1, param, cv=10)\n",
    "Linear_clf2.fit(standard_train_x, train_y2)\n",
    "print('two filished')\n",
    "\n",
    "Linear_clf3 = GridSearchCV(linear_svc_l1, param, cv=10)\n",
    "Linear_clf3.fit(standard_train_x, train_y3)\n",
    "print('three finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best C of model1:  0.1\n",
      "the best C of model2:  100.0\n",
      "the best C of model3:  1.0\n"
     ]
    }
   ],
   "source": [
    "C1 = Linear_clf1.best_params_['C']\n",
    "C2 = Linear_clf2.best_params_['C']\n",
    "C3 = Linear_clf3.best_params_['C']\n",
    "\n",
    "print(\"the best C of model1: \",C1)\n",
    "print(\"the best C of model2: \",C2)\n",
    "print(\"the best C of model3: \",C3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear_svc1 = LinearSVC(penalty='l1', dual=False, C= C1, random_state=1).fit(standard_train_x, train_y1)\n",
    "Linear_svc2 = LinearSVC(penalty='l1', dual=False, C= C2, random_state=1).fit(standard_train_x, train_y2)\n",
    "Linear_svc3 = LinearSVC(penalty='l1', dual=False, C= C3, random_state=1).fit(standard_train_x, train_y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict with the models\n",
    "Linear_test_predict_y1 = pd.Series(Linear_svc1.predict(standard_test_x))\n",
    "Linear_test_predict_y2 = pd.Series(Linear_svc2.predict(standard_test_x))\n",
    "Linear_test_predict_y3 = pd.Series(Linear_svc3.predict(standard_test_x))\n",
    "\n",
    "#contact three results\n",
    "Linear_test_predict_y = pd.concat([Linear_test_predict_y1, Linear_test_predict_y2, Linear_test_predict_y3], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hamming loss of Family :  0.0699\n",
      "hamming score of Family :  0.9301 \n",
      "\n",
      "hamming loss of Genus  :  0.0491\n",
      "hamming score of Genus  :  0.9509 \n",
      "\n",
      "hamming loss of Species:  0.0431\n",
      "hamming score of Species:  0.9569 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "hamming_loss_Family =hamming_loss(Linear_test_predict_y1, test_y1)\n",
    "hamming_score_Family = 1- hamming_loss_Family\n",
    "hamming_loss_Genus =hamming_loss(Linear_test_predict_y2, test_y2)\n",
    "hamming_score_Genus = 1- hamming_loss_Genus\n",
    "hamming_loss_Species =hamming_loss(Linear_test_predict_y3, test_y3)\n",
    "hamming_score_Species = 1- hamming_loss_Species\n",
    "print(\"hamming loss of Family : \", round(hamming_loss_Family ,4))\n",
    "print(\"hamming score of Family : \", round(hamming_score_Family ,4),\"\\n\")\n",
    "print(\"hamming loss of Genus  : \", round(hamming_loss_Genus ,4))\n",
    "print(\"hamming score of Genus  : \", round(hamming_score_Genus ,4),\"\\n\")\n",
    "print(\"hamming loss of Species: \", round(hamming_loss_Species ,4))\n",
    "print(\"hamming score of Species: \", round(hamming_score_Species ,4),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total test hamming score of LinearSVC is :  0.9097\n",
      "total test hamming loss of LinearSVC is :  0.0903\n"
     ]
    }
   ],
   "source": [
    "Linear_test_hamming_score = hamming_score_multilabel(Linear_test_predict_y,test_y)\n",
    "Linear_test_hamming_loss = 1-Linear_test_hamming_score\n",
    "print(\"total test hamming score of LinearSVC is : \", round(Linear_test_hamming_score,4))\n",
    "print(\"total test hamming loss of LinearSVC is : \", round(Linear_test_hamming_loss,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iv. Repeat 1(b)iii by using SMOTE or any other method you know to remedy class imbalance. Report your conclusions about the classifiers you trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state=1)\n",
    "smote_train_x1, smote_train_y1 = sm.fit_resample(standard_train_x, train_y1)\n",
    "smote_train_x2, smote_train_y2 = sm.fit_resample(standard_train_x, train_y2)\n",
    "smote_train_x3, smote_train_y3 = sm.fit_resample(standard_train_x, train_y3)\n",
    "\n",
    "smote_train_x1, smote_train_x2, smote_train_x3 = pd.DataFrame(smote_train_x1), pd.DataFrame(smote_train_x2), pd.DataFrame(smote_train_x3)\n",
    "smote_train_y1, smote_train_y2, smote_train_y3 = pd.Series(smote_train_y1), pd.Series(smote_train_y2), pd.Series(smote_train_y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'C':np.logspace(-3,6,10)}\n",
    "linear_svc_l1 = LinearSVC(penalty='l1', dual=False, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one finished.\n",
      "two filished\n",
      "three finished\n"
     ]
    }
   ],
   "source": [
    "#build model for each label\n",
    "smote_clf1 = GridSearchCV(linear_svc_l1, param, cv=10)\n",
    "smote_clf1.fit(smote_train_x1, smote_train_y1)\n",
    "print('one finished.')\n",
    "\n",
    "smote_clf2 = GridSearchCV(linear_svc_l1, param, cv=10)\n",
    "smote_clf2.fit(smote_train_x2, smote_train_y2)\n",
    "print('two filished')\n",
    "\n",
    "smote_clf3 = GridSearchCV(linear_svc_l1, param, cv=10)\n",
    "smote_clf3.fit(smote_train_x3, smote_train_y3)\n",
    "print('three finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best C of model1:  10.0\n",
      "the best C of model2:  100.0\n",
      "the best C of model3:  100.0\n"
     ]
    }
   ],
   "source": [
    "C1 = smote_clf1.best_params_['C']\n",
    "C2 = smote_clf2.best_params_['C']\n",
    "C3 = smote_clf3.best_params_['C']\n",
    "\n",
    "print(\"the best C of model1: \",C1)\n",
    "print(\"the best C of model2: \",C2)\n",
    "print(\"the best C of model3: \",C3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_svc1 = LinearSVC(penalty='l1', dual=False, C= C1, random_state=1).fit(smote_train_x1, smote_train_y1)\n",
    "smote_svc2 = LinearSVC(penalty='l1', dual=False, C= C2, random_state=1).fit(smote_train_x2, smote_train_y2)\n",
    "smote_svc3 = LinearSVC(penalty='l1', dual=False, C= C3, random_state=1).fit(smote_train_x3, smote_train_y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict with the models\n",
    "smote_test_predict_y1 = pd.Series(smote_svc1.predict(standard_test_x))\n",
    "smote_test_predict_y2 = pd.Series(smote_svc2.predict(standard_test_x))\n",
    "smote_test_predict_y3 = pd.Series(smote_svc3.predict(standard_test_x))\n",
    "\n",
    "#contact three results\n",
    "smote_test_predict_y = pd.concat([smote_test_predict_y1, smote_test_predict_y2, smote_test_predict_y3], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hamming loss of Family :  0.0764\n",
      "hamming score of Family :  0.9236 \n",
      "\n",
      "hamming loss of Genus  :  0.0857\n",
      "hamming score of Genus  :  0.9143 \n",
      "\n",
      "hamming loss of Species:  0.0426\n",
      "hamming score of Species:  0.9574 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "hamming_loss_Family =hamming_loss(smote_test_predict_y1, test_y1)\n",
    "hamming_score_Family = 1- hamming_loss_Family\n",
    "hamming_loss_Genus =hamming_loss(smote_test_predict_y2, test_y2)\n",
    "hamming_score_Genus = 1- hamming_loss_Genus\n",
    "hamming_loss_Species =hamming_loss(smote_test_predict_y3, test_y3)\n",
    "hamming_score_Species = 1- hamming_loss_Species\n",
    "print(\"hamming loss of Family : \", round(hamming_loss_Family ,4))\n",
    "print(\"hamming score of Family : \", round(hamming_score_Family ,4),\"\\n\")\n",
    "print(\"hamming loss of Genus  : \", round(hamming_loss_Genus ,4))\n",
    "print(\"hamming score of Genus  : \", round(hamming_score_Genus ,4),\"\\n\")\n",
    "print(\"hamming loss of Species: \", round(hamming_loss_Species ,4))\n",
    "print(\"hamming score of Species: \", round(hamming_score_Species ,4),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total test hamming score of LinearSVC after SMOTE is :  0.8648\n",
      "total test hamming loss of LinearSVC after SMOTE is :  0.1352\n"
     ]
    }
   ],
   "source": [
    "smote_test_hamming_score = hamming_score_multilabel(smote_test_predict_y,test_y)\n",
    "smote_test_hamming_loss = 1-smote_test_hamming_score\n",
    "print(\"total test hamming score of LinearSVC after SMOTE is : \", round(smote_test_hamming_score,4))\n",
    "print(\"total test hamming loss of LinearSVC after SMOTE is : \", round(smote_test_hamming_loss,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     Conclusion: After doing SMOTE for training dataset, the hamming loss of each label increase, the total test hamming score is worse than the result before doing up-sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. K-Means Clustering on a Multi-Class and Multi-Label Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte-Carlo Simulation: Perform the following procedures 50 times, and report the average and standard deviation of the 50 Hamming Distances that you calculate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Use k-means clustering on the whole Anuran Calls (MFCCs) Data Set (do not split the data into train and test, as we are not performing supervised learning in this exercise). Choose k automatically based on one of the methods provided in the slides (CH or Gap Statistics or scree plots or Silhouettes) or any other method you know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KMeans_silhouette_score(data_x, k):\n",
    "    kmeans_model = KMeans(n_clusters=k).fit(data_x)\n",
    "    labels = kmeans_model.labels_\n",
    "    return silhouette_score(data_x, labels, metric=\"euclidean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MFCCs_ 1</th>\n",
       "      <th>MFCCs_ 2</th>\n",
       "      <th>MFCCs_ 3</th>\n",
       "      <th>MFCCs_ 4</th>\n",
       "      <th>MFCCs_ 5</th>\n",
       "      <th>MFCCs_ 6</th>\n",
       "      <th>MFCCs_ 7</th>\n",
       "      <th>MFCCs_ 8</th>\n",
       "      <th>MFCCs_ 9</th>\n",
       "      <th>MFCCs_10</th>\n",
       "      <th>...</th>\n",
       "      <th>MFCCs_13</th>\n",
       "      <th>MFCCs_14</th>\n",
       "      <th>MFCCs_15</th>\n",
       "      <th>MFCCs_16</th>\n",
       "      <th>MFCCs_17</th>\n",
       "      <th>MFCCs_18</th>\n",
       "      <th>MFCCs_19</th>\n",
       "      <th>MFCCs_20</th>\n",
       "      <th>MFCCs_21</th>\n",
       "      <th>MFCCs_22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.152936</td>\n",
       "      <td>-0.105586</td>\n",
       "      <td>0.200722</td>\n",
       "      <td>0.317201</td>\n",
       "      <td>0.260764</td>\n",
       "      <td>0.100945</td>\n",
       "      <td>-0.150063</td>\n",
       "      <td>-0.171128</td>\n",
       "      <td>0.124676</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.156436</td>\n",
       "      <td>0.082245</td>\n",
       "      <td>0.135752</td>\n",
       "      <td>-0.024017</td>\n",
       "      <td>-0.108351</td>\n",
       "      <td>-0.077623</td>\n",
       "      <td>-0.009568</td>\n",
       "      <td>0.057684</td>\n",
       "      <td>0.118680</td>\n",
       "      <td>0.014038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.171534</td>\n",
       "      <td>-0.098975</td>\n",
       "      <td>0.268425</td>\n",
       "      <td>0.338672</td>\n",
       "      <td>0.268353</td>\n",
       "      <td>0.060835</td>\n",
       "      <td>-0.222475</td>\n",
       "      <td>-0.207693</td>\n",
       "      <td>0.170883</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.254341</td>\n",
       "      <td>0.022786</td>\n",
       "      <td>0.163320</td>\n",
       "      <td>0.012022</td>\n",
       "      <td>-0.090974</td>\n",
       "      <td>-0.056510</td>\n",
       "      <td>-0.035303</td>\n",
       "      <td>0.020140</td>\n",
       "      <td>0.082263</td>\n",
       "      <td>0.029056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.152317</td>\n",
       "      <td>-0.082973</td>\n",
       "      <td>0.287128</td>\n",
       "      <td>0.276014</td>\n",
       "      <td>0.189867</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>-0.242234</td>\n",
       "      <td>-0.219153</td>\n",
       "      <td>0.232538</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.237384</td>\n",
       "      <td>0.050791</td>\n",
       "      <td>0.207338</td>\n",
       "      <td>0.083536</td>\n",
       "      <td>-0.050691</td>\n",
       "      <td>-0.023590</td>\n",
       "      <td>-0.066722</td>\n",
       "      <td>-0.025083</td>\n",
       "      <td>0.099108</td>\n",
       "      <td>0.077162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.224392</td>\n",
       "      <td>0.118985</td>\n",
       "      <td>0.329432</td>\n",
       "      <td>0.372088</td>\n",
       "      <td>0.361005</td>\n",
       "      <td>0.015501</td>\n",
       "      <td>-0.194347</td>\n",
       "      <td>-0.098181</td>\n",
       "      <td>0.270375</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.317084</td>\n",
       "      <td>-0.011567</td>\n",
       "      <td>0.100413</td>\n",
       "      <td>-0.050224</td>\n",
       "      <td>-0.136009</td>\n",
       "      <td>-0.177037</td>\n",
       "      <td>-0.130498</td>\n",
       "      <td>-0.054766</td>\n",
       "      <td>-0.018691</td>\n",
       "      <td>0.023954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.087817</td>\n",
       "      <td>-0.068345</td>\n",
       "      <td>0.306967</td>\n",
       "      <td>0.330923</td>\n",
       "      <td>0.249144</td>\n",
       "      <td>0.006884</td>\n",
       "      <td>-0.265423</td>\n",
       "      <td>-0.172700</td>\n",
       "      <td>0.266434</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.298524</td>\n",
       "      <td>0.037439</td>\n",
       "      <td>0.219153</td>\n",
       "      <td>0.062837</td>\n",
       "      <td>-0.048885</td>\n",
       "      <td>-0.053074</td>\n",
       "      <td>-0.088550</td>\n",
       "      <td>-0.031346</td>\n",
       "      <td>0.108610</td>\n",
       "      <td>0.079244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7190</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.554504</td>\n",
       "      <td>-0.337717</td>\n",
       "      <td>0.035533</td>\n",
       "      <td>0.034511</td>\n",
       "      <td>0.443451</td>\n",
       "      <td>0.093889</td>\n",
       "      <td>-0.100753</td>\n",
       "      <td>0.037087</td>\n",
       "      <td>0.081075</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.145668</td>\n",
       "      <td>-0.059364</td>\n",
       "      <td>0.024206</td>\n",
       "      <td>-0.000861</td>\n",
       "      <td>0.069430</td>\n",
       "      <td>0.071001</td>\n",
       "      <td>0.021591</td>\n",
       "      <td>0.052449</td>\n",
       "      <td>-0.021860</td>\n",
       "      <td>-0.079860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7191</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.517273</td>\n",
       "      <td>-0.370574</td>\n",
       "      <td>0.030673</td>\n",
       "      <td>0.068097</td>\n",
       "      <td>0.402890</td>\n",
       "      <td>0.096628</td>\n",
       "      <td>-0.116460</td>\n",
       "      <td>0.063727</td>\n",
       "      <td>0.089034</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.164675</td>\n",
       "      <td>-0.105600</td>\n",
       "      <td>0.030767</td>\n",
       "      <td>0.006457</td>\n",
       "      <td>0.061127</td>\n",
       "      <td>0.068978</td>\n",
       "      <td>0.017745</td>\n",
       "      <td>0.046461</td>\n",
       "      <td>-0.015418</td>\n",
       "      <td>-0.101892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7192</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.582557</td>\n",
       "      <td>-0.343237</td>\n",
       "      <td>0.029468</td>\n",
       "      <td>0.064179</td>\n",
       "      <td>0.385596</td>\n",
       "      <td>0.114905</td>\n",
       "      <td>-0.103317</td>\n",
       "      <td>0.070370</td>\n",
       "      <td>0.081317</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.150025</td>\n",
       "      <td>-0.078615</td>\n",
       "      <td>0.024861</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.082474</td>\n",
       "      <td>0.077771</td>\n",
       "      <td>-0.009688</td>\n",
       "      <td>0.027834</td>\n",
       "      <td>-0.000531</td>\n",
       "      <td>-0.080425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7193</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.519497</td>\n",
       "      <td>-0.307553</td>\n",
       "      <td>-0.004922</td>\n",
       "      <td>0.072865</td>\n",
       "      <td>0.377131</td>\n",
       "      <td>0.086866</td>\n",
       "      <td>-0.115799</td>\n",
       "      <td>0.056979</td>\n",
       "      <td>0.089316</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.153120</td>\n",
       "      <td>-0.075320</td>\n",
       "      <td>0.022903</td>\n",
       "      <td>0.001924</td>\n",
       "      <td>0.051796</td>\n",
       "      <td>0.069073</td>\n",
       "      <td>0.017963</td>\n",
       "      <td>0.041803</td>\n",
       "      <td>-0.027911</td>\n",
       "      <td>-0.096895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7194</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.508833</td>\n",
       "      <td>-0.324106</td>\n",
       "      <td>0.062068</td>\n",
       "      <td>0.078211</td>\n",
       "      <td>0.397188</td>\n",
       "      <td>0.094596</td>\n",
       "      <td>-0.117672</td>\n",
       "      <td>0.058874</td>\n",
       "      <td>0.076180</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.150554</td>\n",
       "      <td>-0.073415</td>\n",
       "      <td>0.042517</td>\n",
       "      <td>0.004158</td>\n",
       "      <td>0.061455</td>\n",
       "      <td>0.072983</td>\n",
       "      <td>-0.003980</td>\n",
       "      <td>0.031560</td>\n",
       "      <td>-0.029355</td>\n",
       "      <td>-0.087910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7195 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MFCCs_ 1  MFCCs_ 2  MFCCs_ 3  MFCCs_ 4  MFCCs_ 5  MFCCs_ 6  MFCCs_ 7  \\\n",
       "0          1.0  0.152936 -0.105586  0.200722  0.317201  0.260764  0.100945   \n",
       "1          1.0  0.171534 -0.098975  0.268425  0.338672  0.268353  0.060835   \n",
       "2          1.0  0.152317 -0.082973  0.287128  0.276014  0.189867  0.008714   \n",
       "3          1.0  0.224392  0.118985  0.329432  0.372088  0.361005  0.015501   \n",
       "4          1.0  0.087817 -0.068345  0.306967  0.330923  0.249144  0.006884   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "7190       1.0 -0.554504 -0.337717  0.035533  0.034511  0.443451  0.093889   \n",
       "7191       1.0 -0.517273 -0.370574  0.030673  0.068097  0.402890  0.096628   \n",
       "7192       1.0 -0.582557 -0.343237  0.029468  0.064179  0.385596  0.114905   \n",
       "7193       1.0 -0.519497 -0.307553 -0.004922  0.072865  0.377131  0.086866   \n",
       "7194       1.0 -0.508833 -0.324106  0.062068  0.078211  0.397188  0.094596   \n",
       "\n",
       "      MFCCs_ 8  MFCCs_ 9  MFCCs_10  ...  MFCCs_13  MFCCs_14  MFCCs_15  \\\n",
       "0    -0.150063 -0.171128  0.124676  ... -0.156436  0.082245  0.135752   \n",
       "1    -0.222475 -0.207693  0.170883  ... -0.254341  0.022786  0.163320   \n",
       "2    -0.242234 -0.219153  0.232538  ... -0.237384  0.050791  0.207338   \n",
       "3    -0.194347 -0.098181  0.270375  ... -0.317084 -0.011567  0.100413   \n",
       "4    -0.265423 -0.172700  0.266434  ... -0.298524  0.037439  0.219153   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "7190 -0.100753  0.037087  0.081075  ... -0.145668 -0.059364  0.024206   \n",
       "7191 -0.116460  0.063727  0.089034  ... -0.164675 -0.105600  0.030767   \n",
       "7192 -0.103317  0.070370  0.081317  ... -0.150025 -0.078615  0.024861   \n",
       "7193 -0.115799  0.056979  0.089316  ... -0.153120 -0.075320  0.022903   \n",
       "7194 -0.117672  0.058874  0.076180  ... -0.150554 -0.073415  0.042517   \n",
       "\n",
       "      MFCCs_16  MFCCs_17  MFCCs_18  MFCCs_19  MFCCs_20  MFCCs_21  MFCCs_22  \n",
       "0    -0.024017 -0.108351 -0.077623 -0.009568  0.057684  0.118680  0.014038  \n",
       "1     0.012022 -0.090974 -0.056510 -0.035303  0.020140  0.082263  0.029056  \n",
       "2     0.083536 -0.050691 -0.023590 -0.066722 -0.025083  0.099108  0.077162  \n",
       "3    -0.050224 -0.136009 -0.177037 -0.130498 -0.054766 -0.018691  0.023954  \n",
       "4     0.062837 -0.048885 -0.053074 -0.088550 -0.031346  0.108610  0.079244  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "7190 -0.000861  0.069430  0.071001  0.021591  0.052449 -0.021860 -0.079860  \n",
       "7191  0.006457  0.061127  0.068978  0.017745  0.046461 -0.015418 -0.101892  \n",
       "7192  0.008696  0.082474  0.077771 -0.009688  0.027834 -0.000531 -0.080425  \n",
       "7193  0.001924  0.051796  0.069073  0.017963  0.041803 -0.027911 -0.096895  \n",
       "7194  0.004158  0.061455  0.072983 -0.003980  0.031560 -0.029355 -0.087910  \n",
       "\n",
       "[7195 rows x 22 columns]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_x = dataset.iloc[:,:22]\n",
    "dataset_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on silhouette score, the best k is :   4\n"
     ]
    }
   ],
   "source": [
    "max_pair=[0,0]\n",
    "#decide the best k\n",
    "for k in range(2, 51):\n",
    "    if KMeans_silhouette_score(dataset_x, k)>max_pair[1]:\n",
    "        max_pair=[k, KMeans_silhouette_score(dataset_x, k)]\n",
    "print(\"Based on silhouette score, the best k is :  \", max_pair[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) In each cluster, determine which family is the majority by reading the true labels. Repeat for genus and species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the best k\n",
    "best_k = max_pair[0]\n",
    "\n",
    "#refit the kmeans model\n",
    "kmeans_model = KMeans(n_clusters=best_k).fit(dataset_x)\n",
    "labels = pd.Series(kmeans_model.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_dict={}\n",
    "#store index of each cluster\n",
    "for cluster in range(4):\n",
    "    cluster_index= labels.index[labels==cluster].tolist()\n",
    "    cluster_dict[cluster]=cluster_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The major family of cluster 0  is  : Leptodactylidae\n",
      "The major Genus of cluster 0  is   : Adenomera\n",
      "The major Species of cluster 0  is : AdenomeraHylaedactylus\n",
      "\n",
      "\n",
      "\n",
      "The major family of cluster 1  is  : Hylidae\n",
      "The major Genus of cluster 1  is   : Hypsiboas\n",
      "The major Species of cluster 1  is : HypsiboasCordobae\n",
      "\n",
      "\n",
      "\n",
      "The major family of cluster 2  is  : Hylidae\n",
      "The major Genus of cluster 2  is   : Hypsiboas\n",
      "The major Species of cluster 2  is : HypsiboasCinerascens\n",
      "\n",
      "\n",
      "\n",
      "The major family of cluster 3  is  : Dendrobatidae\n",
      "The major Genus of cluster 3  is   : Ameerega\n",
      "The major Species of cluster 3  is : Ameeregatrivittata\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#split whole target dataset into three parts\n",
    "dataset_y1, dataset_y2, dataset_y3= dataset_y[\"Family\"], dataset_y[\"Genus\"], dataset_y[\"Species\"]\n",
    "#store major class in a dict to use in next question\n",
    "major_triple = {}\n",
    "for cluster in range(4):\n",
    "    major_family = dataset_y1[cluster_dict[cluster]].value_counts().index[0]\n",
    "    major_Genus = dataset_y2[cluster_dict[cluster]].value_counts().index[0]\n",
    "    major_Species = dataset_y3[cluster_dict[cluster]].value_counts().index[0]\n",
    "    major_triple[cluster]=[major_family, major_Genus, major_Species]\n",
    "    \n",
    "    print(\"The major family of cluster\",cluster, \" is  :\", major_family)\n",
    "    print(\"The major Genus of cluster\",cluster, \" is   :\", major_Genus)\n",
    "    print(\"The major Species of cluster\",cluster, \" is :\", major_Species)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Now for each cluster you have a majority label triplet (family, genus, species). Calculate the average Hamming distance, Hamming score, and Hamming loss between the true labels and the labels assigned by clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Monte-Carlo Simulation: Perform the following procedures 50 times, and report the average and standard deviation of the 50 Hamming Distances that you calculate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a mothod to calculate hamming dustance\n",
    "def calculate_hamming_distance(predict_y, true_y):\n",
    "    predict_y = predict_y.sort_index().reset_index(drop=True)\n",
    "    true_y = true_y.sort_index().reset_index(drop=True)\n",
    "    total_instance = predict_y.shape[0]\n",
    "    exact_match = 0\n",
    "    for i, row in enumerate(predict_y.iterrows()):\n",
    "        if predict_y.loc[i].tolist() == true_y.loc[i].tolist(): \n",
    "            exact_match += 1\n",
    "    return total_instance - exact_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average hamming distance is :  439.75\n",
      "The average hamming score is :  0.6765\n",
      "The average hamming loss is :  0.3235\n"
     ]
    }
   ],
   "source": [
    "predict_y = pd.DataFrame(columns=dataset_y.columns,index=dataset_y.index)\n",
    "for cluster in range(4):\n",
    "    for index in cluster_dict[cluster]:\n",
    "        predict_y.loc[index] = major_triple[cluster]\n",
    "    #calculate total distance,score, loss of all clusters\n",
    "hamming_distance, hamming_score, hamming_loss=0, 0, 0\n",
    "for cluster in range(4):\n",
    "    hamming_distance += calculate_hamming_distance(predict_y.loc[cluster_dict[cluster]], \n",
    "                                                       dataset_y.loc[cluster_dict[cluster]])\n",
    "    hamming_score +=  hamming_score_multilabel(predict_y.loc[cluster_dict[cluster]], \n",
    "                                                   dataset_y.loc[cluster_dict[cluster]])\n",
    "    hamming_loss += 1-hamming_score_multilabel(predict_y.loc[cluster_dict[cluster]], \n",
    "                                                   dataset_y.loc[cluster_dict[cluster]])\n",
    "\n",
    "print(\"The average hamming distance is : \", hamming_distance/4)\n",
    "print(\"The average hamming score is : \", round(hamming_score/4,4))\n",
    "print(\"The average hamming loss is : \", round(hamming_loss/4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte-Carlo Simulation: Perform the following procedures 50 times, and report the average and standard deviation of the 50 Hamming Distances that you calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a mothod to calculate hamming dustance\n",
    "def calculate_hamming_distance(predict_y, true_y):\n",
    "    total_instance = predict_y.shape[0]\n",
    "    exact_match = 0\n",
    "    for i, row in enumerate(predict_y.iterrows()):\n",
    "        if predict_y.loc[i].tolist() == true_y.loc[i].tolist(): \n",
    "            exact_match += 1\n",
    "    return total_instance - exact_match\n",
    "# calculate three parameters\n",
    "def cal_all_parameters(data_x, data_y):\n",
    "    #refit the kmeans model\n",
    "    kmeans_model = KMeans(n_clusters=best_k).fit(dataset_x)\n",
    "    labels = pd.Series(kmeans_model.labels_)\n",
    "    \n",
    "    cluster_dict={}\n",
    "    #store index of each cluster\n",
    "    for cluster in range(4):\n",
    "        cluster_index= labels.index[labels==cluster].tolist()\n",
    "        cluster_dict[cluster]=cluster_index\n",
    "        \n",
    "    #split whole target dataset into three parts\n",
    "    dataset_y1, dataset_y2, dataset_y3= dataset_y[\"Family\"], dataset_y[\"Genus\"], dataset_y[\"Species\"]\n",
    "    #store major class in a dict to use in next question\n",
    "    major_triple = {}\n",
    "    for cluster in range(4):\n",
    "        major_family = dataset_y1[cluster_dict[cluster]].value_counts().index[0]\n",
    "        major_Genus = dataset_y2[cluster_dict[cluster]].value_counts().index[0]\n",
    "        major_Species = dataset_y3[cluster_dict[cluster]].value_counts().index[0]\n",
    "        major_triple[cluster]=[major_family, major_Genus, major_Species]\n",
    "    \n",
    "    #rebuild a predict y\n",
    "    predict_y = pd.DataFrame(columns=dataset_y.columns,index=dataset_y.index)\n",
    "    for cluster in range(4):\n",
    "        for index in cluster_dict[cluster]:\n",
    "            predict_y.loc[index] = major_triple[cluster]\n",
    "    #calculate total distance,score, loss of all clusters\n",
    "    hamming_distance = calculate_hamming_distance(predict_y, dataset_y)\n",
    "    hamming_score =  hamming_score_multilabel(predict_y, dataset_y)\n",
    "    hamming_loss = 1-hamming_score\n",
    "    #return average parameters\n",
    "    return [hamming_distance, hamming_score, hamming_loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete 50%\n",
      "complete 100%\n"
     ]
    }
   ],
   "source": [
    "distance, score, loss = list(), list(), list()\n",
    "# Monte-Carlo Simulation\n",
    "for i in range(50):\n",
    "    if i==25:\n",
    "        print(\"complete 50%\")\n",
    "    if i==49:\n",
    "        print(\"complete 100%\")\n",
    "    results = cal_all_parameters(dataset_x, dataset_y)\n",
    "    #transfer average distance, score to total distance, score\n",
    "    hamming_distance = results[0]\n",
    "    hamming_score = results[1]\n",
    "    hamming_loss = results[2]\n",
    "    #store in a list\n",
    "    distance.append(hamming_distance)\n",
    "    score.append(hamming_score)\n",
    "    loss.append(hamming_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average hamming distance :  1774.76\n",
      "Standard deviation of hamming distance :  55.1567 \n",
      "\n",
      "Average hamming score :  0.7533\n",
      "Standard deviation of hamming score :  0.0077 \n",
      "\n",
      "Average hamming score :  0.2467\n",
      "Standard deviation of hamming score :  0.0077\n"
     ]
    }
   ],
   "source": [
    "#result after preforming 50 times\n",
    "print(\"Average hamming distance : \", (np.array(distance).mean()))\n",
    "print(\"Standard deviation of hamming distance : \", round(np.array(distance).std(),4),\"\\n\")\n",
    "print(\"Average hamming score : \", round(np.array(score).mean(),4))\n",
    "print(\"Standard deviation of hamming score : \", round(np.array(score).std(),4),\"\\n\")\n",
    "print(\"Average hamming score : \", round(np.array(loss).mean(),4))\n",
    "print(\"Standard deviation of hamming score : \", round(np.array(loss).std(),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ISLR 10.7.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we have four observations, for which we compute a\n",
    "dissimilarity matrix, given by\n",
    "\n",
    "| a      | b |c|d|\n",
    "| ----------- | ----------- |----------- |----------- |\n",
    "| null| 0.3 | 0.4 | 0.7 |\n",
    "| 0.3 |null | 0.5 | 0.8 |\n",
    "| 0.4 | 0.5 |null | 0.45|\n",
    "| 0.7 | 0.8 | 0.45|null |\n",
    "\n",
    "For instance, the dissimilarity between the first and second observations is 0.3, and the dissimilarity between the second and fourth\n",
    "observations is 0.8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    (a) On the basis of this dissimilarity matrix, sketch the dendrogram that results from hierarchically clustering these four observations using complete linkage. Be sure to indicate on the plot the height at which each fusion occurs, as well as the observations corresponding to each leaf in the dendrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial import distance_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.spatial.distance import squareform\n",
    "X= np.array([[0,0.3,0.4,0.7],[0.3,0,0.5,0.8],[0.4,0.5,0,0.45],[0.7,0.8,0.45,0]])\n",
    "dist = squareform(X)\n",
    "linked = linkage(dist, 'complete')\n",
    "labelList = [1,2,3,4]\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(linked,orientation='top', labels=labelList, distance_sort='descending',show_leaf_counts=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    (b) Repeat (a), this time using single linkage clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial import distance_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.spatial.distance import squareform\n",
    "X= np.array([[0,0.3,0.4,0.7],[0.3,0,0.5,0.8],[0.4,0.5,0,0.45],[0.7,0.8,0.45,0]])\n",
    "dist = squareform(X)\n",
    "linked = linkage(dist, 'single')\n",
    "labelList = [1,2,3,4]\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(linked,orientation='top', labels=labelList, distance_sort='descending',show_leaf_counts=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    (c) Suppose that we cut the dendogram obtained in (a) such that two clusters result. Which observations are in each cluster?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    answer: If we use complete linkage, we will have culster (1,2) and (3,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    (d) Suppose that we cut the dendogram obtained in (b) such that two clusters result. Which observations are in each cluster?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    answer: If we use single linkage, we will have culster ((1,2),3) and (4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    (e) It is mentioned in the chapter that at each fusion in the dendrogram, the position of the two clusters being fused can be swapped without changing the meaning of the dendrogram. Draw a dendrogram that is equivalent to the dendrogram in (a), for which two or more of the leaves are repositioned, but for which the meaning of the dendrogram is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial import distance_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.spatial.distance import squareform\n",
    "X= np.array([[0,0.3,0.4,0.7],[0.3,0,0.5,0.8],[0.4,0.5,0,0.45],[0.7,0.8,0.45,0]])\n",
    "dist = squareform(X)\n",
    "linked = linkage(dist, 'complete')\n",
    "labelList = [2, 1, 4, 3]\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(linked,orientation='top', labels=labelList, distance_sort='descending',show_leaf_counts=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
